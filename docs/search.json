[
  {
    "objectID": "index.html#evaluación",
    "href": "index.html#evaluación",
    "title": "Introducción",
    "section": "Evaluación",
    "text": "Evaluación\nTema 1: un examen (entre finales de octubre y noviembre)\nTema 2: un examen (el último día de clase)\nNota final: promedio de exámenes. (pilar de la nota final)\nNota final + varias entregas hasta un total de un punto (en total serán seis entregas, no son obligatorias)."
  },
  {
    "objectID": "index.html#temario",
    "href": "index.html#temario",
    "title": "Introducción",
    "section": "Temario",
    "text": "Temario\nDos primeros temas: pruebas diagnósticas. (probabildad) Tercer tema: tema transversal dirante el cuatrimestre, relacionado con la creación de muestras. (simulación)"
  },
  {
    "objectID": "DUDAS.html#ic-del-parámetro-por-el-método-de-los-momentos.",
    "href": "DUDAS.html#ic-del-parámetro-por-el-método-de-los-momentos.",
    "title": "Dudas",
    "section": "2.1 IC del parámetro por el método de los momentos.",
    "text": "2.1 IC del parámetro por el método de los momentos.\nEstamos haciendo IC por el método de los momentos. Entonces, cuando el parámetro a estimar es la media y teniendo en cuenta que la media tiene distrubución normal, ¿podemos hacer el IC por el método “clásico”?\n\nRESPUESTA.\n\nVale, nop.\nDos cosas:\n- En el método de los percentiles (el de clase) parece que es “innecesario” hacer las diferencias porque cuando buscamos los percentiles de las diferencias son los percentiles de una transformación lineal que luego deshacemos. Esto solo lo es para la media. Para la mediana, varianza, etc. no existe esa similitud.\n- No podemos hacer el método “clásico” tal cual porque al incluir el sesgo para calcular la amplitud del intervalo, este sesgo debe ser simulado por bootstrap con cada una de las muestras simuladas."
  },
  {
    "objectID": "DUDAS.html#qué-cojones-significa-validación-de-la-estimación",
    "href": "DUDAS.html#qué-cojones-significa-validación-de-la-estimación",
    "title": "Dudas",
    "section": "2.2 ¿Qué cojones significa validación de la estimación?",
    "text": "2.2 ¿Qué cojones significa validación de la estimación?\nRESPUESTA.\nSignifica cómo de válida es mi estimación para representar a mi población.\nSi mi media es 3.5 pero la muestra va de 3 a 9 (suponiendo que 9 no es outlier) no es representativo.\nTambién ligado a la amplitud de mi IC (en relación con el rango de los datos muestrales)."
  },
  {
    "objectID": "DUDAS.html#la-diferencia-entre-hattheta-y-hattheta-se-podría-llamar-sesgo",
    "href": "DUDAS.html#la-diferencia-entre-hattheta-y-hattheta-se-podría-llamar-sesgo",
    "title": "Dudas",
    "section": "2.3 La diferencia entre \\(\\hat\\theta\\) y \\(\\hat\\theta*\\), ¿se podría llamar sesgo?",
    "text": "2.3 La diferencia entre \\(\\hat\\theta\\) y \\(\\hat\\theta*\\), ¿se podría llamar sesgo?\nSeguro que no."
  },
  {
    "objectID": "DUDAS.html#concordancia-vs-chi2",
    "href": "DUDAS.html#concordancia-vs-chi2",
    "title": "Dudas",
    "section": "2.4 Concordancia vs \\(\\chi^2\\)",
    "text": "2.4 Concordancia vs \\(\\chi^2\\)"
  },
  {
    "objectID": "DUDAS.html#existe-un-intervalo-o-p-valor-asociado-a-la-medida-de-la-concordancia",
    "href": "DUDAS.html#existe-un-intervalo-o-p-valor-asociado-a-la-medida-de-la-concordancia",
    "title": "Dudas",
    "section": "2.5 ¿Existe un intervalo o p-valor asociado a la medida de la concordancia?",
    "text": "2.5 ¿Existe un intervalo o p-valor asociado a la medida de la concordancia?"
  },
  {
    "objectID": "DUDAS.html#concordancia-dos-observadores-cuando-tenemos-solo-un-observador",
    "href": "DUDAS.html#concordancia-dos-observadores-cuando-tenemos-solo-un-observador",
    "title": "Dudas",
    "section": "2.6 Concordancia dos observadores cuando tenemos solo un observador",
    "text": "2.6 Concordancia dos observadores cuando tenemos solo un observador\nLos métodos para medir la concordancia entre dos observadores/pruebas aplicados a dos mediciones de un mismo agente, ¿sirve como fiabiidad? ¿O tenemos otros métodos para medir la fiabilidad?"
  },
  {
    "objectID": "tema_00/tema_00_introduccion.html#propósito",
    "href": "tema_00/tema_00_introduccion.html#propósito",
    "title": "Introducción",
    "section": "Propósito",
    "text": "Propósito\nClasificación de un individuo dentro de un grupo de categorías.\nPrueba diagnóstica: cualquier prueba que me hacen. Una prueba diagnostica también puede ser un examen, ya que evaluo un conocimiento y creo una clasificacion.\nVer la concordancia entre varias pruebas o metodologías.\nUn examen, independientemente de qué profesor lo corrija, debería tener práctiamente la misma nota.\nTodo contraste estadístico podemos usarlos para varias acciones. “Un test puede ser usado para medir algo y también todo lo opuesto”.\nConcordancia = acuerdo.\nNo concordancia = actuar de manera idependiente = independientes.\nSi tengo varias metodologías para un mismo propósito (ej: enseñar a bebés a nadar) tengo que ver qué metodología funciona mejor.\nSi para una prueba tengo dos procedimientos y concuerdan entre ellos, puedo usar uno o otro. Si no hay concordancia mezclar los procedimientos me lleva a error.\nMuestra hetérea sin depender de datos muestrales.\nUn modelo que yo le dé cualquier muestra o información, y no un modelo que sirva solo para lo que he observado.\nLos registros simulados tiene que tener las mismas características que los datos muestrales.\nCómo consigo datos muestrales:\n- Datos poblacionales.\n- Simulacion, datos de individuos ficticios.\nProblema: datos faltantes de una de las variables de la muestra. Vamos a hablar más de los problemas que vamos a encontrar que cómo solucionarlos.\nDebemos ser conscientes de que la muestra actual es poco válida para experimentos futuros. Y posiblemente sea necesario ajustes del modelo en el futuro. \\(\\Rightarrow\\) Al igual que calibramos modelos, calibramos la muestra simulada y el modelo teórico.\nEn la vida real es al revés. Te dan unos datos y tienes que crear un modelo, es decir, crear algo que los represente."
  },
  {
    "objectID": "tema_00/tema_00_introduccion.html#planteamiento-estadístico",
    "href": "tema_00/tema_00_introduccion.html#planteamiento-estadístico",
    "title": "Introducción",
    "section": "Planteamiento ‘estadístico’",
    "text": "Planteamiento ‘estadístico’\n\nUn modelo teórico que podamos aplicar a cualquier conjunto de datos.\nX es una variable aleatoria y es la base del modelo teórico\nLa idea es estudiar X, conocer su comportamiento, tomar decisiones sobre X y aplicarlas sobre datos reales.\n¿La variable X ha sido estudiada antes?\n– Sí. Puedo utilizar esos resultados y aplicarlos a mi problema.\n– No. Debo crear el modelo y estudiarlo teóricamente.\n\n\nModelo teórico\nRepresentación matemática abstracta que describe el comportamiento de un fenómeno o conjunto de datos. Permite realizar inferencias, predicciones y análisis de fenómenos aleatorios o deterministas, y se basan en suposiciones sobre la estructura y distribución subyacente de los datos.\n\nComponentes\n\nVariables (características)\nParámetros. Valores desconocidos que determinan la forma de la distribución de datos.\nDistribución de probabilidad.\nSuposiciones sobre el comportamiento de los datos. Por ejemplo, en un modelo de regresión lineal simple se asume que la relación entre las variables es lineal, que los errores tienen una distribución normal con media cero y varianza constante y que son independientes entre sí.\n\n\n\nUso\nEntender la estructura subyacente de los datos, hacer predicciones sobre nuevas observaciones y probar hipótesis.\nEl modelo teórico tiene su propia unidad de medida: la probabilidad.\nIt’s not the same probabilidades que porcentajes, es impreciso. Si p(“enfermo”) = 0.10 no podemos decir que el 10 % de la pobaclión esté enferma, aunque usemos ese lenguaje. Lo correcto es decir que la probabilidad de que una persona de la población esté enferma es 10 %.\nEl IC no es para el parámetro. El IC es para el estimador del parámetro. El IC contiene el % de las muestras del resto del mundo.\nDebemos interpretar las estimaciones puntuales y la distribución acumulada FDA/CFD."
  },
  {
    "objectID": "tema_00/tema_00_introduccion.html#iteraciones-de-todo-problema-estadístico",
    "href": "tema_00/tema_00_introduccion.html#iteraciones-de-todo-problema-estadístico",
    "title": "Introducción",
    "section": "Iteraciones de todo problema estadístico",
    "text": "Iteraciones de todo problema estadístico\nCuestiones asociadas:\n\n¿A qué alumnos? A todos los alumnos.\n\nTengo una oferta si hago un pedido grande.\n\n¿El modelo es único? No, hay tres tallas: P, M y G.\n\nNecesito: Medir la circunferencia del dedo de los alumnos. ¿A todos? ¿qué dedo?\n\n¿Quién aporta la información de 4? Los alumnos del curso actual. ¿Cómo se realiza la medida?\n\n\n\n\nHay varios procedimientos ¿Por ejemplo?\n\n\n\nLos resultados de cada procedimiento\n\n\n\n\nConcuerdan (puedo utilizar cualquiera o mezclarlos) (*1)\n\nHay alguno “mejor” (*2)\n\n\n\nCon (todas/parte de) las medidas de los alumnos establecemos las tallas P, M y G\n\nDecisión sobre la cantidad a pedir, en total y de cada talla\nPoner en práctica → REVISAR: Si hay errores volver a 4"
  },
  {
    "objectID": "tema_00/tema_00_introduccion.html#ejemplo",
    "href": "tema_00/tema_00_introduccion.html#ejemplo",
    "title": "Introducción",
    "section": "Ejemplo",
    "text": "Ejemplo\nEl modelo teórico asociado a este tipo de epidemias indica que:\n– La probabilidad de que en una familia la madre tenga gripe es 0.1\n– En el 12% de las familias el padre tiene gripe\n– Ambos progenitores tienen gripe en el 2% de las familias (= con probabilidad 0.02)\n\n\n\n\n\n\nImportante\n\n\n\nSi sé controlar el comportamiento de una v.a. y sé simular cien familias, ¿qué esperaría si tengo una simulación buena? Que en estas cien simulaciones haya un comportamiento parecido a las bases del modelo teórico.\nCon el modelo teorico tendré dudas pero con datos simulados tangibles ya no tengo probabilidades, tengo un hecho.\n\n\nPara simular una familia simularía pares de datos (padres). Es decir, ´trabajamos con la distribución conjunta. Simularía el 00, el 01, el 10 y el 11. Esto nos permite usar un único número aleatorio por familia, ya que con una única simulación puedo asignar el estado del par de datos (no necesito simular la madre y luego el padre).\nFijo el estado más probables para todas las familias y si sale la condición de los menos frecuentes, lo cambio.\n0.0 - 0.2 pongo 11 0.2 - 0.3 pongo 10 0.3 - 0.5 pongo 01 0.5 - 1.0 pongo 00\nSi quiero algo MÁS ROBUSTO necesito más muestra.\n\n\n\n\n\n\nNota\n\n\n\nEn un modelo teórico tengo un suceso con una probabilidad tal. Con los datos simulados tengo un porcentaje de cada suceso."
  },
  {
    "objectID": "tema_01/tema_01.html#medidas-de-precisión-en-pruebas-diagnósticas.-índices-de-concordancia",
    "href": "tema_01/tema_01.html#medidas-de-precisión-en-pruebas-diagnósticas.-índices-de-concordancia",
    "title": "TEMA 1",
    "section": "Medidas de precisión en pruebas diagnósticas. Índices de concordancia",
    "text": "Medidas de precisión en pruebas diagnósticas. Índices de concordancia\nSimulación numérica para desarrollar pruebas diagnósticas.\nAnálisis de concordancia: se analiza de manera distinta dependiendo del objetivo (y tipo de variables)\nSiempre es lo mismo, árbitro!\nModelo teótico \\(\\Rightarrow\\) Realidad \\(\\Rightarrow\\) Estimamos\nNo es posible dar un valor de concordancia único sino un grado de amplitud. (el nuestro es este grado pero podíamos haber obtenido en un x % de los casos un valor en este intervalo)"
  },
  {
    "objectID": "tema_01/tema_01_1_introduccion.html#definiciones-basadas-o-pensadas-sobre-el-modelo-teórico",
    "href": "tema_01/tema_01_1_introduccion.html#definiciones-basadas-o-pensadas-sobre-el-modelo-teórico",
    "title": "1. Introducción ✓",
    "section": "Definiciones basadas o pensadas sobre el modelo teórico",
    "text": "Definiciones basadas o pensadas sobre el modelo teórico\nPrevalencia\nProbabilidad de que se observe algo. (porque estamos en el modelo teórico)\nPueba diagnóstica\nPrueba que se usa para ayudar a diagnosticar una enfermedad o afección según los signos y síntomas que presenta una persona. Las pruebas diagnósticas también se usan para diseñar un tratamiento, determinar la eficacia de un tratamiento y hacer un pronóstico.\nRápidamente pensamos que es algo clínico pero no siempre es así. Puede ser un examen donde yo asigne un nivel de conocimiento. La prueba diagnóstica puede fallar porque puede dar una nota menor a la real o dar un mayor conociemiento que el verdadero.\nLas pruebas diagnósticas clínicas o experimentales sirven para localizar y situar a un individuo dentro de una clasificacion precisa (una medida según un baremo) o grupo que le corresponde (aprobado/enfermo, sano/enfermo, brazo roto/brazo no roto).\nDependiendo del tipo de variable al que esté asociada la p.d. nos indica qué metodología vamos a usar.\nDebo tener claro para qué fin realizo la p.d..\nPara ver si una prueba diagnóstica cumple su objetivo nos apoyamos en estimaciones basadas en el modelo teórico. Las estimaciones están basadas en los valores que nos den los individuos. Para tener buenas estimaciones necesito buenas medidas. Un aspecto fundamental en estudios de investigación es garantizar la calidad de los procedimientos de medida. La calidad de una medida depende tanto de su fiabilidad como de su validez.\n\nLa fiabilidad indica hasta qué punto se obtienen los mismos valores al efectuar la medición en más de una ocasión, bajo condiciones similares.\n\nLa validez expresa el grado en el que realmente se mide el fenómeno de interés.\n\nQue una medida sea muy precisa no implica que sea necesariamente válida. Por ejemplo, si se realizan dos mediciones consecutivas a un paciente con una herramienta mal calibrada los valores obtenidos seguramente serán parecidos aunque inexactos. (Fisterra 2024)"
  },
  {
    "objectID": "tema_01/tema_01_1_introduccion.html#clasificación-de-estudios-para-la-evaluación-de-la-calidad-de-los-procedimientos-de-medidas",
    "href": "tema_01/tema_01_1_introduccion.html#clasificación-de-estudios-para-la-evaluación-de-la-calidad-de-los-procedimientos-de-medidas",
    "title": "1. Introducción ✓",
    "section": "Clasificación de estudios para la evaluación de la calidad de los procedimientos de medidas",
    "text": "Clasificación de estudios para la evaluación de la calidad de los procedimientos de medidas\n\nFiabilidad: comparación/variación consigo mismo. (concordancia intraobservador)\n\nConcordancia: comparación/variación con otro observador (interobservador) o entre métodos.\n\nCalibración: comparación/variación con un método estándar.\n\n\n\n\n\n\n\nFiabilidad, repetibilidad.\nMedidas fiables/repetibles/reproducibles.\nEstabilidad \\(\\Rightarrow\\) Individuos con caracteristicas similares darán lugar a medidas similares.\nEstudios de fiabilidad. Intentan evaluar como concuerdan las medidas obtenidas por un único método o instrumento. Se evalúa el error de medida del método mediante el estudio de la concordancia intramétodo, si las medidas concuerdan se puede decir que el método es repetible.\n\n\nConcordancia.\nSi la damos la vuelta podríamos pensar en independencia. Personas que funcionan de forma independiente. (no lo termino de pillar)\nEstudios de concordancia: se desea evaluar como concuerdan las medidas realizadas con el método cuya calidad se desea valorar con los obtenidos por otro método. Valoramos la concordancia entre métodos de medida con el objetivo de determinar si ambos son intercambiables.\nLas técnicas utilizadas para estudiar la concordancia varían según la naturaleza de las variables, dependiendo de si las medidas corresponden a una escala de medida cualitativa o cuantitativa.\nLa concordancia adquiere importancia cuando se desea conocer si con un método o instrumento nuevo, diferente al habitual, se obtienen resultados equivalentes de tal manera que eventualmente uno y otro puedan ser remplazados o intercambiados ya sea porque uno de ellos es más sencillo, menos costoso y por lo tanto más costo-efectivo, o porque uno de ellos resulta más seguro para el paciente, entre otras múltiples razones. En términos generales, la concordancia es el grado en que dos o más observadores, métodos, técnicas u observaciones están de acuerdo sobre el mismo fenómeno observado. La concordancia no evalúa la validez o la certeza sobre una u otra observación con relación a un estándar de referencia dado, sino cuán acordes están entre sí observaciones sobre el mismo fenómeno. (Camargo-Ramos y autores 2012)\n\n\nCalibración.\nNo solo queremos que no haya errores de medida. Cuando calibramos algo lo calibramos con un instrumento. Calibrar es comparar las medidas que tenemos con un método con otras medidas que están aprobadas por todos (gold standar).\nEstudios de calibración: pueden verse como caso particular de los estudios de concordancia entre métodos, cuando se compara un procedimiento de medida con los valores reales o de referencia (gold standard).\n\nEn el mundo clínico existe mucha confusión debido a que heredamos mucha terminología inglesa ya que a veces no tiene una equivalencia.\nLa medida de un individuo vendrá dada por características que podemos medir y por algo intrínseco del propio individuo. ¿Qué factores influyen para que haya resultados diferentes entre individuos con las mismas características? Esos factores son el error. Queremos medir esa parte intrínseca al individuo y que no podemos incluir en el modelo.\nEl error lo queremos controlar midiéndolo con diferentes metodologías. ¿El error es diferente dependiendo de la metodología?\n\\[\n\\displaylines{\n& \\text {Sea X el resultado de la prueba diagnóstica} \\\\\n&\\begin{array}{cccc}\n\\hline  \\text { Tipo de la variable X } & \\text { Objetivo de la prueba } & \\text { Índice o argumento de concordancia } \\\\\n\\hline\n\\text {Cualitativo} & \\text {La prueba clasfica} & \\text {Coeficiente kappa} \\\\\n\\text {Cuantitativo} & \\text {Tiene una unidad de medida} & \\text {Concordancia intraclase} \\\\\n&  & \\text {Método gráfico de Bland y Altman} \\\\\n\\hline\n\\end{array}\n}\n\\]\n\n\n\n\nCamargo-Ramos, C. M., y otros autores. 2012. «Evaluación de factores asociados al embarazo adolescente». Revista Colombiana de Obstetricia y Ginecología 61 (3): 256-62. http://www.scielo.org.co/pdf/rcog/v61n3/v61n3a09.pdf.\n\n\nFisterra. 2024. «La fiabilidad de las mediciones clínicas: análisis de la concordancia para variables numéricas». 2024. https://www.fisterra.com/formacion/metodologia-investigacion/la-fiabilidad-mediciones-clinicas-analisis-concordancia-para-variables-numericas/."
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#clasificaciones-binomiales",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#clasificaciones-binomiales",
    "title": "2. Análisis de concordancia en variables categóricas ✓",
    "section": "Clasificaciones binomiales",
    "text": "Clasificaciones binomiales\nCasos donde la prueba diagnóstica solo presenta dos resultados distintos.\nAnalizamos si dos agentes que clasifican a los individuos según el resultado de la p.d. concuerdan en sus opiniones. También me sirve la discordancia completa. (sé que un agente siempre clasifica lo contrario al otro agente)\nComo la p.d. depende del agente que me da la medida, tengo una variable aleatoria por cada agente.\nX = “clasificación dada por el observador A.”\nY = “clasificación dada por el observador B.”\nDado un individuo este es analizado por el agente A y por el agente B.\n\\[\n\\displaylines{\n& \\text {Medidas para cada inidividuo} \\\\\n&\\begin{array}{ccc}\n\\hline\n\\text{individuo k} & X_k & Y_k\n\\end{array}\n}\n\\]\nResumen de la información del modelo teórico.\nSuponemos distribución de probabilidad conjunta.\n\\[\n\\begin{array}{c|c c c c}\n    & & \\textbf{Y} & & \\\\\n    & \\textbf{Categoría} & \\textbf{1} & \\textbf{2} & \\textbf{Total} \\\\\n\\hline\n\\textbf{X} & \\textbf{1} & \\pi_{11} & \\pi_{12} & \\pi_{1\\cdot} \\\\\n           & \\textbf{2} & \\pi_{21} & \\pi_{22} & \\pi_{2\\cdot} \\\\\n\\hline\n& \\textbf{Total} & \\pi_{\\cdot1} & \\pi_{\\cdot2} & 1 \\\\\n\\end{array}\n\\]\n\\[\n\\begin{array}{}\n&\\pi_{.j} = (X,Y) \\epsilon {(1,1),(1,2),(2,1),(2,2)} \\\\\n\\pi_{ij} = \\text{p({X=i, Y=j})} & \\pi_{i.} = \\text{p({Y=j})} & \\pi_{.j} =  \\text{p({X=i})} \\\\\n\\end{array}\n\\]\nAcuerdo observado o Índice de concordancia global\nEs la aproximación a la concordancia más intuitiva. Expresa el porcentaje de coincidencia en la clasificación de ambos agentes.\nEl problema que plantea este índice básico es que una parte de ese acuerdo puede deberse exclusivamente al azar.\nMedir la concordancia es saber la probabilidad con la que los dos dan la misma clasificación.\n\nMedida de concordancia global: \\(\\Pi_0 = \\pi_{11} + \\pi_{22} = \\sum_{i=j} \\pi_{ij}\\)\n\nEjemplo 1.0.\nSe recogen las respuestas a las entrevistas 1 y 2 de manera independiente.\nEntrevista 1: ¿Consume usted suplementos vitamínicos?\nEntrevista 2: Responda si consume vitaminas sin contar sus aportes alimentarios\n\n\nCaso 1. Concordancia perfecta. {X=Y} o {X\\(\\neq\\)Y}\np({X=Y})=1\n\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.6 & 0 & 0.6 \\\\\n                      & \\textbf{Sí} & 0 & 0.4 & 0.4 \\\\\n\\hline\n& \\textbf{Total} & 0.6 & 0.4 & 1 \\\\\n\\end{array}\n\\]\n\nLa probabilidad de que un individuo conteste lo mismo en ambos cuestionarios es 1.\n\n\nCaso 2. Concordancia.\n\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{2} & \\textbf{Sí} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.4 & 0.2 & 0.6 \\\\\n                      & \\textbf{Sí} & 0.2 & 0.2 & 0.4 \\\\\n\\hline\n& \\textbf{Total} & 0.6 & 0.4 & 1 \\\\\n\\end{array}\n\\]\n\nLa probabilidad de dar la misma respuesta es 0.8.\n\n\nCaso 3. Sin concordancia.\n\n\nLas dos entrevistas concuerdan el 40 % de las ocasiones. Puede no haber asociación real entre las dos entrevistas porque toda la concordancia puede atribuirse al azar.\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.3 & 0.3 & 0.6 \\\\\n                      & \\textbf{Sí} & 0.3 & 0.1 & 0.4 \\\\\n\\hline\n& \\textbf{Total} & 0.6 & 0.4 & 1 \\\\\n\\end{array}\n\\]\n¿Cuál sería la distribución de las respuestas si los individuos responden tirando una moneda? (cara: no, cruz: sí)\np(X=“no”, Y=“sí”) = p(“sale cara primera moneda”, “sale cara segunda moneda”) = 0.5 * 0.5 = 0.25\np(X=“sí”, Y=“no”) = p(X=“no”, Y=“no”) = p(X=“sí”, Y=“sí”) = 0.25\n\\(\\pi_{0}\\) = 0.25 + 0.25 = 0.5\\(\\pi_{i} = \\pi_{1.} * \\pi_{.1} + \\pi_{2.} * \\pi_{.2}\\) = 0.5 * 0.5 + 0.5 * 0.5 (concordancia debida al azar)\n\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.25 & 0.25 & 0.5 \\\\\n                      & \\textbf{Sí} & 0.25 & 0.25 & 0.5 \\\\\n\\hline\n& \\textbf{Total} & 0.5 & 0.5 & 1 \\\\\n\\end{array}\n\\]\n\nEsta sería la máxima no concordancia, ya que si la diagonal principal acumula menos de la mitad de los casos podríamos medir una concordancia inversa.\n\nEl problema de \\(\\Pi_{0}\\) es que incluye concordancia que puede atribuirse al azar. Y concordar por azar no es concordar.\nY bailar de lejos no es bailar.\nSi los dos observadores clasificasen de forma independiente y, por tanto, totalmente al azar entre las dos categorías, la probabilidad de la concordancia \\(p(X=Y)\\) sería:\n\nMedida de concordancia debida al azar: \\(\\Pi_c = \\pi_{1.}\\pi_{.1} + \\pi_{2.}\\pi_{.2} = \\sum_{k=1,2} \\pi_{k.}\\pi_{.k}\\)\n\nA la concordancia global debo restarle la concordancia del azar.\nÍndice kappa (de Cohen)\nEste índice es una medida basada en resultados teóricos.\n\\[k = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c}\\] tal que \\[\\Pi_0 = \\pi_{11} + \\pi_{22} \\quad \\text{y} \\quad \\Pi_c = \\pi_{1.}\\pi_{.1} + \\pi_{2.}\\pi_{.2}\\]\nA la concordancia le quita la concordancia debida al azar y lo compara con la concordancia perfecto sin contar la concordancia al azar. Es decir, a la concordancia y a la concordancia perfecta les substrae la concordancia debida al azar y compara la concordancia sin el azar obtenida contra la máxima concordancia sin el azar posible.\nPodemos tener resultados negativos: el intervalor [-1,0) es sin corcondancia.\nEjemplo 1.0.\n\nCaso 1. Concordancia perfecta.\n\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.6 & 0 & 0.6 \\\\\n                      & \\textbf{Sí} & 0 & 0.4 & 0.4 \\\\\n\\hline\n& \\textbf{Total} & 0.6 & 0.4 & 1 \\\\\n\\end{array}\n\\]\n\nCuando tengo una concordancia perfecta, la concordancia debido al azar es 0.5.\n\\(\\Pi_c = \\pi_{1.}\\pi_{.1} + \\pi_{2.}\\pi_{.2} = 0.36 + 0.24 = 0.5\\)\nY POR EEEEEEEEEEEESO, usamos el índice kappa del señor Cohen.\n\n\n\nCaso 3. Sin corcondancia.\n\nSi la concordancia observada \\(\\Pi_0\\) coincide con la concordancia esperada por el azar \\(\\Pi_1\\) el índice kappa toma el valor 0.\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.25 & 0.25 & 0.5 \\\\\n                      & \\textbf{Sí} & 0.25 & 0.25 & 0.5 \\\\\n\\hline\n& \\textbf{Total} & 0.5 & 0.5 & 1 \\\\\n\\end{array}\n\\] \\(\\Pi_0 = \\pi_{11} + \\pi_{22} = 0.5\\) y \\(\\Pi_c = \\pi_{1.}\\pi_{.1} + \\pi_{2.}\\pi_{.2} = 0.5*0.5 + 0.5*0.5 = 0.5\\)\n\\(k = \\frac{0.5-0.5}{1-0.5} = 0\\)\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.3 & 0.3 & 0.6 \\\\\n                      & \\textbf{Sí} & 0.3 & 0.1 & 0.4 \\\\\n\\hline\n& \\textbf{Total} & 0.6 & 0.4 & 1 \\\\\n\\end{array}\n\\]\n\\(\\Pi_0 = \\pi_{11} + \\pi_{22} = 0.4\\) y \\(\\Pi_c = \\pi_{1.}\\pi_{.1} + \\pi_{2.}\\pi_{.2} = 0.6*0.6 + 0.4*0.4 = 0.52\\)\n\\(k = \\frac{0.4-0.52}{1-0.52} = -0.25\\)\n\nInterpretación del índice kappa (si \\(k\\)&gt;0)\nLos datos observados son un caso particular.\nLa probabilidad de acuerdo observado es una media ponderada del máximo acuerdo y del acuerdo debido al azar \\(\\Pi_c\\), siendo \\(k\\) el peso del máximo acuerdo.\n\n\\[\n\\displaylines{\nk = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c} \\\\ \\\\\nk * (1-\\Pi_{i}) = (\\Pi_{0} - \\Pi_{c}) \\\\ \\\\\n\\Pi_{0} = k + (1-k)*\\Pi_{c}\n}\n\\]\nEl acuerdo observado es una combinación de la concordania perfecta y concordania debida al azar. El ínidce kappa da más peso a uno u otro.\n\nEjemplo 1.1.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.04 & 0.06 & 0.10 \\\\\n                      & \\textbf{No pulmonía} & 0.10 & 0.80 & 0.90 \\\\\n\\hline\n& \\textbf{Total} & 0.14 & 0.86 & 1 \\\\\n\\end{array}\n\\]\n\ndata &lt;- matrix(c(0.04, 0.06, 0.10, 0.80), nrow = 2, byrow = TRUE,\n               dimnames = list(c(\"Pulmonía B\", \"No pulmonía B\"),\n                               c(\"Pulmonía A\", \"No pulmonía A\")))\ndata\n\n              Pulmonía A No pulmonía A\nPulmonía B          0.04          0.06\nNo pulmonía B       0.10          0.80\n\n\n\ndf &lt;- data.frame(col1 = c(0.04, 0.10),\n                 col2 = c(0.06, 0.80)\n)\ncolnames(df) &lt;- c(\"Pulmonía A\", \"No pulmonía A\")\nrownames(df) &lt;- c(\"Pulmonía B\", \"No pulmonía B\")\ndf\n\n              Pulmonía A No pulmonía A\nPulmonía B          0.04          0.06\nNo pulmonía B       0.10          0.80\n\n\n\\(\\Pi_{0} = \\text{\"concordancia absoluta\"} = 0.04 + 0.80 = 0.84\\)\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n\n[1] 0.84\n\n\n\n(pi_0 &lt;- df[1,1] + df[2,2])\n\n[1] 0.84\n\n\n\n(pi_0 &lt;- df[\"Pulmonía B\",\"Pulmonía A\"] + df[\"No pulmonía B\",\"No pulmonía A\"])\n\n[1] 0.84\n\n\n\\(\\Pi_{1} = \\text{\"concordancia debida al azar\"} = 0.10*0.14 + 0.86*0.90 = 0.788\\)\n\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n\n[1] 0.788\n\n\n\n(pi_1 &lt;- sum(df[1]) * sum(df[1,])  + sum(df[2]) * sum(df[2,]))\n\n[1] 0.788\n\n\n\n(pi_1 &lt;- sum(data[1,])*sum(data[,1]) + sum(data[2,])*sum(data[,2]))\n\n[1] 0.788\n\n\nInterpretación.]{underline}\nLa concordancia observada en los informes (84%) está compuesta por un 24,5% de la concordancia máxima y un 78,8% de la esperada al azar.\nClasificaciones multinomiales\n\\[\n\\begin{array}{c|c c c c}\n    & & \\textbf{Y} & & \\\\\n    & \\textbf{Categoría} & \\textbf{1} & \\textbf{...} & \\textbf{t} & \\textbf{Total} \\\\\n\\hline\n\\textbf{X} & \\textbf{1} & \\pi_{11} & ... & \\pi_{1t} & \\pi_{1\\cdot} \\\\\n           & \\textbf{2} & \\pi_{21} & ... & \\pi_{2t} & \\pi_{2\\cdot} \\\\\n           & \\textbf{...} & ... & ... & ... & ... \\\\\n           & \\textbf{t} & \\pi_{2t} & ... & \\pi_{tt} & \\pi_{t\\cdot} \\\\\n\\hline\n& \\textbf{Total} & \\pi_{\\cdot1} & ... & \\pi_{\\cdot t} & 1 \\\\\n\\end{array}\n\\]\n\nMedida de concordancia global: \\(\\Pi_0 = \\sum_{i=1}^{t} \\pi_{ii}\\)\n\n\nMedida de concordancia debida al azar: \\(\\Pi_c = \\sum_{i=1}^{t} \\pi_{i.}\\pi_{.i}\\)\n\n\nÍndice kappa: \\(k = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c}\\)\n\nHay veces que nos interesa saber si la concordancia es mayor en unas categorías o en otras. Por ejemplo, en la categoría grave hay muchas concordancia pero en las categorías leve y media no. En este caso se suele estandarizar para tener dos categorías y medir concordancia para cada categoría.\nEjemplo 1.2.\n\ndf &lt;- data.frame(rbind(c(0.1125, 0.1, 0.0375),\n                       c(0.1125, 0.3625, 0.0625),\n                       c(0, 0.0375, 0.175))\n)\ncolnames(df) &lt;- c(\"Leve\", \"Moderada\", \"Grave\")\nrownames(df) &lt;- c(\"Leve\", \"Moderada\", \"Grave\")\ndf\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\n\n\n\ndf_extended &lt;- rbind(df, \"Total\" = colSums(df))\ndf_extended\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\nTotal    0.2250   0.5000 0.2750\n\n\n\ndf_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\ndf_extended\n\n           Leve Moderada  Grave  Total\nLeve     0.1125   0.1000 0.0375 0.2500\nModerada 0.1125   0.3625 0.0625 0.5375\nGrave    0.0000   0.0375 0.1750 0.2125\nTotal    0.2250   0.5000 0.2750 1.0000\n\n\nMedida de concordancia global\n\n(pi_0 &lt;- sum(diag(as.matrix(df))))\n\n[1] 0.65\n\n\nMedida de concordancia debida al azar\n\npi_1 &lt;- 0\nt &lt;- dim(df_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_extended[i,t]*df_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\n[1] 0.3834375\n\n\nÍndice kappa\n\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] 0.4323365\n\n\nEl índice kappa no mide la distancia entre las discordancias. No es lo mismo decir (X=7, Y=8) que (X=7, Y=2).\nComparando cada categoría con el resto\n\ndf\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\n\n\n\nCódigocat &lt;- \"Leve\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Leve\"\n        Leve  Otras Total\nLeve  0.1125 0.1375  0.25\nOtras 0.1125 0.6375  0.75\nTotal 0.2250 0.7750  1.00\n[1] \"Medida de concordancia global:\"\n[1] 0.75\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.6375\n[1] \"Índice kappa:\"\n[1] 0.3103448\n\n\n\nCódigocat &lt;- \"Moderada\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Moderada\"\n         Moderada Otras  Total\nModerada   0.3625 0.175 0.5375\nOtras      0.1375 0.325 0.4625\nTotal      0.5000 0.500 1.0000\n[1] \"Medida de concordancia global:\"\n[1] 0.6875\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.5\n[1] \"Índice kappa:\"\n[1] 0.375\n\n\n\nCódigocat &lt;- \"Grave\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Grave\"\n      Grave  Otras  Total\nGrave 0.175 0.0375 0.2125\nOtras 0.100 0.6875 0.7875\nTotal 0.275 0.7250 1.0000\n[1] \"Medida de concordancia global:\"\n[1] 0.8625\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.629375\n[1] \"Índice kappa:\"\n[1] 0.6290051\n\n\nHasta ahora hemos hablado de probabilidades ya que estamos con el modelo teórico. Cuando trabajamos con datos de una muestra hablaremos de porcentajes. \\(\\Rightarrow\\) “Este % es consecuencia de un acuerdo de xx y un azar de xx”\n\nEl índice kappa tiene una distribución teórica y lo estimo según una muestra. Con una poca información (la muestra) doy un intervalo. No doy intervalos de confianza para valores desconocidos sino para las estimaciones.\n\nInconvenientes del índice kappa\nEjemplo 1.1.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.04 & 0.06 & 0.10 \\\\\n                      & \\textbf{No pulmonía} & 0.10 & 0.80 & 0.90 \\\\\n\\hline\n& \\textbf{Total} & 0.14 & 0.86 & 1 \\\\\n\\end{array}\n\\]\nLa prevalencia de pulmonía es baja: 014 para el radiólogo A y 0.1 para el rediólogo B.\nAmbas marginales desequilibradas: la prevalencia observada dista mucho de 0.5 (a favor de los diagnósticos negativos)\nEl bajo valor del índice kappa se explica porque nos encontramos en el peor de los escenarios: baja prevalencia y marginales desequilibradas.\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.788\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.245283\n\nEl valor del índice kappa depende de la prevalencia de la característica observada.\nEjemplo 1.1.a.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.30 & 0.06 & 0.36 \\\\\n                      & \\textbf{No pulmonía} & 0.10 & 0.54 & 0.64 \\\\\n\\hline\n& \\textbf{Total} & 0.40 & 0.60 & 1 \\\\\n\\end{array}\n\\]\nAumentamos la prevalencia: 0.40 para el radiólogo A y 0.36 para el rediólogo B.\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.528\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.6610169\n\n\nCuanto más cercana a 0,5 sea la prevalencia (más equilibradas estén las marginales) mayor es el índice kappa, para igual probabilidad de acuerdos observados.\n\n\\[\\Downarrow\\]\n\nPrevalencias muy altas o muy bajas penalizan el índice kappa.\n\nEl valor del índice kappa depende de la simetría y homogeneidad de las marginales.\nEn el primer caso el comportamiento de los agentes es homogéneo: pues ambos emiten informes positivos con mayor frecuencia.\nEn el segundo caso el comportamiento de los agentes es heterogéneo y asimétrico.\nEn ambos casos los diagnósticos del agente A distan 0.2 de 0.5.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.45 & 0.15 & 0.60 \\\\\n                      & \\textbf{No pulmonía} & 0.25 & 0.15 & 0.40 \\\\\n\\hline\n& \\textbf{Total} & 0.70 & 0.30 & 1 \\\\\n&  & 0.50+0.20 & 0.50-0.20 &  \\\\\n\\end{array}\n\\]\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.54\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.6521739\n\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.25 & 0.35 & 0.60 \\\\\n                      & \\textbf{No pulmonía} & 0.05 & 0.35 & 0.40 \\\\\n\\hline\n& \\textbf{Total} & 0.30 & 0.70 & 1 \\\\\n&  & 0.50-0.20 & 0.50+0.20 &  \\\\\n\\end{array}\n\\]\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.46\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.7037037\n\nEl valor del índice kappa depende del equilibrio de las marginales.\nCuanto mayor sea la diferencia de la prevalencia observada de cada agente respecto de 0.5 mayor es el índice kappa. (incluso para un mismo valor de acuerdos observados).\nÍndice kappa ponderado.\nSólo tiene sentido para variables ordinales.\nEstá diseñado para recoger la idea de que algunas discordancias son más severas que otras y, por tanto, asigna pesos que representan la importancia entre los desacuerdos. El máximo peso se da a la concordancia perfecta y pesos proporcionalmente menores según la importancia del desacuerdo.\nNo tiene la misma importancia un desacuerdo en la clasificación entre las categorías leve y moderada que entre leve y grave, obviamente la última representa un mayor desacuerdo que la primera.\n\nMedida de concordancia global: \\(\\Pi_0 = \\sum_{i=1}^{t} \\sum_{j=1}^{t} w_{ij} \\pi_{ij}\\)\n\n\nMedida de concordancia debida al azar: \\(\\Pi_c = \\sum_{i=1}^{t} \\sum_{j=1}^{t} w_{ij} \\pi_{i.}\\pi_{.j}\\)\n\n\nÍndice kappa: \\(k_{w} = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c}\\)\n\nLos pesos satisfacen:\n\\[\nw_{ij} = \\begin{cases}\nw_{ii} = 1 \\\\\n0 \\le w_{ij} \\le 1 \\\\\nw_{ij} = w_{ji}\n\\end{cases}\n\\]\n\\[\nw_{ij} = \\begin{cases}\n1, \\hspace{1em} i = j \\\\\n0, \\hspace{1em} i \\neq j \\\\\n\\end{cases} \\hspace{1em} \\text{sii} \\hspace{1em} k_{w} = k \\\\\n\\] \\[\n\\text{si t = 2} \\Rightarrow k_{w} = k\n\\]\nPonderaciones.\n\nCicchetti-Allison (1971) \\(w_{ij} = 1 - \\frac{|i-j|}{t-1}\\)\nFleiss-Cohen (1973) \\(w_{ij} = 1 - \\frac{(i-j)^2}{(t-1)^2}\\)\nTratamiento muestral de los índices kappa\nSea:\n\nn el número de individuos en la muestra.\n\n\\(p_ij\\) la proporción de individuos asignados a la categoría i por el observador X y a la categoría j por el observador Y, donde i, j =1,2,…,t.\n\n\\(p_{i.}\\) la proporción marginal de que un sujeto sea asignado a la clase i por el observador X (análogo para \\(p_{.j}\\), la clase j y el observador Y), donde i, j =1,2,…,t.\n\n\n\\(p_{i.}\\) = \\(p_{i1}\\) + … + \\(p_{it}\\)\n\n\n\\(p_{.j}\\) = \\(p_{1j}\\) + … + \\(p_{tj}\\)\n\n\n\n\nTenemos:\n\\[E(p_{ij}) = \\pi_{ij}, \\quad E(p_{i.}) = \\pi_{i.}, \\quad E(p_{.j}) = \\pi_{.j}\\]\nEl estimador del índice de kappa ponderado es:\n\\[\n\\kappa_w = \\frac{P_0 - P_c}{1 - P_c}, \\quad P_0 = \\sum_{i=1}^{t}\\sum_{j=1}^{t} w_{ij}p_{ij}, \\quad P_c = \\sum_{i=1}^{t}\\sum_{j=1}^{t} w_{ij}p_{i.}p_{.j}\n\\]\nEs un estimador con una distribución asintótica normal.\n\\[ \\kappa_w ~ N(\\kappa_w, \\sigma_{\\kappa_w})\\]\nTengo una distribucion normal centrada en k y al elegir un dato al azar (o sea, de la muestra aleatoria) puede tener un valor muy cercano o muy lejano de la media.\nLa varianza también es una estimación hecha con lo que yo he visto. Es otra estimación más. Esto hace que vayamos arrastrando aproximaciones AKA posibles errores.\n\\[\n\\displaylines{\n\\sigma^2_{\\kappa_w} = \\frac{\\sum_i \\sum_j \\pi_{ij} \\left[ w_{ij} - (\\overline{w}_{i \\cdot} + \\overline{w}_{\\cdot j})(1 - \\kappa_w) \\right]^2 - \\left[ \\kappa_w - \\Pi_c (1 - \\kappa_w) \\right]^2}{n (1 - \\Pi_c)^2} \\\\\n\\overline{w}_{i \\cdot} = \\sum_{j=1}^t \\pi_{\\cdot j} w_{ij} \\\\\n\\overline{w}_{\\cdot j} = \\sum_{i=1}^t \\pi_{i \\cdot} w_{ij}\n}\n\\]\nEl estimador de esta varianza su contrapartida muestral, sustituyendo probabilidades por proporciones y el índice kappa por su estimador.\n\\[\n\\displaylines{\n\\hat{\\sigma}^2_{\\kappa_w} = s^2_{\\kappa_w} = \\frac{\\sum_i \\sum_j p_{ij} \\left[ w_{ij} - (\\hat{w}_{i \\cdot} + \\hat{w}_{\\cdot j})(1 - \\hat{\\kappa}_w) \\right]^2 - \\left[ \\hat{\\kappa}_w - P_c (1 - \\hat{\\kappa}_w) \\right]^2}{n (1 - P_c)^2} \\\\\n\\hat{w}_{i \\cdot} = \\sum_{j=1}^t p_{\\cdot j} w_{ij} \\\\\n\\hat{w}_{\\cdot j} = \\sum_{i=1}^t p_{i \\cdot} w_{ij}\n}\n\\] Límites de confianza para el índice kappa con nivel de confianza \\(\\alpha\\):\n\\[ \\hat{\\kappa}_w +- z_\\alpha s_{\\hat{\\kappa}_w}\\]\nEscala de valoración del índice Kappa.\n\\[\n\\begin{array}{c|c}\n\\textbf{Kappa} & \\textbf{Grado de concordancia} \\\\\n\\hline\n&lt; 0.00 & \\text{Sin concordancia} \\\\\n0.00 - 0.20  & \\text{Insignificante} \\\\\n0.20 - 0.40  & \\text{Discreta} \\\\\n0.40 - 0.60  & \\text{Moderada} \\\\\n0.60 - 0.80  & \\text{Sustancial} \\\\\n0.80 - 1  & \\text{Casi perfecta} \\\\\n\\end{array}\n\\]\nEjemplo 1.3.\nDos radiólogos independientes informan de la presencia o ausencia de neumonía en 100 radiografías, siendo los resultados los siguientes.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Neumonía} & \\textbf{No neumonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Neumonía} & 4 & 6 & 10 \\\\\n                      & \\textbf{No neumonía} & 10 & 80 & 90 \\\\\n\\hline\n& \\textbf{Total} & 14 & 86 & 100 \\\\\n\\end{array}\n\\]\n\\(P_0 = \\frac{4+80}{100} = 0.84 \\quad \\quad P_c = \\frac{10*14 + 90*86}{100^2} = 0.788\\)\\(\\hat{\\kappa_w} = \\frac{0.84 - 0.788}{1 - 0.788} = 0.245\\)\\(s^2_{\\kappa_w} = 0.018 \\quad \\quad z_{0.05}s_{\\kappa_w} = 1.65 \\sqrt{0.018} = 0.22\\)\nSe podría hacer también un aproximación asintótica pero conlleva arrastrar una simulacion más.\nLa estimación puntual la tengo. Como no tengo una muestra más grande puedo remuestrear.\nLos estimadores por intervalo miden la precision de la estimación puntual. Cómo de buena es esa estimación puntual para representar otra posible muestra que podríamos haber obtenido.\nUna estimación puntual que se encuentra en alguno de los intervalos de Landis y Koch.\nLa validez de la estimacion hay que refrentarla con un IC. La estimación puntual de 0.245 se considera discreta y el \\(IC_90 = (0.025, 0.465)\\) abarca varios niveles, desde discreta hasta sustancial. Si el IC estuviera solo en el mismo intervalo que la estimación puntual diríamos que el IC respalda/valida la estimacion puntual. Al abarcar más de un nivel deberemos explicarlo.\nEn el 90 % de estudios similares a este tenemos una concordancia que iría desde insignificante a moderada. La validaz de nuestra estimación no es super cool, es cuestionable, ya que hay otros estudios donde podemos encontrar una concordancia diferente a la que hemos calculado en esta muestra.\nH0: las opiniones son independientes (que no hay relación entre las opiniones) (si hay una concordancia que sea casual) AKA \\(\\rho\\) = 0.\nPara poder admitir \\(\\rho\\) = 0 el IC tendría que darlo como un valor posible y en este caso no lo da. Rechazamos que haya un criterio diferente. (juzgamos que los dos puntúen por igual)\nEjemplo 1.4.\nDos psiquiatras evaluaron a 129 pacientes que habían sido diagnosticados previamente como clínicamente deprimidos. Las categorías de clasificación fueron: 0 para no deprimido, 1 para moderadamente deprimido y 2 para clínicamente deprimido. La tabla siguiente muestra los resultados de la clasificación realizada por los dos psiquiatras.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Psiquiatra 1} \\\\\n& & \\textbf{0} & \\textbf{1} & \\textbf{2} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Psiquiatra 2} & \\textbf{0} & 11 & 2 & 19 & 32 \\\\\n                      & \\textbf{1} & 1 & 3 & 3 & 7 \\\\\n                      & \\textbf{2} & 0 & 8 & 82 & 90 \\\\\n\\hline\n& \\textbf{Total} & 12 & 13 & 104 & 129 \\\\\n\\end{array}\n\\]\nDatos.\n\nCódigolibrary(psych)\n(A &lt;- matrix(c(11,2,19,1,3,3,0,8,82),ncol=3,byrow=TRUE))\n\n     [,1] [,2] [,3]\n[1,]   11    2   19\n[2,]    1    3    3\n[3,]    0    8   82\n\n\nÍndice kappa en R por defecto.\nEn la diagonal pone peso 0 (yo quiero que en la diagonal tiene 1 y conforme me alejo lo voy bajando).\nÍndice kappa sin ponderar.\nLe damos importancia a todo lo que coincide pero no le damos peso a la cercanía entre dos diagnósticos. La elección por el kappa ponderado es crucial cuando los niveles de clasificación son mayores a dos niveles.\nPesos Ciccetti y Allison.\n\nCódigo(w_CA&lt;- matrix(c( 1,0.5,0, 0.5,1,0.5,0,0.5,1),ncol=3))\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.5  0.0\n[2,]  0.5  1.0  0.5\n[3,]  0.0  0.5  1.0\n\n\nEstimaciones de kappa.\n\nCódigokappa1 &lt;- cohen.kappa(A, w=w_CA, n.obs=sum(A), alpha=0.1)\nstr(kappa1)\n## List of 11\n##  $ kappa         : num 0.375\n##  $ weighted.kappa: num 0.402\n##  $ n.obs         : num 129\n##  $ agree         : num [1:3, 1:3] 0.08527 0.00775 0 0.0155 0.02326 ...\n##  $ weight        : num [1:3, 1:3] 1 0.5 0 0.5 1 0.5 0 0.5 1\n##  $ var.kappa     : num 0.00622\n##  $ var.weighted  : num 0.00688\n##  $ confid        : num [1:2, 1:3] 0.245 0.265 0.375 0.402 0.504 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:2] \"unweighted kappa\" \"weighted kappa\"\n##   .. ..$ : chr [1:3] \"lower\" \"estimate\" \"upper\"\n##  $ plevel        : num 0.1\n##  $ bad           : logi FALSE\n##  $ Call          : language cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels,      w.exp = w.exp)\n##  - attr(*, \"class\")= chr [1:2] \"psych\" \"kappa\"\n\n\n\nkappa1$kappa\n## [1] 0.3745225\nkappa1$weighted.kappa\n## [1] 0.4018192\nkappa1$var.kappa\n## [1] 0.006221038\nkappa1$var.weighted\n## [1] 0.006884677\nkappa1$confid\n##                      lower  estimate     upper\n## unweighted kappa 0.2447870 0.3745225 0.5042579\n## weighted kappa   0.2653391 0.4018192 0.5382992\n\nPesos de Fleiss y Cohen.\n\n(w_FC&lt;- matrix(c( 1,0.75,0,0.75,1,0.75,0,0.75,1),ncol=3))\n\n     [,1] [,2] [,3]\n[1,] 1.00 0.75 0.00\n[2,] 0.75 1.00 0.75\n[3,] 0.00 0.75 1.00\n\n\nEstimaciones de kappa.\n\nCódigokappa2 &lt;- cohen.kappa(A, w=w_FC, n.obs=sum(A),alpha=0.1)\nstr(kappa2)\n## List of 11\n##  $ kappa         : num 0.375\n##  $ weighted.kappa: num 0.42\n##  $ n.obs         : num 129\n##  $ agree         : num [1:3, 1:3] 0.08527 0.00775 0 0.0155 0.02326 ...\n##  $ weight        : num [1:3, 1:3] 1 0.75 0 0.75 1 0.75 0 0.75 1\n##  $ var.kappa     : num 0.00622\n##  $ var.weighted  : num 0.00796\n##  $ confid        : num [1:2, 1:3] 0.245 0.274 0.375 0.42 0.504 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:2] \"unweighted kappa\" \"weighted kappa\"\n##   .. ..$ : chr [1:3] \"lower\" \"estimate\" \"upper\"\n##  $ plevel        : num 0.1\n##  $ bad           : logi FALSE\n##  $ Call          : language cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels,      w.exp = w.exp)\n##  - attr(*, \"class\")= chr [1:2] \"psych\" \"kappa\"\n\n\n\nkappa2$weighted.kappa\n## [1] 0.4203694\nkappa2$var.weighted\n## [1] 0.007955659\nkappa2$confid\n##                      lower  estimate     upper\n## unweighted kappa 0.2447870 0.3745225 0.5042579\n## weighted kappa   0.2736575 0.4203694 0.5670813\n\n\\[\n\\begin{array}{c|c}\n\\textbf{Estadístico} & \\textbf{Estimación} & \\textbf{Varianza estimada} & LI_{90} \\\\\n\\hline\n\\text{Kappa sin ponderar} & 0.3745225 & 0.006221038 & 0.2447870 \\\\\n\\text{Kappa ponderado (Ciccetti-Allison)} & 0.4018192 & 0.006884677 & 0.2653391 \\\\\n\\text{Kappa ponderado (Fleiss-Cohen)} & 04203694 & 0007955659 & 0.2736575 \\\\\n\\end{array}\n\\]\nEstamos teniendo resultados 0.3, 0.4. Las estimaciones puntuales nos incitan a concordancias moderadas. No podemos decir que no tengan un criterio común. algo tienen.\n¿Qué pasha cuando tenemos que analizar la validez de una estimación pero tenemos una muestra pequeña? Po a la metodología booststrap\nNos vamos a Tema 3, página 55.\nDepués de remuestrar tengo las 23 radiografías con la clasificación de los dos radiólogos cuando remuestre.\nObtengo muestras bootstrap con 23 radiografias pero la clasificación es la misma para cada radiografía.\nCalculo el kappa \\(\\Rightarrow\\) calculo las diferencias \\(\\Rightarrow\\) Despercio las más alejadas \\(\\Rightarrow\\) Me quedo con el 95 % central.\nSi hay más de 2 observadores \\(\\Rightarrow\\) Índice de Kappa para múltiples observadores (Fleiss JL Statistical Methods for Rates and Proportions, 2003)\n\n\n\n      \n         1. Introducción ✓\n                \n  \n  \n      \n        3. Análisis de concordancia en variables numéricas ✓"
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#clasificaciones-multinomiales",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#clasificaciones-multinomiales",
    "title": "2. Análisis de concordancia en variables categóricas ✓",
    "section": "Clasificaciones multinomiales",
    "text": "Clasificaciones multinomiales\n\\[\n\\begin{array}{c|c c c c}\n    & & \\textbf{Y} & & \\\\\n    & \\textbf{Categoría} & \\textbf{1} & \\textbf{...} & \\textbf{t} & \\textbf{Total} \\\\\n\\hline\n\\textbf{X} & \\textbf{1} & \\pi_{11} & ... & \\pi_{1t} & \\pi_{1\\cdot} \\\\\n           & \\textbf{2} & \\pi_{21} & ... & \\pi_{2t} & \\pi_{2\\cdot} \\\\\n           & \\textbf{...} & ... & ... & ... & ... \\\\\n           & \\textbf{t} & \\pi_{2t} & ... & \\pi_{tt} & \\pi_{t\\cdot} \\\\\n\\hline\n& \\textbf{Total} & \\pi_{\\cdot1} & ... & \\pi_{\\cdot t} & 1 \\\\\n\\end{array}\n\\]\n\nMedida de concordancia global: \\(\\Pi_0 = \\sum_{i=1}^{t} \\pi_{ii}\\)\n\n\nMedida de concordancia debida al azar: \\(\\Pi_c = \\sum_{i=1}^{t} \\pi_{i.}\\pi_{.i}\\)\n\n\nÍndice kappa: \\(k = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c}\\)\n\nHay veces que nos interesa saber si la concordancia es mayor en unas categorías o en otras. Por ejemplo, en la categoría grave hay muchas concordancia pero en las categorías leve y media no. En este caso se suele estandarizar para tener dos categorías y medir concordancia para cada categoría.\nEjemplo 1.2.\n\ndf &lt;- data.frame(rbind(c(0.1125, 0.1, 0.0375),\n                       c(0.1125, 0.3625, 0.0625),\n                       c(0, 0.0375, 0.175))\n)\ncolnames(df) &lt;- c(\"Leve\", \"Moderada\", \"Grave\")\nrownames(df) &lt;- c(\"Leve\", \"Moderada\", \"Grave\")\ndf\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\n\n\n\ndf_extended &lt;- rbind(df, \"Total\" = colSums(df))\ndf_extended\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\nTotal    0.2250   0.5000 0.2750\n\n\n\ndf_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\ndf_extended\n\n           Leve Moderada  Grave  Total\nLeve     0.1125   0.1000 0.0375 0.2500\nModerada 0.1125   0.3625 0.0625 0.5375\nGrave    0.0000   0.0375 0.1750 0.2125\nTotal    0.2250   0.5000 0.2750 1.0000\n\n\nMedida de concordancia global\n\n(pi_0 &lt;- sum(diag(as.matrix(df))))\n\n[1] 0.65\n\n\nMedida de concordancia debida al azar\n\npi_1 &lt;- 0\nt &lt;- dim(df_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_extended[i,t]*df_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\n[1] 0.3834375\n\n\nÍndice kappa\n\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] 0.4323365\n\n\nEl índice kappa no mide la distancia entre las discordancias. No es lo mismo decir (X=7, Y=8) que (X=7, Y=2).\nComparando cada categoría con el resto\n\ndf\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\n\n\n\nCódigocat &lt;- \"Leve\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Leve\"\n        Leve  Otras Total\nLeve  0.1125 0.1375  0.25\nOtras 0.1125 0.6375  0.75\nTotal 0.2250 0.7750  1.00\n[1] \"Medida de concordancia global:\"\n[1] 0.75\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.6375\n[1] \"Índice kappa:\"\n[1] 0.3103448\n\n\n\nCódigocat &lt;- \"Moderada\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Moderada\"\n         Moderada Otras  Total\nModerada   0.3625 0.175 0.5375\nOtras      0.1375 0.325 0.4625\nTotal      0.5000 0.500 1.0000\n[1] \"Medida de concordancia global:\"\n[1] 0.6875\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.5\n[1] \"Índice kappa:\"\n[1] 0.375\n\n\n\nCódigocat &lt;- \"Grave\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Grave\"\n      Grave  Otras  Total\nGrave 0.175 0.0375 0.2125\nOtras 0.100 0.6875 0.7875\nTotal 0.275 0.7250 1.0000\n[1] \"Medida de concordancia global:\"\n[1] 0.8625\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.629375\n[1] \"Índice kappa:\"\n[1] 0.6290051\n\n\nHasta ahora hemos hablado de probabilidades ya que estamos con el modelo teórico. Cuando trabajamos con datos de una muestra hablaremos de porcentajes. \\(\\Rightarrow\\) “Este % es consecuencia de un acuerdo de xx y un azar de xx”\n\nEl índice kappa tiene una distribución teórica y lo estimo según una muestra. Con una poca información (la muestra) doy un intervalo. No doy intervalos de confianza para valores desconocidos sino para las estimaciones."
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#inconvenientes-del-índice-kappa",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#inconvenientes-del-índice-kappa",
    "title": "2. Análisis de concordancia en variables categóricas ✓",
    "section": "Inconvenientes del índice kappa",
    "text": "Inconvenientes del índice kappa\nEjemplo 1.1.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.04 & 0.06 & 0.10 \\\\\n                      & \\textbf{No pulmonía} & 0.10 & 0.80 & 0.90 \\\\\n\\hline\n& \\textbf{Total} & 0.14 & 0.86 & 1 \\\\\n\\end{array}\n\\]\nLa prevalencia de pulmonía es baja: 014 para el radiólogo A y 0.1 para el rediólogo B.\nAmbas marginales desequilibradas: la prevalencia observada dista mucho de 0.5 (a favor de los diagnósticos negativos)\nEl bajo valor del índice kappa se explica porque nos encontramos en el peor de los escenarios: baja prevalencia y marginales desequilibradas.\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.788\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.245283\n\nEl valor del índice kappa depende de la prevalencia de la característica observada.\nEjemplo 1.1.a.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.30 & 0.06 & 0.36 \\\\\n                      & \\textbf{No pulmonía} & 0.10 & 0.54 & 0.64 \\\\\n\\hline\n& \\textbf{Total} & 0.40 & 0.60 & 1 \\\\\n\\end{array}\n\\]\nAumentamos la prevalencia: 0.40 para el radiólogo A y 0.36 para el rediólogo B.\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.528\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.6610169\n\n\nCuanto más cercana a 0,5 sea la prevalencia (más equilibradas estén las marginales) mayor es el índice kappa, para igual probabilidad de acuerdos observados.\n\n\\[\\Downarrow\\]\n\nPrevalencias muy altas o muy bajas penalizan el índice kappa.\n\nEl valor del índice kappa depende de la simetría y homogeneidad de las marginales.\nEn el primer caso el comportamiento de los agentes es homogéneo: pues ambos emiten informes positivos con mayor frecuencia.\nEn el segundo caso el comportamiento de los agentes es heterogéneo y asimétrico.\nEn ambos casos los diagnósticos del agente A distan 0.2 de 0.5.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.45 & 0.15 & 0.60 \\\\\n                      & \\textbf{No pulmonía} & 0.25 & 0.15 & 0.40 \\\\\n\\hline\n& \\textbf{Total} & 0.70 & 0.30 & 1 \\\\\n&  & 0.50+0.20 & 0.50-0.20 &  \\\\\n\\end{array}\n\\]\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.54\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.6521739\n\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.25 & 0.35 & 0.60 \\\\\n                      & \\textbf{No pulmonía} & 0.05 & 0.35 & 0.40 \\\\\n\\hline\n& \\textbf{Total} & 0.30 & 0.70 & 1 \\\\\n&  & 0.50-0.20 & 0.50+0.20 &  \\\\\n\\end{array}\n\\]\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.46\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.7037037\n\nEl valor del índice kappa depende del equilibrio de las marginales.\nCuanto mayor sea la diferencia de la prevalencia observada de cada agente respecto de 0.5 mayor es el índice kappa. (incluso para un mismo valor de acuerdos observados)."
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#índice-kappa-ponderado.",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#índice-kappa-ponderado.",
    "title": "2. Análisis de concordancia en variables categóricas ✓",
    "section": "Índice kappa ponderado.",
    "text": "Índice kappa ponderado.\nSólo tiene sentido para variables ordinales.\nEstá diseñado para recoger la idea de que algunas discordancias son más severas que otras y, por tanto, asigna pesos que representan la importancia entre los desacuerdos. El máximo peso se da a la concordancia perfecta y pesos proporcionalmente menores según la importancia del desacuerdo.\nNo tiene la misma importancia un desacuerdo en la clasificación entre las categorías leve y moderada que entre leve y grave, obviamente la última representa un mayor desacuerdo que la primera.\n\nMedida de concordancia global: \\(\\Pi_0 = \\sum_{i=1}^{t} \\sum_{j=1}^{t} w_{ij} \\pi_{ij}\\)\n\n\nMedida de concordancia debida al azar: \\(\\Pi_c = \\sum_{i=1}^{t} \\sum_{j=1}^{t} w_{ij} \\pi_{i.}\\pi_{.j}\\)\n\n\nÍndice kappa: \\(k_{w} = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c}\\)\n\nLos pesos satisfacen:\n\\[\nw_{ij} = \\begin{cases}\nw_{ii} = 1 \\\\\n0 \\le w_{ij} \\le 1 \\\\\nw_{ij} = w_{ji}\n\\end{cases}\n\\]\n\\[\nw_{ij} = \\begin{cases}\n1, \\hspace{1em} i = j \\\\\n0, \\hspace{1em} i \\neq j \\\\\n\\end{cases} \\hspace{1em} \\text{sii} \\hspace{1em} k_{w} = k \\\\\n\\] \\[\n\\text{si t = 2} \\Rightarrow k_{w} = k\n\\]\nPonderaciones.\n\nCicchetti-Allison (1971) \\(w_{ij} = 1 - \\frac{|i-j|}{t-1}\\)\nFleiss-Cohen (1973) \\(w_{ij} = 1 - \\frac{(i-j)^2}{(t-1)^2}\\)"
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#tratamiento-muestral-de-los-índices-kappa",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#tratamiento-muestral-de-los-índices-kappa",
    "title": "2. Análisis de concordancia en variables categóricas ✓",
    "section": "Tratamiento muestral de los índices kappa",
    "text": "Tratamiento muestral de los índices kappa\nSea:\n\nn el número de individuos en la muestra.\n\n\\(p_ij\\) la proporción de individuos asignados a la categoría i por el observador X y a la categoría j por el observador Y, donde i, j =1,2,…,t.\n\n\\(p_{i.}\\) la proporción marginal de que un sujeto sea asignado a la clase i por el observador X (análogo para \\(p_{.j}\\), la clase j y el observador Y), donde i, j =1,2,…,t.\n\n\n\\(p_{i.}\\) = \\(p_{i1}\\) + … + \\(p_{it}\\)\n\n\n\\(p_{.j}\\) = \\(p_{1j}\\) + … + \\(p_{tj}\\)\n\n\n\n\nTenemos:\n\\[E(p_{ij}) = \\pi_{ij}, \\quad E(p_{i.}) = \\pi_{i.}, \\quad E(p_{.j}) = \\pi_{.j}\\]\nEl estimador del índice de kappa ponderado es:\n\\[\n\\kappa_w = \\frac{P_0 - P_c}{1 - P_c}, \\quad P_0 = \\sum_{i=1}^{t}\\sum_{j=1}^{t} w_{ij}p_{ij}, \\quad P_c = \\sum_{i=1}^{t}\\sum_{j=1}^{t} w_{ij}p_{i.}p_{.j}\n\\]\nEs un estimador con una distribución asintótica normal.\n\\[ \\kappa_w ~ N(\\kappa_w, \\sigma_{\\kappa_w})\\]\nTengo una distribucion normal centrada en k y al elegir un dato al azar (o sea, de la muestra aleatoria) puede tener un valor muy cercano o muy lejano de la media.\nLa varianza también es una estimación hecha con lo que yo he visto. Es otra estimación más. Esto hace que vayamos arrastrando aproximaciones AKA posibles errores.\n\\[\n\\displaylines{\n\\sigma^2_{\\kappa_w} = \\frac{\\sum_i \\sum_j \\pi_{ij} \\left[ w_{ij} - (\\overline{w}_{i \\cdot} + \\overline{w}_{\\cdot j})(1 - \\kappa_w) \\right]^2 - \\left[ \\kappa_w - \\Pi_c (1 - \\kappa_w) \\right]^2}{n (1 - \\Pi_c)^2} \\\\\n\\overline{w}_{i \\cdot} = \\sum_{j=1}^t \\pi_{\\cdot j} w_{ij} \\\\\n\\overline{w}_{\\cdot j} = \\sum_{i=1}^t \\pi_{i \\cdot} w_{ij}\n}\n\\]\nEl estimador de esta varianza su contrapartida muestral, sustituyendo probabilidades por proporciones y el índice kappa por su estimador.\n\\[\n\\displaylines{\n\\hat{\\sigma}^2_{\\kappa_w} = s^2_{\\kappa_w} = \\frac{\\sum_i \\sum_j p_{ij} \\left[ w_{ij} - (\\hat{w}_{i \\cdot} + \\hat{w}_{\\cdot j})(1 - \\hat{\\kappa}_w) \\right]^2 - \\left[ \\hat{\\kappa}_w - P_c (1 - \\hat{\\kappa}_w) \\right]^2}{n (1 - P_c)^2} \\\\\n\\hat{w}_{i \\cdot} = \\sum_{j=1}^t p_{\\cdot j} w_{ij} \\\\\n\\hat{w}_{\\cdot j} = \\sum_{i=1}^t p_{i \\cdot} w_{ij}\n}\n\\] Límites de confianza para el índice kappa con nivel de confianza \\(\\alpha\\):\n\\[ \\hat{\\kappa}_w +- z_\\alpha s_{\\hat{\\kappa}_w}\\]\nEscala de valoración del índice Kappa.\n\\[\n\\begin{array}{c|c}\n\\textbf{Kappa} & \\textbf{Grado de concordancia} \\\\\n\\hline\n&lt; 0.00 & \\text{Sin concordancia} \\\\\n0.00 - 0.20  & \\text{Insignificante} \\\\\n0.20 - 0.40  & \\text{Discreta} \\\\\n0.40 - 0.60  & \\text{Moderada} \\\\\n0.60 - 0.80  & \\text{Sustancial} \\\\\n0.80 - 1  & \\text{Casi perfecta} \\\\\n\\end{array}\n\\]\nEjemplo 1.3.\nDos radiólogos independientes informan de la presencia o ausencia de neumonía en 100 radiografías, siendo los resultados los siguientes.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Neumonía} & \\textbf{No neumonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Neumonía} & 4 & 6 & 10 \\\\\n                      & \\textbf{No neumonía} & 10 & 80 & 90 \\\\\n\\hline\n& \\textbf{Total} & 14 & 86 & 100 \\\\\n\\end{array}\n\\]\n\\(P_0 = \\frac{4+80}{100} = 0.84 \\quad \\quad P_c = \\frac{10*14 + 90*86}{100^2} = 0.788\\)\\(\\hat{\\kappa_w} = \\frac{0.84 - 0.788}{1 - 0.788} = 0.245\\)\\(s^2_{\\kappa_w} = 0.018 \\quad \\quad z_{0.05}s_{\\kappa_w} = 1.65 \\sqrt{0.018} = 0.22\\)\nSe podría hacer también un aproximación asintótica pero conlleva arrastrar una simulacion más.\nLa estimación puntual la tengo. Como no tengo una muestra más grande puedo remuestrear.\nLos estimadores por intervalo miden la precision de la estimación puntual. Cómo de buena es esa estimación puntual para representar otra posible muestra que podríamos haber obtenido.\nUna estimación puntual que se encuentra en alguno de los intervalos de Landis y Koch.\nLa validez de la estimacion hay que refrentarla con un IC. La estimación puntual de 0.245 se considera discreta y el \\(IC_90 = (0.025, 0.465)\\) abarca varios niveles, desde discreta hasta sustancial. Si el IC estuviera solo en el mismo intervalo que la estimación puntual diríamos que el IC respalda/valida la estimacion puntual. Al abarcar más de un nivel deberemos explicarlo.\nEn el 90 % de estudios similares a este tenemos una concordancia que iría desde insignificante a moderada. La validaz de nuestra estimación no es super cool, es cuestionable, ya que hay otros estudios donde podemos encontrar una concordancia diferente a la que hemos calculado en esta muestra.\nH0: las opiniones son independientes (que no hay relación entre las opiniones) (si hay una concordancia que sea casual) AKA \\(\\rho\\) = 0.\nPara poder admitir \\(\\rho\\) = 0 el IC tendría que darlo como un valor posible y en este caso no lo da. Rechazamos que haya un criterio diferente. (juzgamos que los dos puntúen por igual)\nEjemplo 1.4.\nDos psiquiatras evaluaron a 129 pacientes que habían sido diagnosticados previamente como clínicamente deprimidos. Las categorías de clasificación fueron: 0 para no deprimido, 1 para moderadamente deprimido y 2 para clínicamente deprimido. La tabla siguiente muestra los resultados de la clasificación realizada por los dos psiquiatras.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Psiquiatra 1} \\\\\n& & \\textbf{0} & \\textbf{1} & \\textbf{2} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Psiquiatra 2} & \\textbf{0} & 11 & 2 & 19 & 32 \\\\\n                      & \\textbf{1} & 1 & 3 & 3 & 7 \\\\\n                      & \\textbf{2} & 0 & 8 & 82 & 90 \\\\\n\\hline\n& \\textbf{Total} & 12 & 13 & 104 & 129 \\\\\n\\end{array}\n\\]\nDatos.\n\nCódigolibrary(psych)\n(A &lt;- matrix(c(11,2,19,1,3,3,0,8,82),ncol=3,byrow=TRUE))\n\n     [,1] [,2] [,3]\n[1,]   11    2   19\n[2,]    1    3    3\n[3,]    0    8   82\n\n\nÍndice kappa en R por defecto.\nEn la diagonal pone peso 0 (yo quiero que en la diagonal tiene 1 y conforme me alejo lo voy bajando).\nÍndice kappa sin ponderar.\nLe damos importancia a todo lo que coincide pero no le damos peso a la cercanía entre dos diagnósticos. La elección por el kappa ponderado es crucial cuando los niveles de clasificación son mayores a dos niveles.\nPesos Ciccetti y Allison.\n\nCódigo(w_CA&lt;- matrix(c( 1,0.5,0, 0.5,1,0.5,0,0.5,1),ncol=3))\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.5  0.0\n[2,]  0.5  1.0  0.5\n[3,]  0.0  0.5  1.0\n\n\nEstimaciones de kappa.\n\nCódigokappa1 &lt;- cohen.kappa(A, w=w_CA, n.obs=sum(A), alpha=0.1)\nstr(kappa1)\n## List of 11\n##  $ kappa         : num 0.375\n##  $ weighted.kappa: num 0.402\n##  $ n.obs         : num 129\n##  $ agree         : num [1:3, 1:3] 0.08527 0.00775 0 0.0155 0.02326 ...\n##  $ weight        : num [1:3, 1:3] 1 0.5 0 0.5 1 0.5 0 0.5 1\n##  $ var.kappa     : num 0.00622\n##  $ var.weighted  : num 0.00688\n##  $ confid        : num [1:2, 1:3] 0.245 0.265 0.375 0.402 0.504 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:2] \"unweighted kappa\" \"weighted kappa\"\n##   .. ..$ : chr [1:3] \"lower\" \"estimate\" \"upper\"\n##  $ plevel        : num 0.1\n##  $ bad           : logi FALSE\n##  $ Call          : language cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels,      w.exp = w.exp)\n##  - attr(*, \"class\")= chr [1:2] \"psych\" \"kappa\"\n\n\n\nkappa1$kappa\n## [1] 0.3745225\nkappa1$weighted.kappa\n## [1] 0.4018192\nkappa1$var.kappa\n## [1] 0.006221038\nkappa1$var.weighted\n## [1] 0.006884677\nkappa1$confid\n##                      lower  estimate     upper\n## unweighted kappa 0.2447870 0.3745225 0.5042579\n## weighted kappa   0.2653391 0.4018192 0.5382992\n\nPesos de Fleiss y Cohen.\n\n(w_FC&lt;- matrix(c( 1,0.75,0,0.75,1,0.75,0,0.75,1),ncol=3))\n\n     [,1] [,2] [,3]\n[1,] 1.00 0.75 0.00\n[2,] 0.75 1.00 0.75\n[3,] 0.00 0.75 1.00\n\n\nEstimaciones de kappa.\n\nCódigokappa2 &lt;- cohen.kappa(A, w=w_FC, n.obs=sum(A),alpha=0.1)\nstr(kappa2)\n## List of 11\n##  $ kappa         : num 0.375\n##  $ weighted.kappa: num 0.42\n##  $ n.obs         : num 129\n##  $ agree         : num [1:3, 1:3] 0.08527 0.00775 0 0.0155 0.02326 ...\n##  $ weight        : num [1:3, 1:3] 1 0.75 0 0.75 1 0.75 0 0.75 1\n##  $ var.kappa     : num 0.00622\n##  $ var.weighted  : num 0.00796\n##  $ confid        : num [1:2, 1:3] 0.245 0.274 0.375 0.42 0.504 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:2] \"unweighted kappa\" \"weighted kappa\"\n##   .. ..$ : chr [1:3] \"lower\" \"estimate\" \"upper\"\n##  $ plevel        : num 0.1\n##  $ bad           : logi FALSE\n##  $ Call          : language cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels,      w.exp = w.exp)\n##  - attr(*, \"class\")= chr [1:2] \"psych\" \"kappa\"\n\n\n\nkappa2$weighted.kappa\n## [1] 0.4203694\nkappa2$var.weighted\n## [1] 0.007955659\nkappa2$confid\n##                      lower  estimate     upper\n## unweighted kappa 0.2447870 0.3745225 0.5042579\n## weighted kappa   0.2736575 0.4203694 0.5670813\n\n\\[\n\\begin{array}{c|c}\n\\textbf{Estadístico} & \\textbf{Estimación} & \\textbf{Varianza estimada} & LI_{90} \\\\\n\\hline\n\\text{Kappa sin ponderar} & 0.3745225 & 0.006221038 & 0.2447870 \\\\\n\\text{Kappa ponderado (Ciccetti-Allison)} & 0.4018192 & 0.006884677 & 0.2653391 \\\\\n\\text{Kappa ponderado (Fleiss-Cohen)} & 04203694 & 0007955659 & 0.2736575 \\\\\n\\end{array}\n\\]\nEstamos teniendo resultados 0.3, 0.4. Las estimaciones puntuales nos incitan a concordancias moderadas. No podemos decir que no tengan un criterio común. algo tienen.\n¿Qué pasha cuando tenemos que analizar la validez de una estimación pero tenemos una muestra pequeña? Po a la metodología booststrap\nNos vamos a Tema 3, página 55.\nDepués de remuestrar tengo las 23 radiografías con la clasificación de los dos radiólogos cuando remuestre.\nObtengo muestras bootstrap con 23 radiografias pero la clasificación es la misma para cada radiografía.\nCalculo el kappa \\(\\Rightarrow\\) calculo las diferencias \\(\\Rightarrow\\) Despercio las más alejadas \\(\\Rightarrow\\) Me quedo con el 95 % central.\nSi hay más de 2 observadores \\(\\Rightarrow\\) Índice de Kappa para múltiples observadores (Fleiss JL Statistical Methods for Rates and Proportions, 2003)"
  },
  {
    "objectID": "tema_01/tema_01_3_concordancia_numericas.html#concordancia-entre-las-puntuaciones-dadas-a-un-conjunto-de-individuos",
    "href": "tema_01/tema_01_3_concordancia_numericas.html#concordancia-entre-las-puntuaciones-dadas-a-un-conjunto-de-individuos",
    "title": "3. Análisis de concordancia en variables numéricas ✓",
    "section": "Concordancia entre las puntuaciones dadas a un conjunto de individuos",
    "text": "Concordancia entre las puntuaciones dadas a un conjunto de individuos\nPara establecer comparaciones necesito medidas que no tengan unidades.\nMedidas absolutas\nLas medidas absolutas tienen las mismas unidades que la variable respuesta.\n\n\nDesviación cuadrática media.\n\n\\(\\symbf{RMSE = \\sqrt{\\frac{1}{n}\\sum_{i}{x_i}^2}}\\)\n\n\n\nÍndice de desviación total.\n\nSe utiliza principalmente cuando se quiere evaluar la precisión y exactitud de un método con respecto a otro.\nResponde a cuál es el valor máximo de la desviación absoluta que se puede esperar entre dos mediciones con un cierto nivel de confianza.\n\n\n\\(\\symbf{TDI = P(|X_1 - X_2| \\leq \\delta) = 1 - \\alpha}\\)\n\n\n\\(X_1, X_2\\) son las mediciones a comparar.\n\n\n\\(\\delta\\) es el valor máximo de desviación absoluta.\n\n\n\\(\\alpha\\) es el nivel de significancia o el complemento del nivel de confianza.\n\n\nSi el TDI es bajo, indica que las dos mediciones (o el método y el valor de referencia) están en buen acuerdo y las desviaciones tienden a ser pequeñas.\n\nSi el TDI es alto, significa que hay una mayor variabilidad o discrepancia entre las dos mediciones.\n\n\n\nProbabilidad de cobertura.\n\nÍndice asociado a un intervalo.\nProbabilidad de que la diferencia entre dos mediciones, \\(|X_1 - X_2\\), esté contenida dentro de un intervalo de longitud \\(\\delta\\) (el TDI).\n\\(\\symbf{P(|X_1 - X_2| \\leq \\delta)}\\)\nProbabilidad de que el intervalo de error entre dos mediciones (o entre una medición y un valor de referencia) se mantenga dentro de un rango específico, con un cierto nivel de confianza.\nLa probabilidad de que la diferencia entre dos mediciones caiga dentro de un intervalo predefinido.\nQuiero estudiar con qué probabilidad tomo valores en ese intervalo. no está tan claro que tenga unidades de medida (porque es una prob).\n\nSi le doy la vuelta, qué intervalo tengo para que el x % de las probabilidades estén en él, estoy pidiendo un superior e inferior que sí tienen unidades de medida. Referida a una prbailidad pero en realdiad dada una prob lo que bsco es un intervalo en la variable respuesta que ocurra con esa probabilidad.\n\n\n\nSon buenos para medir las características de esa respuesta. Pero si quiero medir acuerdos comparando dos respuestas distintas no me valen.\nMedidas relativas\nIdea: semejanza de valores.\n\nCoeficiente de Correlación Intraclase. Cuantifica el acuerdo entre agentes.\nCoeficiente de correlación concordante. Analiza la semejanza. (semejanza \\(\\neq\\) acuerdo)\nCoeficiente de correlación de Pearson. Analiza si existe una función que permite llegar de unos valores a otros.\n\nSi no hay relacion:\n- El ICC dice que no hay acuerdo.\n- El coeficiente de correlación concordante dice que no hay semejanza.\n- El coeficiente de correlación de Pearson dice que no hay forma de a partir de unos valores llegar a los otros.\nEjemplos\n\nEjemplo a.\n\n\n\\(Y_a\\) = medida por el método A.\\(Y_b\\) = medida por el método B.\n\\[\n\\displaylines{\n\\begin{array}{c|c c c c}\n\\textbf{Sujeto} & \\textbf{1} & \\textbf{2} & \\textbf{3} & \\textbf{4}  & \\textbf{5} \\\\ \\hline\n\\symbf{Y_a} & 1 & 2 & 3 & 4 & 5 \\\\ \\hline\n\\symbf{Y_b} & 1 & 2 & 3 & 4 & 5 \\\\ \\hline\n\\end{array}\n}\n\\]\n\n\nICC = 1.\n\nCCC = 1.\nComo \\(Y_a = Y_b\\) =&gt; r = 1.\n\n\n\n\n\n\n\n\n\n\n\nEjemplo b.\n\n\n\\(Y_a\\) = medida por el método A.\\(Y_b\\) = medida por el método B.\n\\[\n\\displaylines{\n\\begin{array}{c|c c c c}\n\\textbf{Sujeto} & \\textbf{1} & \\textbf{2} & \\textbf{3} & \\textbf{4}  & \\textbf{5} \\\\ \\hline\n\\symbf{Y_a} & 5 & 6 & 7 & 8 & 9 \\\\ \\hline\n\\symbf{Y_b} & 0 & 1 & 2 & 3 & 4 \\\\ \\hline\n\\end{array}\n}\n\\]\n\nComo \\(Y_b = Y_a - 5\\) =&gt; r = 1. (no sé si hay acuerdo pero sé que puedo hacer una asociación entre uno y otro)\nLas puntuaciones son diferentes pero el orden de los sujetos es el mismo. Además la diferrencia entre puntuaciones es siempre de una unidad. CCC = 1.\naquí frase no he escuchado. ICC = 0.94. No hay independencia, hay un criterio común para los dos. Buscamos si las valoraciones dadas responden a un criterio común (que uno sea más estricto que otro no nos interesa ahora)\n\n\n\n\n\n\n\n\n\n\n\nEjemplo c.\n\n\n\\(Y_a\\) = medida por el método A.\\(Y_b\\) = medida por el método B.\n\\[\n\\displaylines{\n\\begin{array}{c|c c c c}\n\\textbf{Sujeto} & \\textbf{1} & \\textbf{2} & \\textbf{3} & \\textbf{4}  & \\textbf{5} \\\\ \\hline\n\\symbf{Y_a} & 1 & 2 & 3 & 4 & 5 \\\\ \\hline\n\\symbf{Y_b} & 1/2 & 1 & 3/2 & 2 & 5/2 \\\\ \\hline\n\\end{array}\n}\n\\]\n\nAnalizo la relación entre las dos respuestas y veo que con conocer la primera obtengo la segunda. Al exister esa relacion y ser perfecta sé que r=1. \\(Y_b = 1/2 * Y_a\\)\nICC = 0.5.\nCCC &lt; 1. Aunque la puntuación sea diferente, el orden sigue siendo el mismo en ambas valoracioes. Pero la escala de calibracion no es la misma: la diferencia en los primeros es de un punto y de los segundos es de medio punto."
  },
  {
    "objectID": "tema_01/tema_01_3_concordancia_numericas.html#coeficientes-de-correlación-intraclase-icc",
    "href": "tema_01/tema_01_3_concordancia_numericas.html#coeficientes-de-correlación-intraclase-icc",
    "title": "3. Análisis de concordancia en variables numéricas ✓",
    "section": "Coeficientes de correlación intraclase (ICC)",
    "text": "Coeficientes de correlación intraclase (ICC)\nEl ICC cuantifica el ajuste (la concordancia) entre varias valoraciones de agentes de una variable numérica. Un conjunto de agentes/jueces dan valoraciones a una serie de individuos.\nProblema\nHay muchos ICC. Y R da todos. \\(\\Rightarrow\\) Tendremos muchas salidas y hay que interpretar la situación de nuestros datos para saber qué ICC es el acorde a nuestro modelo teórico.\nEn la situación donde la evaluación era la asignación de categorías no nos afectaba saber cómo se han recodigo los datos.\nEl ICC es la proporción de variabilidad debida a la variabilidad de los “sujetos”.\nhttps://github.com/cran/psych/blob/master/R/ICC.R\nTendremos un ICC para cada modelo ANOVA.\n\nMedir la concordancia:\n\nMedir el modelo que va por detrás.\nEn el diseño de experientos medir la variabilidad total.\nMedir la variabilidad del modelo / explicada por el modelo.\n\n\nAl coeficiente desde el punto de vista teórico lo llamamos rho, \\(\\rho\\).\n\nEste coeficiente es un valor desconocido y lo aproximamos muestralmente.\n\nCuando hablemos del estimador hablaremos del ICC, \\(\\hat{\\rho}\\).\n\nPosibles situaciones. (en los modelos unifactorial y bifactorial)\nTenemos una muestra de n sujetos valorados por k agentes.\n\n\nCada sujeto es valorado por un conjunto diferente de k jueces seleccionados aleatoriamente. (al sujeto 1 le evalúan k jueces, al sujeto 2 otros k jueces distintos, etc.)\n\n“cada vez que hago un juicio (cada vez que hago una valoración sobre un sujeto) escojo un juez al azar”\n\n\n\nSe selecciona una muestra aleatoria de k jueces y cada uno valora a todos los sujetos.\n\n“tengo muchos métodos y como no puedo estudiar todos selecciono una muestra de jueces al azar” (introducir incertidumbre de haber podido elegir otros cuatro)\n\n\n\nLos k jueces son fijos (la población, no son una muesta) y cada uno valora a todos los sujetos.\n\n“tengo solo cuatro jueces y cojo los cuatro para medir la concordancia entre todos ellos.”\n\n\n\nModelo teórico.\n\nSea \\(y_{ij}\\) = la puntuación del juez i al sujeto j. (al revés de como lo habría hecho yo y cualquier persona normal)\nj, sujeto. i = 1…n\ni, juez. j = 1…k\n\n\\[\ny_{ij} \\text{ depende de} = \\begin{cases}\n\\text{parte común} \\\\\n\\text{influencia del sujeto i} \\\\\n\\text{influencia del juez j} \\\\\n\\text{interacción sujeto/juez} \\\\\n\\text{azar (lo que no hemos incluido en el modelo)} \\\\\n\\end{cases}\n\\]\nPara analizar cómo de concordantes son esas valoraciones tengo que identificar el modelo de la recogida de datos. No es lo mismo estudiar la concordancia de los jueces si tengo la opinión de todos, si he tenido que elegir cuatro de ellos, si me imponen cuáles elegir, si tengo que elegir cada vez uno distinto, etc.\n\\(\\rho = \\frac{Cov(y_{ij},y_{i'j})}{Var(y_{ij})} = \\frac{\\sigma_{\\beta}^2}{\\sigma_{\\beta}^2 + \\sigma_{e}^2} = \\text{variabilidad de los sujetos respecto a la variabilidad total del modelo}\\)\n\n\n\\(Cov(y_{ij},y_{i'j})\\): puntuación de un individuo dada por diferentes jueces.\n\n\n\\(Var(y_{ij})\\): toda la variabilidad de lo observado.\n\nVarianza.\nVarianza de los datos = varianza del modelo + varianza de todo lo demás.\nCaso 1. ANOVA unifactorial\nCada sujeto ha sido evaluado por un número k de jueces distinto y desconocido para cada sujeto. No sé si la primera puntuación de un individuo y la primera puntuación de otro individuo la ha dado el mismo juez. Si no tengo la influencia del juez medida tampoco puedo medir la interacción.\nFuentes de variabilidad no controladas:\n\nvariabilidad debida a los jueces\nvariabilidad debida a la interacción entre juez y sujeto\nvariabilidad debida al error\n\nModelo teórico.\n\\[\ny_{ij} = \\mu + \\beta_j + e_{ij}, \\quad i = 1, \\dots, k, \\quad j = 1, \\dots, n\n\\]\n\\[\n\\left.\n\\begin{aligned}\n  \\{\\beta_j\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{\\beta}^2) \\\\\n  \\{e_{ij}\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{e}^2)\n\\end{aligned}\n\\right\\}\n\\quad \\text{independientes.}\n\\]\n\\[\n\\displaylines{\n& \\textbf {ANOVA y esperanzas de los cuadrados medios} \\\\\n&\\begin{array}\n{|c|c|c|c|}\n\\hline  \\textbf { Fuente de variación } & \\textbf { SC } & \\textbf { g.l. } & \\textbf { CM }  & \\textbf { E(CM) } \\\\\n\\hline\n\\text {Intersujetos} &\\text {SCB = } k\\sum_{j=1}^{n}(\\overline{y}_{·j} - \\overline{y}_{··})^2 & \\text {n-1} & \\text {CMB = } \\frac{SCB}{n-1} & k\\sigma_{\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\text {Intrasujetos} & \\text {SCW = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{·j})^2 & \\text {n(k-1) = N-n} & \\text {CMW = } \\frac{SCW}{n(k-1)} & \\sigma_{e}^2\\\\\n\\hline\n\\text {Total} & \\text {SCT = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{··})^2 & \\text {N-1} & \\text {CMT = } \\frac{SCW}{N-1} & \\sigma^2\\\\\n\\hline\n\\end{array}\n}\n\\]\n\n\nIntersujetos. Entre los sujetos. Variabilidad entre los sujetos que participan. Captura las diferencias naturales entre individuos.\n\nSi hay n sujetos, los g.l. de la variabilidad entre ellos es n-1.\n\n\n\nIntrasujetos. Dentro de los sujetos. Variabilidad que existe dentro de un mismo sujeto debido a los diferentes jueces.\n\nComo es dentro de los sujetos y cada sujeto tiene k evaluaciones, g.l. = k-1.\nComo tenemos n sujetos, g.l. totales son n(k-1).\n\n\n\nICC.\n\\[\n\\displaylines{\\rho = \\frac{Cov(y_{ij},y_{i'j})}{Var(y_{ij})} = \\frac{\\sigma_{\\beta}^2}{\\sigma_{\\beta}^2 + \\sigma_{e}^2} \\Rightarrow \\\\\n\\hat\\rho = ICC(1,1) = \\frac{CMB - CMW}{CMB + (k-1)CMW}}\\]\nContraste de hipótesis.\n\\(H0: \\rho =0. \\quad \\quad\\) Estadístico:\n\\[F_0 = \\frac{CMB}{CMW}, \\quad p-value = p(F_{n-1,N-n} &gt; F_{0})\\]\nIntervalo de confianza.\n\\[IC_{1-\\alpha}(\\hat\\rho) = (\\frac{F_{L}-1}{F_{L}+(k-1)}, \\frac{F_{U}-1}{F_{U}+(k-1)})\\] tal que\n\\[F_{L} = \\frac{F_{0}}{F_{n-1, n(k.1), \\alpha/2}} \\quad \\quad F_{U} = F_{0} * F_{n(k.1), n-1, \\alpha/2}\\]\nEjemplo\nEjemplo 1.5. (Shrout y Fleiss (1979))\nLa siguiente tabla muestra cuatro valoraciones para cada uno de 6 sujetos.\n\\[\n\\begin{array}{c|cccc}\n& \\textit{Juez} \\\\\n\\hline\n\\textit{Sujeto} & {1} & {2} & {3} & {4} \\\\\n\\hline\n1 & {9} & {2} & {5} & {8} \\\\\n2 & {6} & {1} & {3} & {2} \\\\\n3 & {8} & {4} & {6} & {8} \\\\\n4 & {7} & {1} & {2} & {6} \\\\\n5 & {10} & {5} & {6} & {9} \\\\\n6 & {6} & {2} & {4} & {7} \\\\\n\\end{array}\n\\]\n\nCódigoA&lt;- matrix(c(9,2,5,8,\n 6,1,3,2,\n 8,4,6,8,\n 7,1,2,6,\n 10,5,6,9,\n 6,2,4,7),ncol=4,byrow=TRUE)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    9    2    5    8\n[2,]    6    1    3    2\n[3,]    8    4    6    8\n[4,]    7    1    2    6\n[5,]   10    5    6    9\n[6,]    6    2    4    7\n\n\nLas valoraciones para cada sujeto corresponden a jueces diferentes para cada sujeto.\n\\[\n\\displaylines{\n&\\begin{array}\n{|c|c|c|c|}\n\\hline  \\textbf { Fuente de variación }& \\textbf { g.l. } & \\textbf { CM } \\\\\n\\hline\n\\text {Intersujetos / Entresujetos} & 5 & 11.24 \\\\\n\\hline\n\\text {Intrasujetos / Dentro de sujetos} & 18 & 6.26 \\\\\n\\hline\n\\end{array}\n}\n\\]\n\nCódigon &lt;- 6\nk &lt;- 4\n\ndatos &lt;- data.frame(\n  Fuente_de_variación = c(\"Entre sujetos\", \"Dentro de sujetos\"),\n  g_l = c(n-1, n*k-n),\n  Cuadrado_medio = c(11.24, 6.26)\n)\n\ndatos\n\n  Fuente_de_variación g_l Cuadrado_medio\n1       Entre sujetos   5          11.24\n2   Dentro de sujetos  18           6.26\n\n\n\\[\n\\displaylines{\n\\text{ICC} = \\frac{CMB - CMW}{CMB + (k-1)CMW} \\\\\n\\text{ICC} = \\frac{11.24 - 6.26}{11.24 + 3*6.26} = 0.290\n}\n\\]\n\n(ICC &lt;- (11.24 - 6.26) / (11.24 + 3*6.26))\n\n[1] 0.1658894\n\n\n\nCódigolibrary(psych)\nICC &lt;- ICC(A, lmer=FALSE)\nICC$results[\"Single_raters_absolute\",]\n\n                       type       ICC        F df1 df2         p lower bound\nSingle_raters_absolute ICC1 0.1657418 1.794678   5  18 0.1647688  -0.1329323\n                       upper bound\nSingle_raters_absolute   0.7225601\n\n\nEl experimento no se ha diseñado para valorar sobre quienes dan evaluaciones (no se ha diseñado para controlar el efecto de los jueces).\nEstoy mirando la independencia entre las valoraciones de un mismo individuo (para el \\(ind_1\\) las ka valoraciones, para el \\(ind_2\\) las ka valoraciones, etc.)\nICC(1,1) = 0,1657 \\(\\quad \\quad\\) \\(IC_{95}\\) = (-0.13, 0.72)\nMi objetivo es estudiar si \\(\\rho = 0\\).\n\nNo puedo rechazar la H0: independencia entre las valoraciones AKA las puntuaciones dadas al individuo son independientes.\nMedimos si un sujeto tiene medidas concordantes sin poder valorar a los jueces.\nCaso 2. ANOVA bifactorial de efectos aleatorios\n(el caso más complicado precisamente por la aleatoriedad que queremos extrapolar a todos los jueces)\nCada sujeto ha sido evaluado por un número k de jueces distinto y desconocido para cada sujeto. No sé si la primera puntuación de un individuo y la primera puntuación de otro individuo la ha dado el mismo juez. Si no tengo la influencia del juez medida tampoco puedo medir la interacción — CAMBIAR\nBusco unos sujetos al azar y busco unos jueces al azar \\(\\Rightarrow\\) Dos factores.\nLa puntuación puntual viene dada por:\n\\(y_{ij} =\\) un algo común para todos \\(\\quad + \\quad\\) lo que influya factor sujeto \\(\\quad + \\quad\\) lo que influya factor juez \\(\\quad + \\quad\\) la interaccion \\(\\quad + \\quad\\) lo que dependa de todo lo que no he incluido.\nEn este escenario hay un grado de concordancia más alta debido a que el primer juez es elegido al azar pero es el mismo para todos, el segundo juez es elegido al azar pero es el mismo para todos, etc.\nDe un grupo de jueces he elegido unos pocos al azar. como podría ser cualquieras los que elijo, me sirve para representar a todos: “independencia entre las valoraciones de los jueces a un mismo individuo”.\nFuentes de variabilidad no controladas:\n\nvariabilidad debida al error\n\nModelo teórico.\n\\[\ny_{ij} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + e_{ij}, \\quad i = 1, \\dots, k, \\quad j = 1, \\dots, n\n\\]\n\\[\n\\left.\n\\begin{aligned}\n  \\{\\alpha_i\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{\\alpha}^2) \\\\\n  \\{\\beta_j\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{\\beta}^2) \\\\\n  \\{(\\alpha\\beta_{ij}\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{\\alpha\\beta}^2) \\\\\n  \\{e_{ij}\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{e}^2)\n\\end{aligned}\n\\right\\}\n\\quad \\text{independientes.}\n\\]\n\\[\n\\displaylines{\n& \\textbf {ANOVA y esperanzas de los cuadrados medios} \\\\\n&\\begin{array}\n{|c|c|c|c|}\n\\hline  \\textbf { F. variación } & \\textbf { SC } & \\textbf { g.l. } & \\textbf { CM }  & \\textbf { E(CM) } \\\\\n\\hline\n\\text {Intersujetos} &\\text {SCB = } k\\sum_{j=1}^{n}(\\overline{y}_{·j} - \\overline{y}_{··})^2 & \\text {n-1} & \\text {CMB = } \\frac{SCB}{n-1} & k\\sigma_{\\beta}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\text {Intrasujetos} & \\text {SCW = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{·j})^2 & \\text {n(k-1) = N-n} & \\text {CMW = } \\frac{SCW}{n(k-1)} & \\sigma_{\\alpha}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\hfill \\text {Intraagentes} & \\text {SCA = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}(\\overline{y}_{i·} - \\overline{y}_{··})^2 & \\text {(k-1)} & \\text {CMA = } \\frac{SCA}{(k-1)} & n\\sigma_{\\alpha}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\hfill \\text {Residual} & \\text {SCE = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{·i} - \\overline{y}_{·j} + \\overline{y}_{··})^2 & \\text {(n-1)(k-1)} & \\text {CME = } \\frac{SCE}{(n-1)(k-1)} & \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\text {Total} & \\text {SCT = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{··})^2 & \\text {N-1} & \\text {CMT = } \\frac{SCW}{N-1} & \\sigma^2\\\\\n\\hline\n\\end{array}\n}\n\\]\nEstamos haciendo una concordancia de todos los jueces, no oslo de los k de la muestra.\nICC.\n\\[\n\\displaylines{\\rho = \\frac{Cov(y_{ij},y_{i'j})}{Var(y_{ij})} = \\frac{\\sigma_{\\beta}^2}{\\sigma_{\\alpha}^2 + \\sigma_{\\beta}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2} \\Rightarrow \\\\\n\\hat\\rho = ICC(2,1) = \\frac{CMB - CME}{CMB + (k-1)CME + \\frac{k(CMA-CME)}{n}}}\\]\nContraste de hipótesis.\n¿hay un cirterio comun de los jeuces para lar las valoracion? rho=0, no.\n\\(H0: \\rho =0. \\quad \\quad\\) Estadístico:\n\\[F_0 = \\frac{CMB}{CME}, \\quad p-value = p(F_{n-1,v} &gt; F_{0})\\]\nIntervalo de confianza.\n\\[IC_{1-\\alpha}(\\hat\\rho) = (\\frac{n(CMB-F^*)}{F^*[kCMA+(kn-k-n)CME]+nCMB}, \\frac{n(F_*CMB-CME)}{kCMA+(kn-k-n)CME+nF_*CMB})\\] tal que\n\\[F_* = F_{n-1, \\nu, \\alpha/2} \\quad \\quad F_* = F_{\\nu, n-1, \\alpha/2}\\] \\[g.l. = \\nu = \\frac{(n-1)(k-1) \\left\\{ k \\hat{\\rho} \\frac{\\text{CMA}}{\\text{CME}} + n \\left[ 1 + (k-1) \\hat{\\rho} \\right] - k \\hat{\\rho} \\right\\}^2}{(n-1) k^2 \\hat{\\rho}^2 \\left( \\frac{\\text{CMA}}{\\text{CME}} \\right)^2 + \\left\\{ n \\left[ 1 + (k-1) \\hat{\\rho} \\right] - k \\hat{\\rho} \\right\\}^2} \\]\nEl IC no tiene el 0. No rechazo independencia entre las valoracions que los jeuces dan a los suejtos. ahora podemos dar quienes dan las valoraciones\nEjemplo\nEjemplo 1.5 (Shrout y Fleiss (1979))\n\nCódigoA&lt;- matrix(c(9,2,5,8,\n 6,1,3,2,\n 8,4,6,8,\n 7,1,2,6,\n 10,5,6,9,\n 6,2,4,7),ncol=4,byrow=TRUE)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    9    2    5    8\n[2,]    6    1    3    2\n[3,]    8    4    6    8\n[4,]    7    1    2    6\n[5,]   10    5    6    9\n[6,]    6    2    4    7\n\n\nCada uno de los 4 jueces seleccionados aleatoriamente valora a todos y cada uno de los 6 sujetos:\n\\[\n\\displaylines{\n&\\begin{array}\n{|c|c|c|c|}\n\\hline  \\textbf { Fuente de variación }& \\textbf { g.l. } & \\textbf { CM } \\\\\n\\hline\n\\text {Intersujetos / Entresujetos} & 5 & 11.24 \\\\\n\\hline\n\\text {Intrasujetos / Dentro de sujetos} & 18 & 6.26 \\\\\n\\hline\n\\hfill \\text {Intraagentes / Entre jueces } & 3 & 32.49 \\\\\n\\hline\n\\hfill \\text {Residual} & 15 & 1.02\\\\\n\\hline\n\\end{array}\n}\n\\]\n\nCódigodf &lt;- data.frame(\n  \"Fuente de variación\" = c(\"Intersujetos / Entresujetos\", \"Intrasujetos / Dentro de sujetos\", \"Entre jueces\", \"Residual\"),\n  \"g.l.\" = c(5, 18, 3, 15),\n  \"Cuadrado medio\" = c(11.24, 6.26, 32.49, 1.02)\n)\n\ndf\n\n               Fuente.de.variación g.l. Cuadrado.medio\n1      Intersujetos / Entresujetos    5          11.24\n2 Intrasujetos / Dentro de sujetos   18           6.26\n3                     Entre jueces    3          32.49\n4                         Residual   15           1.02\n\n\n\\[\n\\displaylines{\n\\text{ICC} = \\frac{CMB - CME}{CMB + (k-1) \\cdot CME + \\frac{k \\cdot (CMA - CME)}{n}} \\\\\n\\text{ICC} = \\frac{11.24 - 1.02}{11.24 + 3 \\cdot 1.02 + \\frac{4 \\cdot (32.49 - 1.02)}{6}} = 0.290\n}\n\\]\n\n(ICC &lt;- (11.24 - 1.02) / (11.24 + 3*1.02 + 4*(32.49 - 1.02)/6))\n\n[1] 0.2896825\n\n\n\nCódigolibrary(psych)\nICC &lt;- ICC(A, lmer=FALSE)\nICC$results[\"Single_random_raters\",]\n\n                     type       ICC        F df1 df2            p lower bound\nSingle_random_raters ICC2 0.2897638 11.02725   5  15 0.0001345665  0.01878651\n                     upper bound\nSingle_random_raters   0.7610844\n\n\n¿De quién depende ICC? la experimentacion conduce a comparar resultados, comparar puntuaciones de un individuos según diferentes jueces/criterios.\nCada sujeto es valorado por un conjunto diferente de k jueces, seleccionados aleatoriamente. Un juez puede evaluar más de una vez a un sujeto (¿creo que ha dicho eso?).\nCojo k jueces y evalúan al sujeto 1. Cojo otros k jueces y evalúan al sujeto 2. Selecciono otros k jueces y evalúan al siguiente sujeto. Etc. Un juez ha podido salir para evaluar a más de un sujeto.\nDiferencia entre esta resolución del ejercicio y el antetior (diferencia entre ICC_1 e ICC_2): en este método medimos cómo o cuánto fluyen las puntuaciones, no tanto la concordancia entre los jueces.\nCaso 3. ANOVA bifactorial de efectos mixtos.\n(24/10/2024)\nModelo teórico.\n\\[\ny_{ij} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + e_{ij}, \\quad i = 1, \\dots, k, \\quad j = 1, \\dots, n\n\\] \\[\n\\sum_{i=1}^{k}{\\alpha_i} = 0 \\quad\\quad \\text{ojo, eh, esto es la suma de unas ctes, es decir, son efectos fijos}\n\\]\n\\[\n\\left.\n\\begin{aligned}\n  \\{\\beta_j\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{\\beta}^2) \\\\\n  \\{e_{ij}\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{e}^2)\n\\end{aligned}\n\\right\\}\n\\quad \\text{independientes.}\n\\]\n\\[\n\\{(\\alpha\\beta_{ij}\\} \\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{\\alpha\\beta}^2)\n\\quad \\text{independientes.}\n\\] \\[\n\\sum_{i=1}^{k}{\\alpha\\beta_{ij}} = 0 \\quad \\Rightarrow \\quad Cov(\\alpha\\beta_{ij}, \\alpha\\beta_{i'j}) = -\\frac{\\sigma^2_{\\alpha\\beta}}{k-1}\n\\]\n\\[\n\\displaylines{\n& \\textbf {ANOVA y esperanzas de los cuadrados medios} \\\\\n&\\begin{array}\n{|c|c|c|c|}\n\\hline  \\textbf { F. variación } & \\textbf { SC } & \\textbf { g.l. } & \\textbf { CM }  & \\textbf { E(CM) } \\\\\n\\hline\n\\text {Intersujetos} &\\text {SCB = } k\\sum_{j=1}^{n}(\\overline{y}_{·j} - \\overline{y}_{··})^2 & \\text {n-1} & \\text {CMB = } \\frac{SCB}{n-1} & k\\sigma_{\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\text {Intrasujetos} & \\text {SCW = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{·j})^2 & \\text {n(k-1) = N-n} & \\text {CMW = } \\frac{SCW}{n(k-1)} & \\frac{1}{k}\\sum_{i=1}^{k}\\alpha^2_i + \\frac{k}{k-1}\\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\hfill \\text {Intraagentes} & \\text {SCA = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}(\\overline{y}_{i·} - \\overline{y}_{··})^2 & \\text {(k-1)} & \\text {CMA = } \\frac{SCA}{(k-1)} & \\frac{n}{k}\\sum_{i=1}^{k}\\alpha^2_i + \\frac{k}{k-1}\\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\hfill \\text {Residual} & \\text {SCE = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{·i} - \\overline{y}_{·j} + \\overline{y}_{··})^2 & \\text {(n-1)(k-1)} & \\text {CME = } \\frac{SCE}{(n-1)(k-1)} & \\frac{k}{k-1}\\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\text {Total} & \\text {SCT = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{··})^2 & \\text {N-1} & \\text {CMT = } \\frac{SCW}{N-1} & \\sigma^2\\\\\n\\hline\n\\end{array}\n}\n\\]\n¿Por qué alphabeta son DEPENDIENTES?\nYo le pregunté eso a ChatGPT y me dijo:\nEn un modelo ANOVA bifactorial de efectos mixtos, como el que se muestra en la imagen, los términos \\((\\alpha\\beta_{ij})\\), que representan la interacción entre los efectos fijos \\(\\alpha{i}\\)y los efectos aleatorios \\(\\beta_{j}\\), son dependientes porque la varianza de las interacciones está ligada tanto a la variabilidad de los efectos aleatorios como a los efectos fijos.\nEl efecto \\((\\alpha\\beta_{ij})\\) refleja cómo la combinación de un nivel del factor fijo iii con un nivel del factor aleatorio \\(\\beta_{j}\\) puede influir en la variable de respuesta \\(y_{ij}\\). Los efectos aleatorios \\(\\beta_{j}\\) y el término de interacción están correlacionados porque ambos se ven influenciados por las mismas fuentes de variabilidad del factor aleatorio.\nEsto ocurre debido a que el factor aleatorio introduce variabilidad que no es completamente explicada por los efectos fijos \\(\\alpha{i}\\), generando una dependencia en el término de interacción. Esta dependencia puede modelarse con una distribución de varianza conjunta, como se indica en la imagen: \\(N(0, \\sigma_{\\alpha \\beta}^2)\\).\nPor otro lado, los términos \\(\\beta_{j}\\) y \\(e_{ij}\\) son independientes entre sí, ya que representan fuentes de variabilidad distintas: \\(\\beta_{j}\\) describe la variabilidad entre niveles del factor aleatorio, mientras que \\(e_{ij}\\) representa el error experimental o residual.\nICC.\n\\[\n\\displaylines{\\rho = \\frac{Cov(y_{ij},y_{i'j})}{Var(y_{ij})} = \\frac{\\sigma_{\\beta}^2 - \\frac{\\sigma^2_{\\alpha\\beta}}{k-1}}{\\sigma_{\\alpha}^2 + \\sigma_{\\beta}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2} = \\frac{\\sigma_{\\beta}^2 - \\frac{\\sigma^2_{\\alpha\\beta}}{k-1}}{\\sigma_{\\beta}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2} \\Rightarrow \\\\\n\\hat\\rho = ICC(3,1) = \\frac{CMB - CME}{CMB + (k-1)CME}}\\]\nContraste de hipótesis.\n\\(H0: \\rho =0. \\quad \\quad\\) Estadístico:\n\\[F_0 = \\frac{CMB}{CME}, \\quad p-value = p(F_{n-1,(n-1)(k-1)} &gt; F_{0})\\]\nIntervalo de confianza.\n\\[IC_{1-\\alpha}(\\hat\\rho) = (\\frac{F_{L}-1}{F_{L}+(k-1)}, \\frac{F_{U}-1}{F_{U}+(k-1)})\\] tal que\n\\[F_{L} = \\frac{F_{0}}{F_{n-1, (n-1)(k-1), \\alpha/2}} \\quad \\quad F_{U} = F_{0} * F_{(n-1)(k-1), n-1, \\alpha/2}\\]\nEl IC no tiene el 0. No rechazo independencia entre las valoracions que los jeuces dan a los suejtos. ahora podemos dar quienes dan las valoraciones\nEjemplo\nEjemplo 1.5 (Shrout y Fleiss (1979)).\n\nCódigoA&lt;- matrix(c(9,2,5,8,\n 6,1,3,2,\n 8,4,6,8,\n 7,1,2,6,\n 10,5,6,9,\n 6,2,4,7),ncol=4,byrow=TRUE)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    9    2    5    8\n[2,]    6    1    3    2\n[3,]    8    4    6    8\n[4,]    7    1    2    6\n[5,]   10    5    6    9\n[6,]    6    2    4    7\n\n\nCada uno de los 4 jueces valora a todos y cada uno de los 6 sujetos:\n\\[\n\\displaylines{\n&\\begin{array}\n{|c|c|c|c|}\n\\hline  \\textbf { Fuente de variación }& \\textbf { g.l. } & \\textbf { CM } \\\\\n\\hline\n\\text {Intersujetos / Entresujetos} & 5 & 11.24 \\\\\n\\hline\n\\text {Intrasujetos / Dentro de sujetos} & 18 & 6.26 \\\\\n\\hline\n\\hfill \\text {Intraagentes / Entre jueces } & 3 & 32.49 \\\\\n\\hline\n\\hfill \\text {Residual} & 15 & 1.02\\\\\n\\hline\n\\end{array}\n}\n\\]\n\nCódigodf &lt;- data.frame(\n  \"Fuente de variación\" = c(\"Intersujetos / Entresujetos\", \"Intrasujetos / Dentro de sujetos\", \"Entre jueces\", \"Residual\"),\n  \"g.l.\" = c(5, 18, 3, 15),\n  \"Cuadrado medio\" = c(11.24, 6.26, 32.49, 1.02)\n)\n\ndf\n\n               Fuente.de.variación g.l. Cuadrado.medio\n1      Intersujetos / Entresujetos    5          11.24\n2 Intrasujetos / Dentro de sujetos   18           6.26\n3                     Entre jueces    3          32.49\n4                         Residual   15           1.02\n\n\n\\[\n\\displaylines{\n\\text{ICC} = \\frac{CMB - CME}{CMB + (k-1) \\cdot CME} \\\\\n\\text{ICC} = \\frac{11.24 - 1.02}{11.24 + 3} = 0.7148\n}\n\\]\n\n(ICC &lt;- (11.24 - 1.02) / (11.24 + 3*1.02))\n\n[1] 0.7146853\n\n\n\nCódigolibrary(psych)\nICC &lt;- ICC(A, lmer=FALSE)\nICC$results[\"Single_fixed_raters\",]\n\n                    type       ICC        F df1 df2            p lower bound\nSingle_fixed_raters ICC3 0.7148407 11.02725   5  15 0.0001345665   0.3424648\n                    upper bound\nSingle_fixed_raters   0.9458583\n\n\nTenemos:\n\nICC, la ESTIMACIÓN del rho.\nEl estadístico del contraste.\nEl IC: refleja algo sobre la H0."
  },
  {
    "objectID": "tema_01/tema_01_3_concordancia_numericas.html#método-gráfico-de-bland-y-altman",
    "href": "tema_01/tema_01_3_concordancia_numericas.html#método-gráfico-de-bland-y-altman",
    "title": "3. Análisis de concordancia en variables numéricas ✓",
    "section": "Método gráfico de Bland y Altman",
    "text": "Método gráfico de Bland y Altman\nMétodo gráfico, sin análisis estadístico de apoyo.\nSe ha extendido su uso para analizar la concordancia entre dos métodos que utilizan las mismas unidades de medida. Consiste en representar gráficamente el promedio de las dos observaciones frente a su diferencia.\nPermite examinar la magnitud de las discrepancias y su relación con la magnitud de la medida.\nSolamente permite comparar dos observaciones por individuo. Si k medidas con k instrumentos sobre un individuo (k&gt;2) se necesitan hacer comparaciones dos a dos.\n¿Hay una concordancia entre medidas de un mismo individuo?\nSi hubiese concordancia y fuese perfecta el resultado del método gráfico tendría que ser exactamente igual que el resultado con el método no gráfico.\nSea \\((X_1, Y_1), (X_2, Y_2), (X_3, Y_3)\\),…, tal que:\n\\(n_i\\) ~ individuo i.\\(X_i\\) ~ resultado de la variable en el individuo i con el método 1.\\(X_i\\) ~ resultado de la variable en el individuo i con el método 2.\nSe realiza un cambio de referencia.$ \\[\n\\left( \\frac{X_i + Y_i}{2}, \\quad \\quad  X_i - Y_i \\right)\n\\] Gráficamente, se pasa de un diagrama de dispersión a un diagrama de Bland-Altman.\n\\[\\left\\{(X_j, Y_j), \\quad j=1,…n \\right\\} \\quad \\rightarrow \\quad \\left\\{{\\left( \\frac{X_i + Y_i}{2},X_i - Y_i \\right), \\quad j=1,…n}\\right\\}\\]\nSi X e Y fueran iguales (CONCORDANTES) tendríamos medias muy cerca del 0.\nEjemplo 1.6.\nslide 41. pegar.\n¿Sirve hacer un estudio de regresión lineal?\nUn modelo lineal es capaz de encontrar una ecuacion que relacione ambas valoraciones X e Y (se podría considerar incluso como un cambio de escala). En el estudio de concordancia no se pretende ver la relacion entre valoraciones, sino si existe concordancia.\nValdría exclusivamente si al ecuación obtenido fuese Y = X.\nFigura 1.\nCuando damos una medida con el monitor esta suele ser superior a la dada por el esfigmomanómetro.\nFigura 2.\n¿Sigmomanómetro - Monitor?¿Monitor - Sigmomanómetro? Con al figura 1 sabemos y el eje de las diferencias de la figura 2 sabemos que ha sido Sigmomanómetro - Monitor.\nIncluye una recta de la referencia promedio y dos bandas de confianza que calculadas apoyándonos en las propiedades de la media.\nEstamos suponiendo:\n\nQue las difenrencias tiene distribución media (que la van a tener)\nQue hay varianza constante en las diferencias.\n\nLímites de concordancia\nDadas las diferencias entre X e Y, \\(\\left\\{d_j = (X_j, Y_j), \\quad j=1,…n\\right\\}\\), sea \\(\\bar{d} = \\frac{1}{N}\\sum d_j\\) y sea \\(s_d = \\text{desviacion tipica de la muestra de las diferencias de los individuos}\\).\nSuponemos que las diferencias tienen distribución normal y varianza constante. Los límites de concordancia se definen como:\n\\[\\bar{d} \\pm 1.96s_d\\] El \\((1-\\alpha) %\\) muestras de los individuos tienen diferencia muestras que van desde \\(\\bar{d} + 1.96s_d\\) hasta \\(\\bar{d} - 1.96s_d\\).\nSe pueden calcular los errores típicos e intervalos de confianza para los límites de concordancia.\nIC de los límites de concordancia\nAquí entra la suposición de varianza constante. Ya que damos por hecho que tenemos media gaussiana.\nDados unos límites les añadimos una medida de concentración.\nEstos límites siguen una distribucion de t-Student.\nconsigo acotar .\nen media mi diff de mediadas era 16. cuando haría los intervalos tenia -55 y 22. en el caso más optimista tengo -14 29.\ntengo una diferncia que como poco -14+29, y como mucho -61+47\nhay que poner el IC y los intervalos sobre los IC.\n-55,22: intervalo para decir que tengo concentrado el 95 de los datos ahí. este Ic no me dice pracitcamente nada, a esos dos límites les doy una holgura con sus respectivos IC.\nEjemplo 1..\nel 95 % muestras de los individuos tienen diferencia muestras que van de -55 a +22.\nla diff entre un proc y el otro se mueve en promedio en 16 unidades (sobre estima el monitor, o el otro, vamos) a favor del monitor y las diff entre los dos para el 95 % de los individuos se mueve entre 55 y +22.\na parte del Ic tenemos una banda para el IC.\nEjemplo 1.7.\nComparan el volumen de plasma con la medida normal.\nvemos que el movlumen de plasma está por encima de la diagonal, claramente una de las medidas está por encima siempre.\nsi las dos medidas fieran concordantesdeberían estar al rededor del cero.\nen el eje_x bajo las diferencias son más pequeñas y cuando x es grande las diferencias son más grandes. las diferncias resultasn que de donde estén las medidas van creciendo (las diferrencias se abren) oh oh parece que tenemos una varianza que no es cote. las medidas con valores más bajo se dierencian menos que las medidas de cuando tienen altos valores.\nestamos trabajando con la medidas de un individuo sobre una medida estándar. lo que hacemos es trasnformar. Repasar las transofrmciones de Cox: son una familia de transformaciones potenciales usadas en estadística para corregir sesgos en la distribución de errores. Cuando tenemos que transformar medidas que hacen referencia a un estandar lo que se recomienda es hacer medias logar´timocas.\n\\[\n\\log(\\frac{a}{b}) = \\log(a) - \\log(b)\n\\]\ncambiamos la escala y las unidades de medida. obviamente para comunicar los resultados tengo que volver a la escala original\nlo que quiero es darle un intervalo a las medias iniciales.\nla media de media(log) no la estamos usando, sobre todo la usamos para ver dónde tengo los datos, pero la medida que realmente uso para sacar conclusiones es la diferencia (la dif de logaritmos en este caso)\n1.058 &lt; N_i / H_i &lt; 1.153\nes un cociente, la medida de nerder como mínimo es mayor en un 5 % que la de harley.\nla medida de N siempre es superior a la de H, con una superioridad que va de un 5.8 % a un 15.3 %\ndiapo54\ncuando tengamos jeuces desconocidos que dna puntuaciones y jueces perfectamente conocidos y vemos cómo concuerdan o no (caso 1 y caso 3)\nusamos un método recursivo que busca n reiteradamente\ntodos los intervalos tiene la misma estructura. partiendo de mi intervalo incial voy a obtener mi F_0 (ya viene en la salida del ICC). Con ello identifico los gl (dependiendo del caso en el que estemos tendremos unos grados de libertad distintos)\npara un n, saco mis estadísticos y mi IC. Si mi IC no cumple los requisitos del longitud del intervalo que me piden, aumento mi n en una unidad. calculo mis estadísticos y mi IC. Si mi IC no cumple los requisitos del longitud del intervalo que me piden, aumento mi n en una unidad.\npara determinar el Ic seguimos un procedimiento iterado. partiendo del tamaño inicial n de la muestra y del valor del estadístoco F_0 aumentamos el tamaño de la muestra en una unida y calculamos los límites del nuevo IC. si la longitud de ete IC satistace las condcciones ya tenemos determinado n, si no, volvemos a aumentar en una unidad el tamño muestral."
  },
  {
    "objectID": "tema_01/tema_01_4_tamanyo_muestral.html#determinación-del-tamaño-muestral-para-el-coeficiente-kappa",
    "href": "tema_01/tema_01_4_tamanyo_muestral.html#determinación-del-tamaño-muestral-para-el-coeficiente-kappa",
    "title": "4. Cálculo del tamaño muestral ✗",
    "section": "Determinación del tamaño muestral para el coeficiente kappa",
    "text": "Determinación del tamaño muestral para el coeficiente kappa\n\\[\n\\displaylines{\n\\text{Quiero una precisión más pequeña.} \\\\\n{\\Downarrow} \\\\\n\\text{La precisión depende de la varianza.} \\\\\n{\\Downarrow} \\\\\n\\text{La varianza depende del tamaño muestral.}\n}\n\\]\nSupongo que n muestral es grande. ¿Puedo suponerlo? Pos claro amego, n es lo que estoy buscando calcular así que puedo desear que n sea grande.\nPara buscar esta nueva n pongo una condición: que el valor máximo de la varianza sea menor a un valor dado. La varianza con el nuevo tamaño \\(n\\) debe ser menor a la varianza con el tamaño de la muestra \\(n_{0}\\).\nEjemplo.\nSupongamos tengo n=20. Calculo el IC y tengo la amplitud del intervalo. Voy a querer un IC tal que la amplitud sea menor a cierto valor \\(k\\) (puedo tener la suerte de que con mi estudio piloto ya cumpla esa condición). Por lo que quiero un n condicioando a un valor máximo de la varianza (que depende de n).\nFijando el valor máximo de la varianza puedo despejar el valor de n.\n\nCaso dicotómico: dos observadores y dos categorías\n\nDefinicion de la varianza en el modelo teórico.\n\\[\n{\\bf\nV(k) = \\frac{A+B+C}{n(1-\\Pi_c)^4}\n}\n\\]\n\ntal que\n\\[\n\\displaylines{\nA = \\pi_{11}[1- \\pi_c - (\\pi_{1·}+\\pi_{·1})(1-\\pi_0)]^2 + \\pi_{22}[1- \\pi_c - (\\pi_{2·}+\\pi_{·2})(1-\\pi_0)]^2 \\\\\nB = (1-\\pi_0)^2[\\pi_{12}(\\pi_{·1}+\\pi_{2·})^2 + \\pi_{21}(\\pi_{·2}+\\pi_{1·})^2] \\\\\nC = (\\pi_0 - 2\\pi_c+\\pi_0\\pi_c)^2\n}\n\\]\nTodos los valores quedan determinados a partir de \\(\\pi_{1·}\\), \\(\\pi_{·1}\\) y \\(k\\).\n\\[\n\\displaylines{\n\\pi_{2·} = 1 - \\pi_{1·} \\\\\n\\pi_{·2} = 1 - \\pi_{·1} \\\\\n\\pi_{c} = \\pi_{1·}\\pi_{·1} + (1 - \\pi_{1·})(1 - \\pi_{·1}) \\\\\n\\pi_{0} = k(1-\\pi_{c}) + \\pi_{c} \\\\\n\\pi_{22} = (\\pi_{0} - \\pi_{1·} + \\pi_{2·}) / 2 = (\\pi_{0} + 1) / 2 \\\\\n\\pi_{11} = 1 - \\pi_{22} = 1 - (\\pi_{0} + 1) / 2 \\\\\n\\pi_{12} = \\pi_{1·} - \\pi_{11} \\\\\n\\pi_{21} = \\pi_{·1} - \\pi_{11}\n}\n\\]\n\n\nCaso dicotómico: dos observadores y t categorías (kappa sin ponderar)\n\n\\[\n{\\bf\nV(k) = \\frac{A+B+C}{n(1-\\Pi_c)^4}\n}\n\\]\ntal que\n\\[\n\\displaylines{\n\\bf{A = \\sum_{i=1}^{t} \\pi{ii}[1-\\pi_c-(\\pi_{i·}+\\pi_{·i})(1-\\pi_0)^2]} \\\\\n\\bf{B = (1-\\pi_0)^2 \\sum_{i=1}^{t} \\sum_{\\substack{i=1 \\\\ i \\neq j}}^{t} \\pi_{ij}(\\pi_{·i}+\\pi_{j·})^2} \\\\\n\\bf{C = (\\pi_0 - 2\\pi_c+\\pi_0\\pi_c)^2}\n}\n\\]"
  },
  {
    "objectID": "tema_01/tema_01_4_tamanyo_muestral.html#n-óptimo",
    "href": "tema_01/tema_01_4_tamanyo_muestral.html#n-óptimo",
    "title": "4. Cálculo del tamaño muestral ✗",
    "section": "n óptimo",
    "text": "n óptimo\n¿Cuál sería el n óptimo partiendo de una muestra incial y un IC incial para conseguir reducir la amplitud del intervalo?\n\nTengo un n que me da la posibilidad de estimar un n neuvo dada una condición.\n\n\nFijando la longitud del intervalo\n\nSupongamos que tenemos información de una muestra de tamaño \\(n_0\\) para la cual se tiene que \\[IC_{\\hat{k}}^{1-\\alpha} \\hspace{1em} = \\hspace{1em} (\\hat{k_0} - z_{\\frac{\\alpha}{2}} * \\sqrt{Var(k)}, \\hspace{1em} \\hspace{1em} \\hat{k_0} + z_{\\frac{\\alpha}{2}} * \\sqrt{Var(k)}\\]\nLlamamos \\[l_{0} \\hspace{1em} = \\hspace{1em} \\text{longitud del intervalo} = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{Var(k)}\\]\nBusco un tamaño muestral \\(n\\) tal que el IC para \\(\\hat{k}\\) con la muestra de tamaño n tenga una longitud \\(w\\). \\[w = \\text{longitud del intervalo de la muestra de tamaño n} = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{Var(k)} = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{A+B-C}{n_0(1-\\Pi_{c})^4}}\\]\nA la estimacion de \\(n\\) le asignamos el mismo error que tendría con una muestra más pequeña (la peor de las situaciones que podríamos tener). \\[w = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{A+B-C}{n_0(1-\\Pi_{c})^4}} = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{A+B-C}{n(1-\\Pi_{c})^4}}\\]\nBusco un \\(n\\) tal que su longitud sea menor igual a \\(w\\).\n\n\\[\n\\begin{cases}\nl_{0} = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{1}{n_0}} * \\sqrt{\\frac{A+B-C}{(1-\\Pi_{c})^4}} \\\\\nw = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{1}{n}} * \\sqrt{\\frac{A+B-C}{(1-\\Pi_{c})^4}}\n\\end{cases}\n\\]\n\nDespejando en \\(l_{0}\\).\n\n\\[\\frac{l_{0} * \\sqrt{n_0}}{2*z_{\\frac{\\alpha}{2}}} = \\sqrt{\\frac{A+B-C}{(1-\\Pi_{c})^4}}\\]\n\nSustituyo en \\(w\\).\n\n\\[w = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{1}{n}} * \\sqrt{\\frac{A+B-C}{(1-\\Pi_{c})^4}} = \\frac{2*z_{\\frac{\\alpha}{2}}}{\\sqrt{n}} * \\frac{l_{0} * \\sqrt{n_0}}{2*z_{\\frac{\\alpha}{2}}} = \\frac{l_{0} * \\sqrt{n_0}}{\\sqrt{n}}\\]\n\nDespejando en \\(w\\).\n\n\\[w = \\frac{l_{0} * \\sqrt{n_0}}{\\sqrt{n}} \\Longleftrightarrow w^2 = \\frac{l_{0}^2 * n_0}{n}\\]\n\\[n = (\\frac{l_{0}}{w})^2*n_0\\]\n\n\nResolver el contraste de hipótesis sobre \\(k\\)\nla condicion que hemos puesto para fijar n es que la longitud del intervalo la fijamos\ntambien pueddo fijar una hipótesis de un crontraste de hipótesis, pe, diapo52, una hipótesis sobre el índico k_0."
  },
  {
    "objectID": "tema_02/tema_02.html",
    "href": "tema_02/tema_02.html",
    "title": "TEMA 2",
    "section": "",
    "text": "M.S. Pepe.\nThe statistical evaluation of medical tests for classification and predictor.\nValidez y prueba diagnóstica.\npd una prueba que trata de clasificar a nun individuo en un grupo adecuado.\nlas pd las manejamos en un contexto clínico pero la idea es extrapolable a otros contexto (pd de un conocimiento adquirido durante la carrera)\nvalidez de una pd cuando refleja aquello para lo que se ocntruyó. si hago unexamen, este dee estar hecho para ver si un estudieante domina o no los conceptos.\nvalidez en el sentido de represntatividad (“y este estimador es válido/representativo para”).\n¿la pd sirve para lo que fue creada?\nprevalencia = probabilidad. Luego la tendré que estimar.\ntrabajamos en una situacion y cuando se complica la situacion vamos a intentar ponernos en la situación anterior."
  },
  {
    "objectID": "tema_02/tema_02_1_introduccion.html#tests-binarios",
    "href": "tema_02/tema_02_1_introduccion.html#tests-binarios",
    "title": "1. Conceptos básicos ✗",
    "section": "Tests binarios",
    "text": "Tests binarios\nuna variable que dice el estado real del paciente respecto a la enfermedad una variable que dice el resiltado de la pf\nY - resultado de la prueba diagnostica en un individuo D - estado real del individuo\nY, D € {0,1}\nprevalencia, rho, probabilidad real de que tenga la enfermedad. \\(\\rho\\) = p(D=1)\nel test da el resultado de la prueba, no da el diagnóstoco.\nel test tiene un resultado para la enfermedad, no da el diagnóstoco. medimos si la prueba es válida para que cuando haya un positivo la persona pueda tomar decisiones o no en base a esa prueba.\nprobailidades qe determinan cómo de válidas son las pruebas que estamos midienda: la sense y la especi. ambas son probabilides de acertar.\ntb relacionadas con las prb de fallo poqque creamos las prob de error FP y FN.q\nqué paxa para un test ideal. se = sp = 1. qué paxa para ser inutil. acertar en una p que tiene la enfermedad. 1-espc: dar un posititvo a alguien que no lo tiene. se = 1-pe, lo que hace es que a todo el mundo le da positivo tenga o no la enfermedad (pensar si tb se cumple dando negativo a todo el mundo.)\nMedidas asociadas a un test:\nmedidas de clasificacion, probabilidades de clasificar bien a un individuo.\nsi qiero hacer una compartaiva entre varias pruebas diagnositcas, al tener dos medidas, ¿cuál priorizo para la comprativa?\nnovio guapo y pobre vs novio feo y rico.\nnecesitamos una escala doble. o de pasar nuestra decision en la comparativa donde las dos comparativas están reducidas a una única dimensión.\np(“emitir un diagnóstico erróneo”) = p(Y \\(\\neq\\) D) = p() + p() = p() intersección p() = = p(Y=1 | D=0) * p(D=0) + p(Y=0 | D=1) * p(D=1) = = (un error) * p(D=0) + p(otro error) * \\(\\rho\\) = (1 - Sp) * (1-\\(\\rho\\)) + (1 - Se) * \\(\\rho\\)\nesto me dice que es igual de importante cualquiera de los dos errores.\nno se puede dar el mismo peso a ambos errores.\npensar en el significado que tiene una prevalencia alta o baja en relación con cada uno de los errores. a mayor prevalencia da más peso a tal error porque…\nejemplo.\nimaginemos una una pd que siempre da positivo a cualquier individuo.\np(Y=1 | D=0) = p(Y=1 | D=1) = 1.\nsi la prev es grande, la prueba es fantástica. si la prev es pequeña, esta prueba apesta. y la prueba no me sirve para nada porque no discrimina. cuando tengo muchas personas con la enfermedad pareace que acierta.\ncuando comparo entre varias pd cada una tiene dos medidas que evaluan su calidad. tenemos que lidiar ese problema usando otra medidsa a travez de los valores predictivos y las razones de verosimilitudes\nvale mijo pero cuando yo hago un test y tengo un positivo, ¿qué hago?\npara ver de un punto de vista clínico cómo de útiles son. tengo un resutlado de una preueba y tengo que hacer un diagnóstico.\nvuelvo a definir el test ideal y el test inutil. Test ideal: PPV = NPV = 1 Test sin utilidad: PPV = \\(\\rho\\) y NPV = 1 - \\(\\rho\\)\nlos valores predictivos dependen de la prevalencia. que un test sea mas sensible o mas especifico no depende de la prevalencia.\nEjemplo 2.1.\nSe considera una prueba para diagnosticar el cáncer de mama. Supongamos que la prevalencia en la población femenina es del 1%. En un estudio caso-control las probabilidades de resultados positivos del test fueron 0,85 dentro de los casos y 0,03 dentro de los controles. ¿Cuáles son los valores predictivos positivo y negativo?\nque sea de caso-control ahora me viene verga. no me sirve para nada en este punto de evaluacion del modelo.\n0.85 = p(Y=1 | D=1) = Se 0.03 = p(Y=1 | D=0) = 1-Sp\nPPV = P(D=1 | Y=1) = \\(\\frac{p(D=1 \\cap Y=1)}{Y=1}\\) = \\(\\frac{p(D=1 | Y=1) p(D=1)}{p(D=1 | Y=1) p(D=1) + p(D=1 | Y=0) p(D=0)}\\) = \\(\\frac{p(D=1 \\cap Y=1)}{Y=1}\\) 0.850.01 0.030.99 0.2225 PNV = P(D=0 | Y=0) =\nla pd perfecta no es. si fuera perfecta tendríamos que haber obtenido un 1 en ambos valores predctivos (que dependen de los resultados de la prueba)\nutlididad: podemos hace run cribada? parra una persona que da un resultado negativo tenemos una pribabilidad bárbara de no tner la enfermedad (0.9984).\nde cada una de las personas de las que tengo un posit, en el 99.89 % de los casos tengo una confimracion de no tener la enfermedad.\nno me sirve para indicar el positivo de la muestra. para esta gente que me da positivo, o repito la prueba o hago otra prueba.\nqué mriamos, aquello que nos conviene, me conviene las partes altas.\nmedidas asociadas a las medidas diagnosticas desde cai el ppio ya que básicamente lo que estamos haciedno e saplicar el Tma. de Bayes.\nRazones de verosimilitud: indica cómo de pausible es algo. lo que hacemos es una comparacion.\ncomparo dos situaciones dos situaciones donde tengo resultados positivos. comparo cómo de creible es tener positivo en los sanos respecto a en los sanos. lo idonea sería ser alta pq tengo que reflejar mucho más los poitivos en los enfermos que en los sanos.\nven cuánta infor añade el resultado de una pd.\nodds son las oportunidades, comparo las oportunidades. 100 personas, hay uno sano y noventa y nueve enfermos. tengo 99 contra 1.\nTras realizar el test: cómo cambia el resultado de la prueba esa percepción. sabiendo el resultado de la rpeuab que es posti/neg veo cuantas personas sanas tengo y cuánto gana.\nla forma de interpretar las razones de verosimilitud no es probabilistas.\nDLR+: 0.2225/(1-0.994) = Se / (1-Sp) = DLR-: (1-Sp) / Se =\nla estimacion para las razones dependen de cómo estimemos se y sp. los valores predictivos no\nlo que tengo es que un valor positivo incrementa la\nodd pre-test = p(d1) / p(d0) = 1/99 – antes de hacer la prueba – sin hacerme la prueba la únnica informacion que tengo la prevalencia de la enfermedad\nodd post-test = p(d1|y1) / p(d0|y1) = 1/3.5 – cuando tengo el positivo. en la prueba el tener ese posti en la pd aumenta la credebilidade de que tengo esa enfermedad. aumenta la credebilidad de tener la enfermedad (de tener esa situacion), que no la probabilidad. ha aumentado en casi 28.83\n28 veces es la relación que existe entre las dos situaciones (el odd post-test cuando es positivo es oigal que el DLR-)\nsi pensamos en los resultados negativos:\npartimos igual (sin hacerme la pruebea) = 1 a favor de tenerlo / 99 a favor de no tenerlo\nal tener un resultado negativo, p(d1|y0) / p(d0|y0) = 1 / 629.\npasar de tener una credebilidad de tener 1 a 99 a tener una credebilidad de 1 de teer la enfermedad a 629 de no tenerla.\n¿un resultado negativo es seis veces más verosimil en un paciente sano que en uno enfermo?\n0.157 es aprox 1/6, entonces sí. seis veces más creible en uno sano que en uno enfermo\nlas razones se intepretan como hacniedo la comparativa deenfermos osbre sanos y las razones negativas de los enfermos sobre los sanos"
  },
  {
    "objectID": "tema_02/tema_02_1_introduccion.html#estudios-cohorte",
    "href": "tema_02/tema_02_1_introduccion.html#estudios-cohorte",
    "title": "1. Conceptos básicos ✗",
    "section": "Estudios cohorte",
    "text": "Estudios cohorte\ndiapo11.\nnos ha dicho que copiemos este dibujo. (lo que exporté del drawio)\nvalidación de la estimación mediante el intervalo de confianza. (tratar de medir de dónde a dónde podría tener mis estimaciones) ¿y si no hubiera trabajado con esta poblacion sino con otra con las mismas caracteristicas y de las misma poblacion?\nbinomiales estudiadas tanto a nivel asintótico como para muestras pequeñas.\nmejor hacer un ic con una muestra pequña que tirar a bootstarp\nlos elementos de la muestra son totalmente independientes y los resultados de uno no depende del otro. con bootstrp no creamos nada nuevo, depende de la muestra original\nsiempre que tengamos una alternativa teórica es preferible a bootstrap\nSupongamos que realizamos un experimento con resultados {éxito, fracaso} y sea p = p(éxito). Repetimos el experimento n veces en condiciones independientes.\nSea X ~ “número de éxitos en n pruebas”. Modelo teórico B(n,p).\n\\(\\hat p\\) = número de éxitos observados / n = x/n\nIntervalo de confianza exacto o no asintótico de la distribución binomia"
  },
  {
    "objectID": "tema_02/tema_02_2_tratamiento_muestral_pruebas_binarias.html#guión",
    "href": "tema_02/tema_02_2_tratamiento_muestral_pruebas_binarias.html#guión",
    "title": "2.Tratamiento muestral para pruebas binarias ✗",
    "section": "Guión",
    "text": "Guión\n\nEstimaciones puntuales, por intervalo.\nRegiones de confianza.\n\n\nCohortes\nCaso-control\n\n\nComparación entre pruebas diagnósticas\nMétricas para comparar.\nCriterio de periferia o preferencia (no sé lo que pone) basados en las métricas. Alternativas si el criterio no produce una conclusión.\nEstimación de las métricas\nValidación\n\ncada pd tiene por detrás cómo he escogido a esa muestra\ndetrás de las probabilidades hay una distribucion binomial\nCuando n es grande =&gt; IC asintótico\ncenrado en p^ (lo que hemos observado) =\n\\[\n\\frac{x}{m} +- z_{\\alpha/2} \\frac{1}{m}\\sqrt{x(1-\\frac{x}{n}))}\n\\]\nsenisnbiliad: n éxitos positivos en grupo de enfermos\n\npara estimar la prob de falsos positivos — (1-Sp) se usa la v.a.\n\n\\[\nX \\sim \\text{número de enfermos en el grupo de sanos\"} \\sim Bin(n_{\\overline{D}}, (1-Sp))\n\\]\n\\[\nx = n_{\\overline{D}}^+\n\\]\n\nPara estimar p = ppv =&gt; X = número de individuos enferos en los que tienen test postitivo\n\n\\[\nX \\sim Bin(n^+, ppv), \\quad \\quad x =  n_{\\overline{D}}^+\n\\]\n\nPara estimar p = NPV =&gt; X = número de individuos enferos en los que tienen test postitivo\n\n\\[\nX \\sim Bin(n^-, npv), \\quad \\quad x =  n_{\\overline{D}}^-\n\\]\nla cuestion es identificar qué estamos investigando en cada uno de los casos, saber cuál es mi éxito y dentro de qué subgrupo estamos mirando el número de éxitos\nla sensi y la especi son dos cantidades. si estimo un ic para esas cosas, estoy cometiendo un error. si estoy estimando dos cosas, el error total que tengo es mayor o la precision total es menor.\nlos estimadores de la probabildiades de clasificacion son independientes. que la sensi sea grande no influye con que la especi sea grande o pequeña.\nprob de IC 0.9, prob de que la sensi esté dentro de un intervalo, 0.9. prob de que la sensi esté dentro de mi intervalo y la especi dentro de mi intervalo, 0.9*0.9\nya que son indep ambas estimaciones vamos a buscar una region de confianza de tal forma que ambas estimaciones estén dentro de esa región con una prob del 0.9."
  },
  {
    "objectID": "tema_02/tema_02_2_tratamiento_muestral_pruebas_binarias.html#región-de-confianza-para-hatse-1-hatsp",
    "href": "tema_02/tema_02_2_tratamiento_muestral_pruebas_binarias.html#región-de-confianza-para-hatse-1-hatsp",
    "title": "2.Tratamiento muestral para pruebas binarias ✗",
    "section": "Región de confianza para (\\(\\hat{Se}, (1-\\hat{Sp})\\))",
    "text": "Región de confianza para (\\(\\hat{Se}, (1-\\hat{Sp})\\))\nic de la sensi multiplacado por el ic de 1-sp (es un “multiplicado” teórico para entenderlo)\nconfianza bidiensional sea 1-alpha\n1-apha = prob caer en el IC * prob de caer en el otro IC = beta^2 tq beta=sqrt(1-alpha)\nla RC es una zona de confort en dos dimensioesn. si trabajo en con las dos dimensiones separadas consigo una confianza menor a la que yo tenía\ndiapo18 \\(\\hat(ppv)\\)\nestimos los casos positivos: 930 cuántos lo son de verdad: 815\nhat(npv) = 327/535\nen medcal por defecto te calcula la asitótica, aunque tengas una muestra de 3 esto tira por la asintótica siempre.\nEstimamos las razones de verosimilitud con las estimacions de las probabilidades de clasificaciones\n\\(\\hat{DLR^+} = \\frac{\\hat{Se}}{\\hat{1-Sp}}\\)\n\\(\\hat{DLR^-} = \\frac{\\hat{1-Se}}{\\hat{Sp}}\\)\ncomparacion de casso positivos que tengo dentro del grupo de enfermos y sanos- en funcion del cociente decíamos que era más verosimil\nDLR+^: un resultado positivo es 30 veces más creíble en el grupo de enfermos que en el grupo de sanos DLR+^: comparamos negativos en grupo de sanos y enfermos, gripo de enferms en sanos deben ser pocos, por que lo que DLR-^&lt;&lt;&lt;1, para facilitar la comparativa se le suele dar la vuelta (porque debe ser más fácil interpretar un 2 que un 0.5)"
  },
  {
    "objectID": "tema_02/tema_02_2_tratamiento_muestral_pruebas_binarias.html#ic-para-las-razones-de-verosimilutes",
    "href": "tema_02/tema_02_2_tratamiento_muestral_pruebas_binarias.html#ic-para-las-razones-de-verosimilutes",
    "title": "2.Tratamiento muestral para pruebas binarias ✗",
    "section": "IC para las razones de verosimilutes",
    "text": "IC para las razones de verosimilutes\nhay que ponerse en aquellos casos donde es más facil tener exito, supongamos n grande.\npasamos a escala logarítmica, así separo la componente de la sensi y la componente de la especi. trabajamos con el método delta de la diapo 19.\nque dice: las dos razones de vero no son independientes sino que su comportamiento cuando trabajamos con sus logaritmos tienen un comportamiento normal bidimensional. la cobaranza dice: cuando una razon aunmenta la otra disminuye (mirar la matriz de varianzas y covarianzas)\nuna normal bidimensional. de esa nbidimi puedo crear una RConfianza.\nla RC es para los logaritmos de las razones.\n\nlibrary(MASS)\nlibrary(ggplot2)\n\n# Definimos la media y la matriz de covarianza\nmedia &lt;- c(0, 0)\ncovarianza &lt;- matrix(c(1, 0.5, 0.5, 1), nrow = 2)\n\n# Generamos los puntos con una distribución normal multivariada\ndatos &lt;- mvrnorm(n = 1000, mu = media, Sigma = covarianza)\ndatos &lt;- as.data.frame(datos)\ncolnames(datos) &lt;- c(\"X\", \"Y\")\n\nlibrary(plot3D)\nx &lt;- seq(-3, 3, length = 50)\ny &lt;- seq(-3, 3, length = 50)\nz &lt;- outer(x, y, function(x, y) mclust::dmvnorm(cbind(x, y), mean = media, sigma = covarianza))\npersp3D(x, y, z, theta = 30, phi = 20, expand = 0.6, col = \"lightblue\", main = \"Distribución Gaussiana Bidimensional\")\n\n\n\n\nejemplo.\ndado IC del log(hat(DLR+)) = (-0.6,2) cuál es el IC(hat(DLR+)) = (e^-0.6, e^2) = 0.55, 0.79 e^-0.6 &lt; hat(DLR+) &lt; e^2\nel test de esfuerzo da positivo tres veces más en los enfermos que en sanos. es tres veces masversosimili encontrar un positivo en la prueba de esfuerzo en un individuo enfermo que en un individio sano\ncuando tenemos un resultado negativo en esa prueba es casi cuatro veces 3.7 mas verosimil un negativo sobre un individuo sano que sobre un individuo enfermo.\nqueriamo pd que tuvieran razones grands grandes en casso de los positivos y peques peques en el caso de los negativos.\npasamos los IC de la escala logartimica a la escala que nos interesa."
  },
  {
    "objectID": "tema_02/tema_02_2_tratamiento_muestral_pruebas_binarias.html#estudios-caso-control",
    "href": "tema_02/tema_02_2_tratamiento_muestral_pruebas_binarias.html#estudios-caso-control",
    "title": "2.Tratamiento muestral para pruebas binarias ✗",
    "section": "Estudios caso control",
    "text": "Estudios caso control\nun grupo sabemos que tiene la enfermedad y otra sabemos que no la tiene\nteniendo una idea de que las ramas de estos estudios estan equilirbados tendremos una canitdad en los casos y una cantidad en los controles que suelen estar equilirbados. lo bueno es que tenemos una situacion equilibrada, lo malo es que no es una situacion representativa.\nla prevalencia que tenemos en al muestra es muchisimo mayor que en la poblacion. no afecta a la estimacion de sensi, de especi ni las razones. lo que no nos vale son los valores predictivos.\nsupongamos que tengo una etimacion para la prevalencia real.\nen caso control la prevalencia de la poblacion no se conserrva en la muestra, cuando es cohortes ese problema no se tiene. porque he bsuacado un grupo de individuos que se han sometido a un riesgo y he esperado a ver si tenían o no la enfermedad, no he elegido.\ntrabajo con canitdad no que dependan del tamaño de la muestra, sino de la prevalencia. uso el tma bayes para expresar los valores predictivos en funcion de la sensi y la especi, ed, en las proabildiades de clasficiacion.\nppv = p(D=1 | Y=1) = —formula del otro día =&gt; ppv = num: Se * prev den: se.prev + (1-sp)(1-prev)\nnpv (sin gorro) = Sp(1-prev) / Sp(1-prev) + (1-se)prev\nsupongams que no sabemos el valor exacto de la prev.\nsi la prev es máxima, uno: ppv=1. si la prev es mínimo, cero: ppv=0.\nsacar graficquito (para algún ejercicio o ejemplo) cómo aumenta el ppv para cierto se y sp en función de cómo aumenta la pervalencia de cero a uno\nsi la prev es máxima, uno: npv=0. si la prev es mínimo, cero: npv=1.\nla estimacion del valor\n“o la estimacion ha cambiado y ha crecido, cómo afecta”, el ppc crece al aumentar la prevalencia y el npv decrece al aumentar la rpevalencia\nrelacion entre el logaritmo de la fversolimilitu con el logartimo de esa x que no sé lo que es.\n\\(\\frac{PPV}{1-PPV} = DLR^+ (\\frac{\\rho}{1-\\rho})\\)\n\\(ln(\\frac{PPV}{1-PPV}9 = ln(DLR^) + ln(\\frac{\\rho}{1-\\rho})\\)\nSi el IC_ln(DLR+) = (a,b). Al logit de rho le llamo H. H = \\(ln(\\frac{\\rho}{1-\\rho})\\)\n\\[\\exp^{a+H} &lt; \\frac{\\hat{ppv}}{1-\\hat{ppv}} \\Rightarrow \\exp^{a+H} - \\exp^{a+H} \\hat{ppv} &lt;  \\hat{ppv}\\]\n\\[\\frac{\\hat{ppv}}{1-\\hat{ppv}} &lt; \\exp^{b+H} \\Rightarrow \\hat{ppv} &lt; \\exp^{a+H} - \\exp^{a+H} \\hat{ppv} &lt; \\]\n\\[\\frac{\\exp^{a+h}}{1+\\exp^{a+h}} &lt; \\hat{ppv} &lt; \\frac{\\exp^{b+h}}{1+\\exp^{b+h}}\\]\ndiapo23\nlogit(rho) = H\nlogit(npv) = -H - ln(DLR-)\nSupongamo que IC_ln(hat(DLR-)) = (A, B) (luego le pongo yo otra letras para no confundirlo con a y b) =&gt;\nA &lt; ln(hat(DLR-)) &lt; B\n-B &lt; -ln(hat(DLR-)) &lt; -A\n\nH - B &lt; logit(npv) &lt; - H - A\n(H-B) &lt; NPV / (1-NPV) &lt; - (H+A)\n(H-B) &lt; ln(NPV / (1-NPV)) &lt; - (H+A)\n\ne^(- (H-B)) &lt; NPV / (1-NPV) &lt; e^(- (H+A))\ne^(- (H-B)) * (1-NPV) &lt; NPV &lt; e^(- (H+A)) * (1-NPV)\ne^(- (H-B)) / (1 e^(- (H-B))) &lt; NPV &lt; e^(- (H-A)) / (1 e^(- (H-A)))\ndiapo24. el ic me dice que estima mal la rpeuab diagnóstica. si dando positivo solo acierto al enfermedad de 0.08 a 0.2 (oooo algo así)\nel ic 008 a 0.2 no es muy grande pq lo máximo .\ntienes un valor positivo en la prueba, dando un diagnóstico positivo, cómo de bien lo haces. la pd no es buena pq solo acertaría entre un 0.08 y un 0.2 basándome en haber visto un valor positivo en la prueba.\nsi uno toma esta pd para hacer un diagnóstico en funcion del resultado los aciertos son muy pocos.\ndiapo25.\ncada mediada está enfocada a cosas distintas. la probabildiad de prediccion: cómo refleja la prueba el estado real del paciente.\nlos valores precitivos es al reves, los valroes clinicos de la prueba.\nlo tercero como de creible es un valor postivio o negativo.\npuedeo elegir cualquiera para comparar dos pd.\nlos cocienes son fáciles, los odd ratio son odiosos.\nsi tengo dos pd basadas en unidades distintas, no puedo hacer restas. pero si tengo cocientes sí puedo hacer comparaciones.\nsi no preferencia basada en prob de clasificacion hayla en la basada en la ppv\nsi fallo en la i) voy a la iii) a ver qué pasa"
  },
  {
    "objectID": "tema_03/tema_03.html#simulación-numérica-para-desarrollar-pruebas-diagnósticas.",
    "href": "tema_03/tema_03.html#simulación-numérica-para-desarrollar-pruebas-diagnósticas.",
    "title": "TEMA 3",
    "section": "Simulación numérica para desarrollar pruebas diagnósticas.",
    "text": "Simulación numérica para desarrollar pruebas diagnósticas.\nEste es un tema instrumental. Iremos recurriendo a sus contenidos a medida que avance el desarrollo de la asignatura.\nContenidos:\n\nIntroducción (metodología y otros aspectos básicos)\nSelección aleatoria de pacientes y asignación de tratamientos\nObtención de datos simulados\nValidación de estimaciones: Metodología Bootstrap\nValidación del Índice Kappa\nTécnicas de remuestreo aplicadas a la inferencia de curvas ROC\nDeterminar el número de réplicas de simulación\nControlar la calidad de los datos simulados\nAplicaciones"
  },
  {
    "objectID": "tema_03/tema_03_1_introduccion.html#whats-simular",
    "href": "tema_03/tema_03_1_introduccion.html#whats-simular",
    "title": "1. Introducción ✓",
    "section": "What’s simular?",
    "text": "What’s simular?\nSIMULAR. Representar la realidad con un modelo.\nQueremos es una simulación estocástica \\(\\Rightarrow\\) Obtener diferentes “variedades” de una situacion donde el azar interviene. \\(\\Rightarrow\\) Lo que se conoce como estimacion de Monte Carlo.\n\nUn proceso estocástico es aquel cuyo comportamiento no es determinista, en la medida en que el subsiguiente estado del sistema se determina tanto por las acciones predecibles del proceso como por elementos aleatorios.\nUna simulación de Monte Carlo es un modelo probabilístico que puede incluir un elemento de incertidumbre o aleatoriedad en su predicción. https://aws.amazon.com/es/what-is/monte-carlo-simulation/. Técnica numérica basada en conceptos y resultados probabilísticos que consigue IMITAR un fenómeno (situación o sistema) real. Obvio que no tenemos la certeza de cuál va a ser el fenómeno, hay incertidumbre.\n\n\nLa simulación es la antítesis de los modelos teóricos en el sentido de que la simulación no duda nunca porque ya no tiene probabilidades, tiene datos. La simulación no echa cuentas, da resultados (pa’ echar cuentas ya tengo el modelo teórico).\n\nPara simular un modelo tengo que basarme en un modelo teórico. Si quiero hacer simulaciones de la realidad debo conocer el comportamiento teórico del modelo. E identificar qué partes de ese modelo depende del azar y cuáles no. Esos cambios aleatorios impactados por el azar debo definirla como una variable aleatoria.\nPor ello, no hablamos de muestras malas ni muestras buenas. Habrá simulaciones malas o simulaciones buenas.\nLos números aleatorios que nos da el ordenador en realidad son números pseudoalatorios. Dado un valor inicial se consigue el siguiente número, y a partir del segundo el tercero, etc.\nPlanteamiento de un modelo de simulación:\n\nDesarrollar un modelo que represente la situación real que se quiere investigar.\nIdentificar qué partes o fases de la situación real cambian aleatoriamente. (sexo del bebé)\nDescribir los cambios aleatorios con variables aleatorias. (Sexo: {XX, XY}, p(XX)=0.5)\nGenerar observaciones aleatorias del sistema investigado.\nValidar el modelo simulado comparando los valores simulados con las observaciones reales.\n\nLa simulación tiene dos fases:\n1. Simulación de un valor aleatorio.\n2. Dado valor aleatorio asignar el valor simulado.\nCondiciones de una simulación\n\nQue sea rápida (que la generacion de números aletorios sea rápida)\n\nLos números aleatorios generados se distribuyan entre 0-1\n\nLos números aleatorios generados se repartan por igual entre 0-1\n\nLos números aleatorios parezcan independientes\n\nLos números generados no son independientes, ya que dada una semilla siempre recreo la misma sucesión de números aleatorios. H0: necesitamos que esos números pseudoaletorios parezcan independietes (ya que sabemos que no lo son\nEjemplo\nRealizar una asignación aleatoria del sexo de un bebé. Conocido el sexo vamos a simular el peso de cada bebé.\nCriterio: dado un número aleatorio (entre 0-1) elegir el sexo del bebé. Quiero que ambos elementos tengan la misma probabilidad (divido el intervalo en dos partes iguales)\nLa base de mi modelo teorico:\nx = sexo del bebé       y = peso del bebé (kg)\n\nQuiero generar varios bebés.\n(x1, peso_1)\n(x2, peso 2)\n\nPara cada bebé tengo sexo y peso (muestras paradas pero independencia entre las observaciones).\n\np(X=varón) = 0.5 = p(x=hembra)\npeso | x=varón ~ N(3.266, 0.514)\npeso | x=hembra ~ N(3.155, 0.495)\n\n\nGenero un número aleatorio.\nHago una simulación.\nSaco los datos simulados.\nReciclo semilla.\n\n\n\n\n\nTengo dos variables que simular.\n- Un número aleatorio para simular la primera variable.\n- Otra variable que no conozco pero que está condicionada. Otro número para simular para la otra variable.\n\nn=6; set.seed(20175)\n\nU=runif(n,min=0,max=1)\np=0.5;\npeso=numeric(n);\nsexo=character(n);\n\nfor(i in 1:n) {\n  if (U[i]&lt;p){\n    pp=rnorm(1,3.266,0.514)\n    peso[i]=pp\n    sexo[i]=\"varon\"\n } else {\n   pp=rnorm(1,3.155, 0.495)\n   peso[i]=pp\n   sexo[i]=\"mujer\"\n }\n}\n\n\ncat(\"Números aleatorios:\", U, \"\\n\")\n\nNúmeros aleatorios: 0.7621414 0.3668522 0.3111475 0.6270264 0.07654514 0.7816305 \n\ncat(\"Sexo:\", sexo, \"\\n\")\n\nSexo: mujer varon varon mujer varon mujer \n\ncat(\"Peso:\", peso, \"\\n\")\n\nPeso: 3.913199 3.536088 2.989453 3.101755 2.76329 3.636893 \n\n\n\nn=100000; set.seed(20175)\nU=runif(n,min=0,max=1)\np=0.5;\npeso=numeric(n);\nsexo=character(n);\n\nfor(i in 1:n) {\n  if (U[i]&lt;p){\n    pp=rnorm(1,3.266,0.514)\n    peso[i]=pp\n    sexo[i]=\"varon\"\n } else {\n   pp=rnorm(1,3.155, 0.495)\n   peso[i]=pp\n   sexo[i]=\"mujer\"\n }\n}\n\n# Histograma muestral n=100000\nhist(peso,freq=FALSE)\n\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nComo tengo dos muestras, dos variables, necesito que la creación de la simulación de cada una de las variables sea independiente. Necesito que la semilla de cada variable aleatoria, AKA la semilla de cada variable simulada, sea diferente. Al ser números pseudoaleatorios estaría condicionada la primera y segunda variable simulada.\n(esto no tiene nada que ver con que una variable esté condicionada a la otra)\n\n\n\n\n\n\n\n\nAcerca del ejercicio que nos mandó\n\n\n\nPuedo simular un número aleatorio para un hijo y luego otro para el segundo hijo. O un método para simular con un único número aleatorio los dos hijos a la vez (vamos, crear todas las condiciones con un único número aleatorio).\n\n\nMuy bien mijo, ¿pero y si tengo que simular algunas de las distribuciones no conocidas?\nExisten métodos generales para ello. Si tengo que simular distribución conocida estoy ok. Si tengo que generar una variable desconocida entonces tendré que crearla por distintos métodos.\n\n\n\n\nLa pregunta es, ¿qué nrum le pongo?"
  },
  {
    "objectID": "tema_03/tema_03_2_selección_aleatoria.html",
    "href": "tema_03/tema_03_2_selección_aleatoria.html",
    "title": "2. Selección aleatoria de pacientes y asignación de tratamientos ✗",
    "section": "",
    "text": "Advertencia\n\n\n\nQuedan por incluir unos ejemplos por algún lado del tema 3.\n\n\nObjetivo.\nLlegar a una conclusión que sea válida. Valido = representativo.\nAleatorización.\nExigencia teórica impuesta a experimentos y ensayos clínicos con el objetivo de minimizar la variabilidad de las evaluaciones y evitar la distorsión que pueden producir otros factores en las pruebas experimentales.\nCuántas veces debería hacer la simulación para saber que el resultado es verdaderamente cercano al valor desconocido real. La validez de una estimación está ligada al conportamiento de lo que quiero estimar y con la cantidad de información que tenga.\nHipótesis.\n\nLos pacientes se eligen aleatoriamente. Cualquier grupo de n pacientes tiene las mismas posibilidades de ser elegido.\n\nEl tratamiento es asignado aleatoriamente. No hay preferencias en la asignación, cada paciente tiene las mismas oportunidades de recibir uno de los tratamientos.\n\nDebo de partir con la idea de que todas las personas que son similares.\nSi quiero hacer una comparacion voy a procurar que el tamaño entre las personas que reciben cada uno de los tratamientos es similar.\nEjemplo: aplico varias metodologías de aprendizaje en niños, no puedo tener 10k niños con la metodología antigua y 1 unidad de niños con la metodología nueva, aunque todos sabemos que luego esto nunca se lleva a cabo correctamente. (vamos, la diapo14)\nObjetivos de la aleatorización.\n\nAsegurar que cualquier paciente tiene las mismas oportunidades de recibir el tratamiento experimental.\n\nEliminar sesgos en la selección.\n\nEquilibrar el tamaño de los grupos. (en función del objetivo o de las características experimentales)\n\nVerificar o estudiar la eficacia de los tratamientos.\n\nRazones para la aleatorización.\n\nLos sujetos asignados a cada tratamiento tendrán características similares.\n\nSin similitud =&gt; Sesgo en los resultados.\n\n\n\nNi el investigador, ni el paciente tendrán conocimiento del grupo de asignación en el que se va incluir al participante en el estudio.\n\nSesgo en la selección =&gt; Efectos del tratamiento sobredimensionados.\n\n\n\nLa aleatorización garantiza la validez de los test estadísticos utilizados para comparar tratamientos.\n\nLa aleatorización estratificada y la aleatorización adaptada a las covariables controlan la influencia de las covariables.\nSi quiero ver si un medicamento funciona mejor que otro:\n\nNecestiamos trabajar sobre un modelo matematico que me confirme que los individuos no estén relacionados entre sí (si son familia pues les sentará igual de mal o igual de bien cada uno de los tratamietos) y homogeneidad entre los dos grupos (un grupo más sano que otro)\nSi quiero asginar mi muestra en dos grupos la asignación aleatoria de cada individuo uno por uno va mal si tenemos una muestra reducida (elijo una persona y la asigno aleatoriamente a un grupo, luego con la siguiente). Si n es grande a la larga tendré estabilidad de frecuencias.\n\nAleatorización simple\nEstá basada en una única serie de asignaciones aleatorias. Los pacientes se asignan a los grupos de tratamiento del estudio clínico.\nSe puede imponer un control realizando asignaciones de modo que haya el mismo número de individuos en cada grupo.\nDe un grupo de individuo cojo n de ellos sin que se repitan.\n\nset.seed(178900)\ntrat=sample(1:20,10,replace=FALSE)\nindiv=sort(trat)\nindiv\n\n [1]  2  3  4  6  9 10 11 13 17 18\n\n\nSi tengo un n muy grande participando en el estudio no tengo que preocuparme por la selección.\nVentajas:\n\nEs un procedimiento sencillo y fácil de poner en práctica.\n\nEn estudios de muchos pacientes la aleatorización simple conduce a grupos con un número similar de participantes.\n\nDesventajas:\n\nLos resultados de la aleatorización pueden dar lugar a grupos de tamaños muy desiguales cuando el estudio involucra a un número reducido de pacientes.\n\nProblema:\n\nA veces no tengo todos mis pacientes a mis disposición, por ejemplo, a mitad del estudio este se para y tenemos x individuos sin haberle podido dar el tratamiento (ej: se rompe la máquina que da la dosis y muchos pacientes se quedan sin dosis)\n\nSi el suceso de no poder realizar la asignación a todos los individuos es altamente probable debo tenerlo contemplado. Para ello usamos la aleatorización por bloques.\nPor bloques o restringida\nPermite asignar aleatoriamente sujetos en los grupos de tratamiento de igual tamaño dividiendo a los pacientes potenciales en m bloques de tamaño 2n, n&gt;1.\nEn lugar de colocar las n personas al mogollón, voy equilibrando grupos más pequeños. Así si se interrumpe el experimento tengo un experimento más pequeño pero bien repartido.\nEl procedimiento consiste en repartir toda la muestra en x aleternativas igual que antes pero en vez de tener n/2 y m/2, fijo un tamaño del bloque y dentro de cada bloque jeugo con un reparto equitativo.\nEl procedimiento se basa en la construcción de todos los posibles bloques distintos formados con n asignaciones A (tratamiento) y n asignaciones B (placebo). La elección de cada bloque es aleatoria.\nSelección del tamaño de los bloques.\n\nTamaño n múltiplo del número de tratamientos.\n\nTamaño n no muy grande porque precisamente lo que quiero son bloques no grandes para poder tener la asignación equilibrada.\n\nEl n=6 suele ir bien.\n\nTamaño n dividendo del total de la muestra parece no ser una condición, tal vez el último bloque sea de menor tamaño para asignar a los individuos sin tratamiento.\n\nVentajas:\n\nEl método asegura tener grupos de tamaño equilibrado a lo largo del proceso de asignación, siempre que se use el bloque completo.\n\nDesventajas:\n\nHay que ocultar el tamaño del bloque al clínico para que la asignación no sea predecible.\n\nSi el experimento es doble ciego quiero que el médico no sepa que es asignación por blqoues, ya que si estoy asignando el primer bloque y la mitad de los individuos tienen un tratamiento automáticamente sabe que los n siguientes individuos van a tener el otro tratamiento.\nEjemplo\nAsignar tratamiento o placebo a 60 sujetos utilizando bloques de tamaño 6 (2n=6).\nCreo todos los bloques de asignación posible con todos los órdenes de asignación.\n\n\n\n\nSelecciono con reemplazamiento los bloques necesarios.\n\nset.seed(32581)\nsample(1:20,10,replace=TRUE)\n\n [1]  7  4 15  5 19  3  5  2 12 19\n\n\nEstratificada\nSe utiliza para conseguir un equilibrio entre los grupos respecto a otras características (covariables) de los sujetos. Se complica con la cantidad de covariables que quiera incluir para la creación de grupos.\nControla la posible influencia de las covariables en las conclusiones de la investigación.\nLa asignación uno a uno de los individuos no se puede si quiere controlar las covariables. La asignación estratificada debe hacerse de entrada. Necesito conocer qué niveles tengo para cada covariable y definir el tamaño muestral para cada nivel o combinación de niveles.\nEl número de estratos es el múltiplo del número de niveles de cada covariable.\nEn cada estrato se genera una secuencia de asignación mediante aleatorización simple o por bloques.\nVentajas:\n\nEl método asegura tener grupos de tamaño equilibrado teniendo en cuenta los factores influyentes.\n\nDesventajas:\n\nPrecisa conocer todas las características de los sujetos con anterioridad a la asignación en grupos.\n\nLa técnica se complica al aumentar el número de covariables.\n\nNo puede utilizarse si los sujetos se incluyen en el estudio uno a uno.\nAdaptativa o minimización\nTenemos un equilibrio en los bloques en función de los individuos que vamos recibiendo.\nSe utiliza para minimizar las diferencias de los tamaños de los distintos grupos.\nCada nuevo sujeto se asigna secuencialmente a un grupo concreto de tratamiento teniendo en cuenta las covariables y las asignaciones de los sujetos anteriores.\nEl investigador debe elaborar un plan de aleatorización para asignar los tratamientos a los pacientes\nVentajas:\n\nEl método es útil cuando hay muchas covariables y si la muestra de sujetos es pequeña.\n\nDesventajas:\n\nPrecisa recoger todas las características de los participantes con anterioridad a la aleatorización."
  },
  {
    "objectID": "tema_03/tema_03_3_obtener_datos_simulados.html#simulación-paramétrica",
    "href": "tema_03/tema_03_3_obtener_datos_simulados.html#simulación-paramétrica",
    "title": "3. Obtener datos simulados a partir de observaciones reales ✓",
    "section": "Simulación paramétrica",
    "text": "Simulación paramétrica\nEl peor de mis problemas es tener poca muestra ya que me complica cómo validar mis estimaciones.\nA partir de los datos observados (\\(y_1, …, y_n\\)) calculamos el valor del estimador del parámetro del modelo paramétrico, \\(\\hat{\\theta}\\).\n\n\\(\\theta\\) es el verdadero valor del parámetro poblacional.\n\\(\\hat{\\theta}\\) es el estimador que se calcula a partir de los datos muestrales.\n\\(\\hat{\\theta*}\\) es una estimación del mismo parámetro obtenida mediante una técnica de remuestreo, usada para analizar la variabilidad del estimador \\(\\hat{\\theta}\\)\n\nEjemplo\n\\[\n\\theta = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} |z_1 + z_2| e^{-\\frac{z_1^2 + z_2^2}{2}} \\, dz_1 dz_2\n\\]\n\\[\n\\theta = E \\left[ 2\\pi |Z_1 + Z_2| \\right], \\text{ siendo } Z_1 \\text{ y } Z_2 \\text{ v.a. } N(0,1) \\text{ independientes}\n\\]\n\nNsim=10000\nset.seed(5597)\n\nZ1 &lt;- rnorm(Nsim)\nZ2 &lt;- rnorm(Nsim)\nX &lt;- 2*pi*abs(Z1+Z2)\n\nesperanza_X &lt;- mean(X)\nsd_X &lt;- sd(X)\n\nalpha &lt;- 0.05\nz_a2 &lt;- qnorm(1-(alpha/2))\n\nLower &lt;- esperanza_X - (z_a2*sd_X/sqrt(Nsim))\nUpper &lt;- esperanza_X + (z_a2*sd_X/sqrt(Nsim))\nc(Lower, esperanza_X, Upper)\n\n[1] 7.024079 7.129826 7.235572\n\n\nValidez de la estimación\nSe generan nuevas muestras (\\(y*_1, …, y*_n\\)) a partir de la distribución \\(F(\\hat{\\theta})\\).\n\nObjetivo de la simulación.\nConseguir información sobre la distribución del estimador T de interés.\nSi existen resultados teóricos para la distribución de T o la relación entre el estimador y su parámetro es preferible utilizarlos a depender del resultado de la simulación.\n\n¿Cómo procedemos si tenemos problemas?.\nPosibles problemas:\n\nLas propiedades teóricas de T son complicadas.\n\nNo hay resultados asintóticos.\n\nLa muestra observada es pequeña.\n\n\nTécnicas Bootstrap [Man] (remuestreo con reemplazamiento)\n\nTécnica Bootstrap\nCuando no tenemos información de la población, la distribución empírica de una muestra aleatoria es la mejor representación de la distribución de la población ==&gt; La muestra observada se toma como modelo de la distribución desconocida.\nSi hay un resultado teorico, al teorico. Si hay una aproximación, a la apriximación. Si no hay información suficientepara tirar por lo asintótico (asintótico AKA apoximación) y tengo poca muestra: Bootstrap.\nPara mejorar el conocimiento de la distribución real la técnica bootstrap realiza muestreos con reemplazamiento teniendo en cuenta la distribución empírica.\n\nSi las características del estimador no son conocidas o son muy complejas o tengo muestra muy pequeña, uso Bootstrap. Bootstrap no crea ni destruye nada.\n\nRemuestreo. Saco muestras del mismo tamaño que la inicia. “Mi muestra era esta, pero si tomara nuevas muestras del mismo tamaño mi población sería esta”.\nFinalidad.\n- Validar, mediante intervalos de confianza, la estimación del parámetro que se consigue a partir de la muestra observada.\n- Realizar contrastes de hipótesis.\nProcedimiento.\n\nSean \\((y_1, …, y_n)\\) los resultados de una medida X en n sujetos independientes.\n\nSea \\(\\theta\\) una cantidad referida a X (valor medio, mediana, desviación…).\n\nCon los resultados observados podemos calcular el valor estimación de \\(\\theta\\): \\(\\hat{\\theta}\\)\n\n\nValidación de la estimación por IC.\n\nSimulamos una nueva muestra (\\(y*_1, …, y*_n\\)) remuestreando con repetición en los resultados iniciales y calculamos el valor de \\(\\hat{\\theta*}\\).\n\nRepetimos el proceso r-veces obteniendo r estimaciones bootstrap: \\(\\hat{\\theta_i*}\\), i=1,2,…r.\n\n¿Cuántas muestras? Eso es el capítulo final del tema.\n\n\nCalculamos las diferencias entre las estimaciones bootstrap y la estimación conseguida con la muestra inicial: \\(d_i = \\hat{\\theta_i*} - \\hat{\\theta}\\).\n\nObtenemos los cuantiles asociados \\(\\alpha/2\\) y \\(1-\\alpha/2\\): \\(d_b\\), \\(d_u\\).\n\nEl intervalo de confianza bootstrap \\(1-\\alpha\\) es: \\([\\hat{\\theta} + d_b, \\hat{\\theta} + d_u]\\)\n\n\nLa muestra original la guardo y la dejo apartada y trabajo con las muestras de Bootstrap (bueno esto volver a preguntárselo de cara a algún ejercicio pq tampoco creo si me ha contestado lo mismo dos veces seguidas). Trabajamos solo con las r estimaciones Bootstrap."
  },
  {
    "objectID": "tema_03/tema_03_3_obtener_datos_simulados.html#simulación-no-paramétrica",
    "href": "tema_03/tema_03_3_obtener_datos_simulados.html#simulación-no-paramétrica",
    "title": "3. Obtener datos simulados a partir de observaciones reales ✓",
    "section": "Simulación no paramétrica",
    "text": "Simulación no paramétrica\nNo se asume una distribución teórica, se remuestrea los datos originales para simular nuevas muestras.\ndiapo43 es la distribución del estadístico\npero no es la distribucion de p, sino del estadñistico\ncon la simulacion ya no tengo estimaciones, tengo estimaciones de la probabilidades\nlas muestras no tienen probabilidad, pq las muestras están fijas. tienen frecuencias, no dudo. en el modelo teórico tengo algo genérico, ahí sí hablo de probabilidades"
  },
  {
    "objectID": "tema_03/tema_03_3_obtener_datos_simulados.html#ejemplo-validar-índice-kappa",
    "href": "tema_03/tema_03_3_obtener_datos_simulados.html#ejemplo-validar-índice-kappa",
    "title": "3. Obtener datos simulados a partir de observaciones reales ✓",
    "section": "Ejemplo: Validar índice Kappa",
    "text": "Ejemplo: Validar índice Kappa\nSupongamos que se examinan 20 radiografías de la columna con el fin de detectar daños en la misma. Un par de radiólogos examinan las placas y emiten su diagnóstico: N = sin daño, I = daño incipiente, S = daño severo.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{N} & \\textbf{I} & \\textbf{S} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{N} & 6 & 1 & 0 \\\\\n                      & \\textbf{I} & 1 & 3 & 2 \\\\\n                      & \\textbf{S} & 0 & 3 & 4 \\\\\n\\hline\n& & & & & 20 \\\\\n\\end{array}\n\\]\n\\(\\hat\\kappa_0\\): estimacion con la muestra inicial\nn=20 \\(\\Rightarrow\\) Validacion con la metodlogía bootstrap. - Remuestreo entre las 20 diapos, cada diapo tiene 2 clasificaciones - Para cada muestra tengo una estimación de \\(\\hat\\kappa_{b_{1}}\\). - Haste tener \\(\\hat\\kappa_{b_{1000}}\\), tendré mil valores de mis estimaciones\n\n# 0 = sin lesion, 1 = daño leve, 2 = daño severo\nlibrary(psych)\nrad1 &lt;- c(2,1,0,0,2,0,0,0,2,2,1,1,2,0,1,1,2,2,1,0)\nrad2 &lt;- c(1,1,0,0,2,0,0,0,2,2,1,2,1,1,1,0,2,1,2,0)\n\n\ncokapp &lt;- cohen.kappa(x&lt;-cbind(rad1,rad2))\n#str(cokapp)\ncokapp$kappa\n\n[1] 0.4756554\n\nCoefi &lt;- cokapp$kappa\n\nBootstrap para Kappa\n\n#estructura bootstrap\nN_boot &lt;- 2000\nnn &lt;- length(rad1)\nB1 &lt;- numeric(nn)\nB2 &lt;- numeric(nn)\nk_boo &lt;- N_boot\n\n\n#Remuestreo en las placas de radiografías\nset.seed(108)\ntmp1 &lt;- sample(1:nn, nn*N_boot, replace=TRUE)\n\n\n# Asignamos a cada valor tmp1 la opinion de los radiologos\n# B1 para el radiólogo 1 y B2 para el radiólogo 2\n# Calculamos kappa en cada muestra bootstrap\n\nfor(j in 1:N_boot){\n  jj &lt;- j-1\n  for( i in 1:nn){\n    B1[i] &lt;- rad1[tmp1[nn*jj+i]]\n    B2[i] &lt;- rad2[tmp1[nn*jj+i]] }\n  y &lt;- cbind(B1,B2)\n  ckb &lt;- cohen.kappa(y)\n  k_boo[j] &lt;- ckb$kappa\n}\n\ndiff &lt;- k_boo - cokapp$kappa\ncuantiles &lt;- quantile(diff, c(.05, .95))\n(IC_kappa &lt;- cokapp$kappa + c(cuantiles[1], cuantiles[2]))\n\n       5%       95% \n0.1821561 0.7014925 \n\n\nVamos a analizar las diferencias entre la estimacion de la muestra inicial m0 contra cada una de estas estimaciones:\n\nConcordancia desde insignificante hasta sustancial.\nMuy muy muy válida la estimación no es.\n\nNo es porque la hayamos hecho mal, sino porque no podemos defender a muerte nuesta estimacion puntual.\nLo que sí podemos decir es que algún criterio común tienen."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Camargo-Ramos, C. M., and otros autores. 2012. “Evaluación de\nFactores Asociados Al Embarazo Adolescente.” Revista\nColombiana de Obstetricia y Ginecología 61 (3): 256–62. http://www.scielo.org.co/pdf/rcog/v61n3/v61n3a09.pdf.\n\n\nFisterra. 2024. “La Fiabilidad de Las Mediciones Clínicas:\nAnálisis de La Concordancia Para Variables Numéricas.” 2024. https://www.fisterra.com/formacion/metodologia-investigacion/la-fiabilidad-mediciones-clinicas-analisis-concordancia-para-variables-numericas/."
  }
]