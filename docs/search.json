[
  {
    "objectID": "index.html#evaluación",
    "href": "index.html#evaluación",
    "title": "Introducción",
    "section": "Evaluación",
    "text": "Evaluación\nTema 1: un examen (entre finales de octubre y noviembre)\nTema 2: un examen (el último día de clase)\nNota final: promedio de exámenes. (pilar de la nota final)\nNota final + varias entregas hasta un total de un punto (en total serán seis entregas, no son obligatorias)."
  },
  {
    "objectID": "index.html#temario",
    "href": "index.html#temario",
    "title": "Introducción",
    "section": "Temario",
    "text": "Temario\nDos primeros temas: pruebas diagnósticas. (probabildad) Tercer tema: tema transversal dirante el cuatrimestre, relacionado con la creación de muestras. (simulación)"
  },
  {
    "objectID": "DUDAS.html#ic-del-parámetro-por-el-método-de-los-momentos.",
    "href": "DUDAS.html#ic-del-parámetro-por-el-método-de-los-momentos.",
    "title": "Dudas",
    "section": "2.1 IC del parámetro por el método de los momentos.",
    "text": "2.1 IC del parámetro por el método de los momentos.\nEstamos haciendo IC por el método de los momentos. Entonces, cuando el parámetro a estimar es la media y teniendo en cuenta que la media tiene distrubución normal, ¿podemos hacer el IC por el método “clásico”?\n\nRESPUESTA.\n\nVale, nop.\nDos cosas:\n- En el método de los percentiles (el de clase) parece que es “innecesario” hacer las diferencias porque cuando buscamos los percentiles de las diferencias son los percentiles de una transformación lineal que luego deshacemos. Esto solo lo es para la media. Para la mediana, varianza, etc. no existe esa similitud.\n- No podemos hacer el método “clásico” tal cual porque al incluir el sesgo para calcular la amplitud del intervalo, este sesgo debe ser simulado por bootstrap con cada una de las muestras simuladas."
  },
  {
    "objectID": "DUDAS.html#qué-cojones-significa-validación-de-la-estimación",
    "href": "DUDAS.html#qué-cojones-significa-validación-de-la-estimación",
    "title": "Dudas",
    "section": "2.2 ¿Qué cojones significa validación de la estimación?",
    "text": "2.2 ¿Qué cojones significa validación de la estimación?\nRESPUESTA.\nSignifica cómo de válida es mi estimación para representar a mi población.\nSi mi media es 3.5 pero la muestra va de 3 a 9 (suponiendo que 9 no es outlier) no es representativo.\nTambién ligado a la amplitud de mi IC (en relación con el rango de los datos muestrales)."
  },
  {
    "objectID": "DUDAS.html#la-diferencia-entre-hattheta-y-hattheta-se-podría-llamar-sesgo",
    "href": "DUDAS.html#la-diferencia-entre-hattheta-y-hattheta-se-podría-llamar-sesgo",
    "title": "Dudas",
    "section": "2.3 La diferencia entre \\(\\hat\\theta\\) y \\(\\hat\\theta*\\), ¿se podría llamar sesgo?",
    "text": "2.3 La diferencia entre \\(\\hat\\theta\\) y \\(\\hat\\theta*\\), ¿se podría llamar sesgo?\nSeguro que no."
  },
  {
    "objectID": "DUDAS.html#concordancia-vs-chi2",
    "href": "DUDAS.html#concordancia-vs-chi2",
    "title": "Dudas",
    "section": "2.4 Concordancia vs \\(\\chi^2\\)",
    "text": "2.4 Concordancia vs \\(\\chi^2\\)"
  },
  {
    "objectID": "DUDAS.html#existe-un-intervalo-o-p-valor-asociado-a-la-medida-de-la-concordancia",
    "href": "DUDAS.html#existe-un-intervalo-o-p-valor-asociado-a-la-medida-de-la-concordancia",
    "title": "Dudas",
    "section": "2.5 ¿Existe un intervalo o p-valor asociado a la medida de la concordancia?",
    "text": "2.5 ¿Existe un intervalo o p-valor asociado a la medida de la concordancia?"
  },
  {
    "objectID": "DUDAS.html#concordancia-dos-observadores-cuando-tenemos-solo-un-observador",
    "href": "DUDAS.html#concordancia-dos-observadores-cuando-tenemos-solo-un-observador",
    "title": "Dudas",
    "section": "2.6 Concordancia dos observadores cuando tenemos solo un observador",
    "text": "2.6 Concordancia dos observadores cuando tenemos solo un observador\nLos métodos para medir la concordancia entre dos observadores/pruebas aplicados a dos mediciones de un mismo agente, ¿sirve como fiabiidad? ¿O tenemos otros métodos para medir la fiabilidad?"
  },
  {
    "objectID": "tema_00/tema_00_introduccion.html#propósito",
    "href": "tema_00/tema_00_introduccion.html#propósito",
    "title": "Introducción",
    "section": "Propósito",
    "text": "Propósito\nClasificación de un individuo dentro de un grupo de categorías.\nPrueba diagnóstica: cualquier prueba que me hacen. Una prueba diagnostica también puede ser un examen, ya que evaluo un conocimiento y creo una clasificacion.\nVer la concordancia entre varias pruebas o metodologías.\nUn examen, independientemente de qué profesor lo corrija, debería tener práctiamente la misma nota.\nTodo contraste estadístico podemos usarlos para varias acciones. “Un test puede ser usado para medir algo y también todo lo opuesto”.\nConcordancia = acuerdo.\nNo concordancia = actuar de manera idependiente = independientes.\nSi tengo varias metodologías para un mismo propósito (ej: enseñar a bebés a nadar) tengo que ver qué metodología funciona mejor.\nSi para una prueba tengo dos procedimientos y concuerdan entre ellos, puedo usar uno o otro. Si no hay concordancia mezclar los procedimientos me lleva a error.\nMuestra hetérea sin depender de datos muestrales.\nUn modelo que yo le dé cualquier muestra o información, y no un modelo que sirva solo para lo que he observado.\nLos registros simulados tiene que tener las mismas características que los datos muestrales.\nCómo consigo datos muestrales:\n- Datos poblacionales.\n- Simulacion, datos de individuos ficticios.\nProblema: datos faltantes de una de las variables de la muestra. Vamos a hablar más de los problemas que vamos a encontrar que cómo solucionarlos.\nDebemos ser conscientes de que la muestra actual es poco válida para experimentos futuros. Y posiblemente sea necesario ajustes del modelo en el futuro. \\(\\Rightarrow\\) Al igual que calibramos modelos, calibramos la muestra simulada y el modelo teórico.\nEn la vida real es al revés. Te dan unos datos y tienes que crear un modelo, es decir, crear algo que los represente."
  },
  {
    "objectID": "tema_00/tema_00_introduccion.html#planteamiento-estadístico",
    "href": "tema_00/tema_00_introduccion.html#planteamiento-estadístico",
    "title": "Introducción",
    "section": "Planteamiento ‘estadístico’",
    "text": "Planteamiento ‘estadístico’\n\nUn modelo teórico que podamos aplicar a cualquier conjunto de datos.\nX es una variable aleatoria y es la base del modelo teórico\nLa idea es estudiar X, conocer su comportamiento, tomar decisiones sobre X y aplicarlas sobre datos reales.\n¿La variable X ha sido estudiada antes?\n– Sí. Puedo utilizar esos resultados y aplicarlos a mi problema.\n– No. Debo crear el modelo y estudiarlo teóricamente.\n\n\nModelo teórico\nRepresentación matemática abstracta que describe el comportamiento de un fenómeno o conjunto de datos. Permite realizar inferencias, predicciones y análisis de fenómenos aleatorios o deterministas, y se basan en suposiciones sobre la estructura y distribución subyacente de los datos.\n\nComponentes\n\nVariables (características)\nParámetros. Valores desconocidos que determinan la forma de la distribución de datos.\nDistribución de probabilidad.\nSuposiciones sobre el comportamiento de los datos. Por ejemplo, en un modelo de regresión lineal simple se asume que la relación entre las variables es lineal, que los errores tienen una distribución normal con media cero y varianza constante y que son independientes entre sí.\n\n\n\nUso\nEntender la estructura subyacente de los datos, hacer predicciones sobre nuevas observaciones y probar hipótesis.\nEl modelo teórico tiene su propia unidad de medida: la probabilidad.\nIt’s not the same probabilidades que porcentajes, es impreciso. Si p(“enfermo”) = 0.10 no podemos decir que el 10 % de la pobaclión esté enferma, aunque usemos ese lenguaje. Lo correcto es decir que la probabilidad de que una persona de la población esté enferma es 10 %.\nEl IC no es para el parámetro. El IC es para el estimador del parámetro. El IC contiene el % de las muestras del resto del mundo.\nDebemos interpretar las estimaciones puntuales y la distribución acumulada FDA/CFD."
  },
  {
    "objectID": "tema_00/tema_00_introduccion.html#iteraciones-de-todo-problema-estadístico",
    "href": "tema_00/tema_00_introduccion.html#iteraciones-de-todo-problema-estadístico",
    "title": "Introducción",
    "section": "Iteraciones de todo problema estadístico",
    "text": "Iteraciones de todo problema estadístico\nCuestiones asociadas:\n\n¿A qué alumnos? A todos los alumnos.\n\nTengo una oferta si hago un pedido grande.\n\n¿El modelo es único? No, hay tres tallas: P, M y G.\n\nNecesito: Medir la circunferencia del dedo de los alumnos. ¿A todos? ¿qué dedo?\n\n¿Quién aporta la información de 4? Los alumnos del curso actual. ¿Cómo se realiza la medida?\n\n\n\n\nHay varios procedimientos ¿Por ejemplo?\n\n\n\nLos resultados de cada procedimiento\n\n\n\n\nConcuerdan (puedo utilizar cualquiera o mezclarlos) (*1)\n\nHay alguno “mejor” (*2)\n\n\n\nCon (todas/parte de) las medidas de los alumnos establecemos las tallas P, M y G\n\nDecisión sobre la cantidad a pedir, en total y de cada talla\nPoner en práctica → REVISAR: Si hay errores volver a 4"
  },
  {
    "objectID": "tema_00/tema_00_introduccion.html#ejemplo",
    "href": "tema_00/tema_00_introduccion.html#ejemplo",
    "title": "Introducción",
    "section": "Ejemplo",
    "text": "Ejemplo\nEl modelo teórico asociado a este tipo de epidemias indica que:\n– La probabilidad de que en una familia la madre tenga gripe es 0.1\n– En el 12% de las familias el padre tiene gripe\n– Ambos progenitores tienen gripe en el 2% de las familias (= con probabilidad 0.02)\n\n\n\n\n\n\nImportante\n\n\n\nSi sé controlar el comportamiento de una v.a. y sé simular cien familias, ¿qué esperaría si tengo una simulación buena? Que en estas cien simulaciones haya un comportamiento parecido a las bases del modelo teórico.\nCon el modelo teorico tendré dudas pero con datos simulados tangibles ya no tengo probabilidades, tengo un hecho.\n\n\nPara simular una familia simularía pares de datos (padres). Es decir, ´trabajamos con la distribución conjunta. Simularía el 00, el 01, el 10 y el 11. Esto nos permite usar un único número aleatorio por familia, ya que con una única simulación puedo asignar el estado del par de datos (no necesito simular la madre y luego el padre).\nFijo el estado más probables para todas las familias y si sale la condición de los menos frecuentes, lo cambio.\n0.0 - 0.2 pongo 11 0.2 - 0.3 pongo 10 0.3 - 0.5 pongo 01 0.5 - 1.0 pongo 00\nSi quiero algo MÁS ROBUSTO necesito más muestra.\n\n\n\n\n\n\nNota\n\n\n\nEn un modelo teórico tengo un suceso con una probabilidad tal. Con los datos simulados tengo un porcentaje de cada suceso."
  },
  {
    "objectID": "tema_01/tema_01.html#medidas-de-precisión-en-pruebas-diagnósticas.-índices-de-concordancia",
    "href": "tema_01/tema_01.html#medidas-de-precisión-en-pruebas-diagnósticas.-índices-de-concordancia",
    "title": "TEMA 1",
    "section": "Medidas de precisión en pruebas diagnósticas. Índices de concordancia",
    "text": "Medidas de precisión en pruebas diagnósticas. Índices de concordancia\nSimulación numérica para desarrollar pruebas diagnósticas.\nAnálisis de concordancia: se analiza de manera distinta dependiendo del objetivo (y tipo de variables)\nSiempre es lo mismo, árbitro!\nModelo teótico \\(\\Rightarrow\\) Realidad \\(\\Rightarrow\\) Estimamos\nNo es posible dar un valor de concordancia único sino un grado de amplitud. (el nuestro es este grado pero podíamos haber obtenido en un x % de los casos un valor en este intervalo)"
  },
  {
    "objectID": "tema_01/tema_01_1_introduccion.html#definiciones-basadas-o-pensadas-sobre-el-modelo-teórico",
    "href": "tema_01/tema_01_1_introduccion.html#definiciones-basadas-o-pensadas-sobre-el-modelo-teórico",
    "title": "1. Introducción ✓",
    "section": "Definiciones basadas o pensadas sobre el modelo teórico",
    "text": "Definiciones basadas o pensadas sobre el modelo teórico\nPrevalencia\nProbabilidad de que se observe algo. (porque estamos en el modelo teórico)\nPueba diagnóstica\nPrueba que se usa para ayudar a diagnosticar una enfermedad o afección según los signos y síntomas que presenta una persona. Las pruebas diagnósticas también se usan para diseñar un tratamiento, determinar la eficacia de un tratamiento y hacer un pronóstico.\nRápidamente pensamos que es algo clínico pero no siempre es así. Puede ser un examen donde yo asigne un nivel de conocimiento. La prueba diagnóstica puede fallar porque puede dar una nota menor a la real o dar un mayor conociemiento que el verdadero.\nLas pruebas diagnósticas clínicas o experimentales sirven para localizar y situar a un individuo dentro de una clasificacion precisa (una medida según un baremo) o grupo que le corresponde (aprobado/enfermo, sano/enfermo, brazo roto/brazo no roto).\nDependiendo del tipo de variable al que esté asociada la p.d. nos indica qué metodología vamos a usar.\nDebo tener claro para qué fin realizo la p.d..\nPara ver si una prueba diagnóstica cumple su objetivo nos apoyamos en estimaciones basadas en el modelo teórico. Las estimaciones están basadas en los valores que nos den los individuos. Para tener buenas estimaciones necesito buenas medidas. Un aspecto fundamental en estudios de investigación es garantizar la calidad de los procedimientos de medida. La calidad de una medida depende tanto de su fiabilidad como de su validez.\n\nLa fiabilidad indica hasta qué punto se obtienen los mismos valores al efectuar la medición en más de una ocasión, bajo condiciones similares.\n\nLa validez expresa el grado en el que realmente se mide el fenómeno de interés.\n\nQue una medida sea muy precisa no implica que sea necesariamente válida. Por ejemplo, si se realizan dos mediciones consecutivas a un paciente con una herramienta mal calibrada los valores obtenidos seguramente serán parecidos aunque inexactos. (Fisterra 2024)"
  },
  {
    "objectID": "tema_01/tema_01_1_introduccion.html#clasificación-de-estudios-para-la-evaluación-de-la-calidad-de-los-procedimientos-de-medidas",
    "href": "tema_01/tema_01_1_introduccion.html#clasificación-de-estudios-para-la-evaluación-de-la-calidad-de-los-procedimientos-de-medidas",
    "title": "1. Introducción ✓",
    "section": "Clasificación de estudios para la evaluación de la calidad de los procedimientos de medidas",
    "text": "Clasificación de estudios para la evaluación de la calidad de los procedimientos de medidas\n\nFiabilidad: comparación/variación consigo mismo. (concordancia intraobservador)\n\nConcordancia: comparación/variación con otro observador (interobservador) o entre métodos.\n\nCalibración: comparación/variación con un método estándar.\n\n\n\n\n\n\n\nFiabilidad, repetibilidad.\nMedidas fiables/repetibles/reproducibles.\nEstabilidad \\(\\Rightarrow\\) Individuos con caracteristicas similares darán lugar a medidas similares.\nEstudios de fiabilidad. Intentan evaluar como concuerdan las medidas obtenidas por un único método o instrumento. Se evalúa el error de medida del método mediante el estudio de la concordancia intramétodo, si las medidas concuerdan se puede decir que el método es repetible.\n\n\nConcordancia.\nSi la damos la vuelta podríamos pensar en independencia. Personas que funcionan de forma independiente. (no lo termino de pillar)\nEstudios de concordancia: se desea evaluar como concuerdan las medidas realizadas con el método cuya calidad se desea valorar con los obtenidos por otro método. Valoramos la concordancia entre métodos de medida con el objetivo de determinar si ambos son intercambiables.\nLas técnicas utilizadas para estudiar la concordancia varían según la naturaleza de las variables, dependiendo de si las medidas corresponden a una escala de medida cualitativa o cuantitativa.\nLa concordancia adquiere importancia cuando se desea conocer si con un método o instrumento nuevo, diferente al habitual, se obtienen resultados equivalentes de tal manera que eventualmente uno y otro puedan ser remplazados o intercambiados ya sea porque uno de ellos es más sencillo, menos costoso y por lo tanto más costo-efectivo, o porque uno de ellos resulta más seguro para el paciente, entre otras múltiples razones. En términos generales, la concordancia es el grado en que dos o más observadores, métodos, técnicas u observaciones están de acuerdo sobre el mismo fenómeno observado. La concordancia no evalúa la validez o la certeza sobre una u otra observación con relación a un estándar de referencia dado, sino cuán acordes están entre sí observaciones sobre el mismo fenómeno. (Camargo-Ramos y autores 2012)\n\n\nCalibración.\nNo solo queremos que no haya errores de medida. Cuando calibramos algo lo calibramos con un instrumento. Calibrar es comparar las medidas que tenemos con un método con otras medidas que están aprobadas por todos (gold standar).\nEstudios de calibración: pueden verse como caso particular de los estudios de concordancia entre métodos, cuando se compara un procedimiento de medida con los valores reales o de referencia (gold standard).\n\nEn el mundo clínico existe mucha confusión debido a que heredamos mucha terminología inglesa ya que a veces no tiene una equivalencia.\nLa medida de un individuo vendrá dada por características que podemos medir y por algo intrínseco del propio individuo. ¿Qué factores influyen para que haya resultados diferentes entre individuos con las mismas características? Esos factores son el error. Queremos medir esa parte intrínseca al individuo y que no podemos incluir en el modelo.\nEl error lo queremos controlar midiéndolo con diferentes metodologías. ¿El error es diferente dependiendo de la metodología?\n\\[\n\\displaylines{\n& \\text {Sea X el resultado de la prueba diagnóstica} \\\\\n&\\begin{array}{cccc}\n\\hline  \\text { Tipo de la variable X } & \\text { Objetivo de la prueba } & \\text { Índice o argumento de concordancia } \\\\\n\\hline\n\\text {Cualitativo} & \\text {La prueba clasfica} & \\text {Coeficiente kappa} \\\\\n\\text {Cuantitativo} & \\text {Tiene una unidad de medida} & \\text {Concordancia intraclase} \\\\\n&  & \\text {Método gráfico de Bland y Altman} \\\\\n\\hline\n\\end{array}\n}\n\\]\n\n\n\n\nCamargo-Ramos, C. M., y otros autores. 2012. «Evaluación de factores asociados al embarazo adolescente». Revista Colombiana de Obstetricia y Ginecología 61 (3): 256-62. http://www.scielo.org.co/pdf/rcog/v61n3/v61n3a09.pdf.\n\n\nFisterra. 2024. «La fiabilidad de las mediciones clínicas: análisis de la concordancia para variables numéricas». 2024. https://www.fisterra.com/formacion/metodologia-investigacion/la-fiabilidad-mediciones-clinicas-analisis-concordancia-para-variables-numericas/."
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#clasificaciones-binomiales",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#clasificaciones-binomiales",
    "title": "2. Análisis de concordancia en variables categóricas ✓",
    "section": "Clasificaciones binomiales",
    "text": "Clasificaciones binomiales\nCasos donde la prueba diagnóstica solo presenta dos resultados distintos.\nAnalizamos si dos agentes que clasifican a los individuos según el resultado de la p.d. concuerdan en sus opiniones. También me sirve la discordancia completa. (sé que un agente siempre clasifica lo contrario al otro agente)\nComo la p.d. depende del agente que me da la medida, tengo una variable aleatoria por cada agente.\nX = “clasificación dada por el observador A.”\nY = “clasificación dada por el observador B.”\nDado un individuo este es analizado por el agente A y por el agente B.\n\\[\n\\displaylines{\n& \\text {Medidas para cada inidividuo} \\\\\n&\\begin{array}{ccc}\n\\hline\n\\text{individuo k} & X_k & Y_k\n\\end{array}\n}\n\\]\nResumen de la información del modelo teórico.\nSuponemos distribución de probabilidad conjunta.\n\\[\n\\begin{array}{c|c c c c}\n    & & \\textbf{Y} & & \\\\\n    & \\textbf{Categoría} & \\textbf{1} & \\textbf{2} & \\textbf{Total} \\\\\n\\hline\n\\textbf{X} & \\textbf{1} & \\pi_{11} & \\pi_{12} & \\pi_{1\\cdot} \\\\\n           & \\textbf{2} & \\pi_{21} & \\pi_{22} & \\pi_{2\\cdot} \\\\\n\\hline\n& \\textbf{Total} & \\pi_{\\cdot1} & \\pi_{\\cdot2} & 1 \\\\\n\\end{array}\n\\]\n\\[\n\\begin{array}{}\n&\\pi_{.j} = (X,Y) \\epsilon {(1,1),(1,2),(2,1),(2,2)} \\\\\n\\pi_{ij} = \\text{p({X=i, Y=j})} & \\pi_{i.} = \\text{p({Y=j})} & \\pi_{.j} =  \\text{p({X=i})} \\\\\n\\end{array}\n\\]\nAcuerdo observado o Índice de concordancia global\nEs la aproximación a la concordancia más intuitiva. Expresa el porcentaje de coincidencia en la clasificación de ambos agentes.\nEl problema que plantea este índice básico es que una parte de ese acuerdo puede deberse exclusivamente al azar.\nMedir la concordancia es saber la probabilidad con la que los dos dan la misma clasificación.\n\nMedida de concordancia global: \\(\\Pi_0 = \\pi_{11} + \\pi_{22} = \\sum_{i=j} \\pi_{ij}\\)\n\nEjemplo 1.0.\nSe recogen las respuestas a las entrevistas 1 y 2 de manera independiente.\nEntrevista 1: ¿Consume usted suplementos vitamínicos?\nEntrevista 2: Responda si consume vitaminas sin contar sus aportes alimentarios\n\n\nCaso 1. Concordancia perfecta. {X=Y} o {X\\(\\neq\\)Y}\np({X=Y})=1\n\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.6 & 0 & 0.6 \\\\\n                      & \\textbf{Sí} & 0 & 0.4 & 0.4 \\\\\n\\hline\n& \\textbf{Total} & 0.6 & 0.4 & 1 \\\\\n\\end{array}\n\\]\n\nLa probabilidad de que un individuo conteste lo mismo en ambos cuestionarios es 1.\n\n\nCaso 2. Concordancia.\n\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{2} & \\textbf{Sí} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.4 & 0.2 & 0.6 \\\\\n                      & \\textbf{Sí} & 0.2 & 0.2 & 0.4 \\\\\n\\hline\n& \\textbf{Total} & 0.6 & 0.4 & 1 \\\\\n\\end{array}\n\\]\n\nLa probabilidad de dar la misma respuesta es 0.8.\n\n\nCaso 3. Sin concordancia.\n\n\nLas dos entrevistas concuerdan el 40 % de las ocasiones. Puede no haber asociación real entre las dos entrevistas porque toda la concordancia puede atribuirse al azar.\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.3 & 0.3 & 0.6 \\\\\n                      & \\textbf{Sí} & 0.3 & 0.1 & 0.4 \\\\\n\\hline\n& \\textbf{Total} & 0.6 & 0.4 & 1 \\\\\n\\end{array}\n\\]\n¿Cuál sería la distribución de las respuestas si los individuos responden tirando una moneda? (cara: no, cruz: sí)\np(X=“no”, Y=“sí”) = p(“sale cara primera moneda”, “sale cara segunda moneda”) = 0.5 * 0.5 = 0.25\np(X=“sí”, Y=“no”) = p(X=“no”, Y=“no”) = p(X=“sí”, Y=“sí”) = 0.25\n\\(\\pi_{0}\\) = 0.25 + 0.25 = 0.5\\(\\pi_{i} = \\pi_{1.} * \\pi_{.1} + \\pi_{2.} * \\pi_{.2}\\) = 0.5 * 0.5 + 0.5 * 0.5 (concordancia debida al azar)\n\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.25 & 0.25 & 0.5 \\\\\n                      & \\textbf{Sí} & 0.25 & 0.25 & 0.5 \\\\\n\\hline\n& \\textbf{Total} & 0.5 & 0.5 & 1 \\\\\n\\end{array}\n\\]\n\nEsta sería la máxima no concordancia, ya que si la diagonal principal acumula menos de la mitad de los casos podríamos medir una concordancia inversa.\n\nEl problema de \\(\\Pi_{0}\\) es que incluye concordancia que puede atribuirse al azar. Y concordar por azar no es concordar.\nY bailar de lejos no es bailar.\nSi los dos observadores clasificasen de forma independiente y, por tanto, totalmente al azar entre las dos categorías, la probabilidad de la concordancia \\(p(X=Y)\\) sería:\n\nMedida de concordancia debida al azar: \\(\\Pi_c = \\pi_{1.}\\pi_{.1} + \\pi_{2.}\\pi_{.2} = \\sum_{k=1,2} \\pi_{k.}\\pi_{.k}\\)\n\nA la concordancia global debo restarle la concordancia del azar.\nÍndice kappa (de Cohen)\nEste índice es una medida basada en resultados teóricos.\n\\[k = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c}\\] tal que \\[\\Pi_0 = \\pi_{11} + \\pi_{22} \\quad \\text{y} \\quad \\Pi_c = \\pi_{1.}\\pi_{.1} + \\pi_{2.}\\pi_{.2}\\]\nA la concordancia le quita la concordancia debida al azar y lo compara con la concordancia perfecto sin contar la concordancia al azar. Es decir, a la concordancia y a la concordancia perfecta les substrae la concordancia debida al azar y compara la concordancia sin el azar obtenida contra la máxima concordancia sin el azar posible.\nPodemos tener resultados negativos: el intervalor [-1,0) es sin corcondancia.\nEjemplo 1.0.\n\nCaso 1. Concordancia perfecta.\n\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.6 & 0 & 0.6 \\\\\n                      & \\textbf{Sí} & 0 & 0.4 & 0.4 \\\\\n\\hline\n& \\textbf{Total} & 0.6 & 0.4 & 1 \\\\\n\\end{array}\n\\]\n\nCuando tengo una concordancia perfecta, la concordancia debido al azar es 0.5.\n\\(\\Pi_c = \\pi_{1.}\\pi_{.1} + \\pi_{2.}\\pi_{.2} = 0.36 + 0.24 = 0.5\\)\nY POR EEEEEEEEEEEESO, usamos el índice kappa del señor Cohen.\n\n\n\nCaso 3. Sin corcondancia.\n\nSi la concordancia observada \\(\\Pi_0\\) coincide con la concordancia esperada por el azar \\(\\Pi_1\\) el índice kappa toma el valor 0.\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.25 & 0.25 & 0.5 \\\\\n                      & \\textbf{Sí} & 0.25 & 0.25 & 0.5 \\\\\n\\hline\n& \\textbf{Total} & 0.5 & 0.5 & 1 \\\\\n\\end{array}\n\\] \\(\\Pi_0 = \\pi_{11} + \\pi_{22} = 0.5\\) y \\(\\Pi_c = \\pi_{1.}\\pi_{.1} + \\pi_{2.}\\pi_{.2} = 0.5*0.5 + 0.5*0.5 = 0.5\\)\n\\(k = \\frac{0.5-0.5}{1-0.5} = 0\\)\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.3 & 0.3 & 0.6 \\\\\n                      & \\textbf{Sí} & 0.3 & 0.1 & 0.4 \\\\\n\\hline\n& \\textbf{Total} & 0.6 & 0.4 & 1 \\\\\n\\end{array}\n\\]\n\\(\\Pi_0 = \\pi_{11} + \\pi_{22} = 0.4\\) y \\(\\Pi_c = \\pi_{1.}\\pi_{.1} + \\pi_{2.}\\pi_{.2} = 0.6*0.6 + 0.4*0.4 = 0.52\\)\n\\(k = \\frac{0.4-0.52}{1-0.52} = -0.25\\)\n\nInterpretación del índice kappa (si \\(k\\)&gt;0)\nLos datos observados son un caso particular.\nLa probabilidad de acuerdo observado es una media ponderada del máximo acuerdo y del acuerdo debido al azar \\(\\Pi_c\\), siendo \\(k\\) el peso del máximo acuerdo.\n\n\\[\n\\displaylines{\nk = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c} \\\\ \\\\\nk * (1-\\Pi_{i}) = (\\Pi_{0} - \\Pi_{c}) \\\\ \\\\\n\\Pi_{0} = k + (1-k)*\\Pi_{c}\n}\n\\]\nEl acuerdo observado es una combinación de la concordania perfecta y concordania debida al azar. El ínidce kappa da más peso a uno u otro.\n\nEjemplo 1.1.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.04 & 0.06 & 0.10 \\\\\n                      & \\textbf{No pulmonía} & 0.10 & 0.80 & 0.90 \\\\\n\\hline\n& \\textbf{Total} & 0.14 & 0.86 & 1 \\\\\n\\end{array}\n\\]\n\ndata &lt;- matrix(c(0.04, 0.06, 0.10, 0.80), nrow = 2, byrow = TRUE,\n               dimnames = list(c(\"Pulmonía B\", \"No pulmonía B\"),\n                               c(\"Pulmonía A\", \"No pulmonía A\")))\ndata\n\n              Pulmonía A No pulmonía A\nPulmonía B          0.04          0.06\nNo pulmonía B       0.10          0.80\n\n\n\ndf &lt;- data.frame(col1 = c(0.04, 0.10),\n                 col2 = c(0.06, 0.80)\n)\ncolnames(df) &lt;- c(\"Pulmonía A\", \"No pulmonía A\")\nrownames(df) &lt;- c(\"Pulmonía B\", \"No pulmonía B\")\ndf\n\n              Pulmonía A No pulmonía A\nPulmonía B          0.04          0.06\nNo pulmonía B       0.10          0.80\n\n\n\\(\\Pi_{0} = \\text{\"concordancia absoluta\"} = 0.04 + 0.80 = 0.84\\)\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n\n[1] 0.84\n\n\n\n(pi_0 &lt;- df[1,1] + df[2,2])\n\n[1] 0.84\n\n\n\n(pi_0 &lt;- df[\"Pulmonía B\",\"Pulmonía A\"] + df[\"No pulmonía B\",\"No pulmonía A\"])\n\n[1] 0.84\n\n\n\\(\\Pi_{1} = \\text{\"concordancia debida al azar\"} = 0.10*0.14 + 0.86*0.90 = 0.788\\)\n\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n\n[1] 0.788\n\n\n\n(pi_1 &lt;- sum(df[1]) * sum(df[1,])  + sum(df[2]) * sum(df[2,]))\n\n[1] 0.788\n\n\n\n(pi_1 &lt;- sum(data[1,])*sum(data[,1]) + sum(data[2,])*sum(data[,2]))\n\n[1] 0.788\n\n\nInterpretación.]{underline}\nLa concordancia observada en los informes (84%) está compuesta por un 24,5% de la concordancia máxima y un 78,8% de la esperada al azar.\nClasificaciones multinomiales\n\\[\n\\begin{array}{c|c c c c}\n    & & \\textbf{Y} & & \\\\\n    & \\textbf{Categoría} & \\textbf{1} & \\textbf{...} & \\textbf{t} & \\textbf{Total} \\\\\n\\hline\n\\textbf{X} & \\textbf{1} & \\pi_{11} & ... & \\pi_{1t} & \\pi_{1\\cdot} \\\\\n           & \\textbf{2} & \\pi_{21} & ... & \\pi_{2t} & \\pi_{2\\cdot} \\\\\n           & \\textbf{...} & ... & ... & ... & ... \\\\\n           & \\textbf{t} & \\pi_{2t} & ... & \\pi_{tt} & \\pi_{t\\cdot} \\\\\n\\hline\n& \\textbf{Total} & \\pi_{\\cdot1} & ... & \\pi_{\\cdot t} & 1 \\\\\n\\end{array}\n\\]\n\nMedida de concordancia global: \\(\\Pi_0 = \\sum_{i=1}^{t} \\pi_{ii}\\)\n\n\nMedida de concordancia debida al azar: \\(\\Pi_c = \\sum_{i=1}^{t} \\pi_{i.}\\pi_{.i}\\)\n\n\nÍndice kappa: \\(k = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c}\\)\n\nHay veces que nos interesa saber si la concordancia es mayor en unas categorías o en otras. Por ejemplo, en la categoría grave hay muchas concordancia pero en las categorías leve y media no. En este caso se suele estandarizar para tener dos categorías y medir concordancia para cada categoría.\nEjemplo 1.2.\n\ndf &lt;- data.frame(rbind(c(0.1125, 0.1, 0.0375),\n                       c(0.1125, 0.3625, 0.0625),\n                       c(0, 0.0375, 0.175))\n)\ncolnames(df) &lt;- c(\"Leve\", \"Moderada\", \"Grave\")\nrownames(df) &lt;- c(\"Leve\", \"Moderada\", \"Grave\")\ndf\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\n\n\n\ndf_extended &lt;- rbind(df, \"Total\" = colSums(df))\ndf_extended\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\nTotal    0.2250   0.5000 0.2750\n\n\n\ndf_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\ndf_extended\n\n           Leve Moderada  Grave  Total\nLeve     0.1125   0.1000 0.0375 0.2500\nModerada 0.1125   0.3625 0.0625 0.5375\nGrave    0.0000   0.0375 0.1750 0.2125\nTotal    0.2250   0.5000 0.2750 1.0000\n\n\nMedida de concordancia global\n\n(pi_0 &lt;- sum(diag(as.matrix(df))))\n\n[1] 0.65\n\n\nMedida de concordancia debida al azar\n\npi_1 &lt;- 0\nt &lt;- dim(df_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_extended[i,t]*df_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\n[1] 0.3834375\n\n\nÍndice kappa\n\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] 0.4323365\n\n\nEl índice kappa no mide la distancia entre las discordancias. No es lo mismo decir (X=7, Y=8) que (X=7, Y=2).\nComparando cada categoría con el resto\n\ndf\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\n\n\n\nCódigocat &lt;- \"Leve\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Leve\"\n        Leve  Otras Total\nLeve  0.1125 0.1375  0.25\nOtras 0.1125 0.6375  0.75\nTotal 0.2250 0.7750  1.00\n[1] \"Medida de concordancia global:\"\n[1] 0.75\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.6375\n[1] \"Índice kappa:\"\n[1] 0.3103448\n\n\n\nCódigocat &lt;- \"Moderada\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Moderada\"\n         Moderada Otras  Total\nModerada   0.3625 0.175 0.5375\nOtras      0.1375 0.325 0.4625\nTotal      0.5000 0.500 1.0000\n[1] \"Medida de concordancia global:\"\n[1] 0.6875\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.5\n[1] \"Índice kappa:\"\n[1] 0.375\n\n\n\nCódigocat &lt;- \"Grave\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Grave\"\n      Grave  Otras  Total\nGrave 0.175 0.0375 0.2125\nOtras 0.100 0.6875 0.7875\nTotal 0.275 0.7250 1.0000\n[1] \"Medida de concordancia global:\"\n[1] 0.8625\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.629375\n[1] \"Índice kappa:\"\n[1] 0.6290051\n\n\nHasta ahora hemos hablado de probabilidades ya que estamos con el modelo teórico. Cuando trabajamos con datos de una muestra hablaremos de porcentajes. \\(\\Rightarrow\\) “Este % es consecuencia de un acuerdo de xx y un azar de xx”\n\nEl índice kappa tiene una distribución teórica y lo estimo según una muestra. Con una poca información (la muestra) doy un intervalo. No doy intervalos de confianza para valores desconocidos sino para las estimaciones.\n\nInconvenientes del índice kappa\nEjemplo 1.1.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.04 & 0.06 & 0.10 \\\\\n                      & \\textbf{No pulmonía} & 0.10 & 0.80 & 0.90 \\\\\n\\hline\n& \\textbf{Total} & 0.14 & 0.86 & 1 \\\\\n\\end{array}\n\\]\nLa prevalencia de pulmonía es baja: 014 para el radiólogo A y 0.1 para el rediólogo B.\nAmbas marginales desequilibradas: la prevalencia observada dista mucho de 0.5 (a favor de los diagnósticos negativos)\nEl bajo valor del índice kappa se explica porque nos encontramos en el peor de los escenarios: baja prevalencia y marginales desequilibradas.\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.788\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.245283\n\nEl valor del índice kappa depende de la prevalencia de la característica observada.\nEjemplo 1.1.a.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.30 & 0.06 & 0.36 \\\\\n                      & \\textbf{No pulmonía} & 0.10 & 0.54 & 0.64 \\\\\n\\hline\n& \\textbf{Total} & 0.40 & 0.60 & 1 \\\\\n\\end{array}\n\\]\nAumentamos la prevalencia: 0.40 para el radiólogo A y 0.36 para el rediólogo B.\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.528\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.6610169\n\n\nCuanto más cercana a 0,5 sea la prevalencia (más equilibradas estén las marginales) mayor es el índice kappa, para igual probabilidad de acuerdos observados.\n\n\\[\\Downarrow\\]\n\nPrevalencias muy altas o muy bajas penalizan el índice kappa.\n\nEl valor del índice kappa depende de la simetría y homogeneidad de las marginales.\nEn el primer caso el comportamiento de los agentes es homogéneo: pues ambos emiten informes positivos con mayor frecuencia.\nEn el segundo caso el comportamiento de los agentes es heterogéneo y asimétrico.\nEn ambos casos los diagnósticos del agente A distan 0.2 de 0.5.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.45 & 0.15 & 0.60 \\\\\n                      & \\textbf{No pulmonía} & 0.25 & 0.15 & 0.40 \\\\\n\\hline\n& \\textbf{Total} & 0.70 & 0.30 & 1 \\\\\n&  & 0.50+0.20 & 0.50-0.20 &  \\\\\n\\end{array}\n\\]\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.54\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.6521739\n\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.25 & 0.35 & 0.60 \\\\\n                      & \\textbf{No pulmonía} & 0.05 & 0.35 & 0.40 \\\\\n\\hline\n& \\textbf{Total} & 0.30 & 0.70 & 1 \\\\\n&  & 0.50-0.20 & 0.50+0.20 &  \\\\\n\\end{array}\n\\]\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.46\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.7037037\n\nEl valor del índice kappa depende del equilibrio de las marginales.\nCuanto mayor sea la diferencia de la prevalencia observada de cada agente respecto de 0.5 mayor es el índice kappa. (incluso para un mismo valor de acuerdos observados).\nÍndice kappa ponderado.\nSólo tiene sentido para variables ordinales.\nEstá diseñado para recoger la idea de que algunas discordancias son más severas que otras y, por tanto, asigna pesos que representan la importancia entre los desacuerdos. El máximo peso se da a la concordancia perfecta y pesos proporcionalmente menores según la importancia del desacuerdo.\nNo tiene la misma importancia un desacuerdo en la clasificación entre las categorías leve y moderada que entre leve y grave, obviamente la última representa un mayor desacuerdo que la primera.\n\nMedida de concordancia global: \\(\\Pi_0 = \\sum_{i=1}^{t} \\sum_{j=1}^{t} w_{ij} \\pi_{ij}\\)\n\n\nMedida de concordancia debida al azar: \\(\\Pi_c = \\sum_{i=1}^{t} \\sum_{j=1}^{t} w_{ij} \\pi_{i.}\\pi_{.j}\\)\n\n\nÍndice kappa: \\(k_{w} = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c}\\)\n\nLos pesos satisfacen:\n\\[\nw_{ij} = \\begin{cases}\nw_{ii} = 1 \\\\\n0 \\le w_{ij} \\le 1 \\\\\nw_{ij} = w_{ji}\n\\end{cases}\n\\]\n\\[\nw_{ij} = \\begin{cases}\n1, \\hspace{1em} i = j \\\\\n0, \\hspace{1em} i \\neq j \\\\\n\\end{cases} \\hspace{1em} \\text{sii} \\hspace{1em} k_{w} = k \\\\\n\\] \\[\n\\text{si t = 2} \\Rightarrow k_{w} = k\n\\]\nPonderaciones.\n\nCicchetti-Allison (1971) \\(w_{ij} = 1 - \\frac{|i-j|}{t-1}\\)\nFleiss-Cohen (1973) \\(w_{ij} = 1 - \\frac{(i-j)^2}{(t-1)^2}\\)\nTratamiento muestral de los índices kappa\nSea:\n\nn el número de individuos en la muestra.\n\n\\(p_ij\\) la proporción de individuos asignados a la categoría i por el observador X y a la categoría j por el observador Y, donde i, j =1,2,…,t.\n\n\\(p_{i.}\\) la proporción marginal de que un sujeto sea asignado a la clase i por el observador X (análogo para \\(p_{.j}\\), la clase j y el observador Y), donde i, j =1,2,…,t.\n\n\n\\(p_{i.}\\) = \\(p_{i1}\\) + … + \\(p_{it}\\)\n\n\n\\(p_{.j}\\) = \\(p_{1j}\\) + … + \\(p_{tj}\\)\n\n\n\n\nTenemos:\n\\[E(p_{ij}) = \\pi_{ij}, \\quad E(p_{i.}) = \\pi_{i.}, \\quad E(p_{.j}) = \\pi_{.j}\\]\nEl estimador del índice de kappa ponderado es:\n\\[\n\\kappa_w = \\frac{P_0 - P_c}{1 - P_c}, \\quad P_0 = \\sum_{i=1}^{t}\\sum_{j=1}^{t} w_{ij}p_{ij}, \\quad P_c = \\sum_{i=1}^{t}\\sum_{j=1}^{t} w_{ij}p_{i.}p_{.j}\n\\]\nEs un estimador con una distribución asintótica normal.\n\\[ \\kappa_w ~ N(\\kappa_w, \\sigma_{\\kappa_w})\\]\nTengo una distribucion normal centrada en k y al elegir un dato al azar (o sea, de la muestra aleatoria) puede tener un valor muy cercano o muy lejano de la media.\nLa varianza también es una estimación hecha con lo que yo he visto. Es otra estimación más. Esto hace que vayamos arrastrando aproximaciones AKA posibles errores.\n\\[\n\\displaylines{\n\\sigma^2_{\\kappa_w} = \\frac{\\sum_i \\sum_j \\pi_{ij} \\left[ w_{ij} - (\\overline{w}_{i \\cdot} + \\overline{w}_{\\cdot j})(1 - \\kappa_w) \\right]^2 - \\left[ \\kappa_w - \\Pi_c (1 - \\kappa_w) \\right]^2}{n (1 - \\Pi_c)^2} \\\\\n\\overline{w}_{i \\cdot} = \\sum_{j=1}^t \\pi_{\\cdot j} w_{ij} \\\\\n\\overline{w}_{\\cdot j} = \\sum_{i=1}^t \\pi_{i \\cdot} w_{ij}\n}\n\\]\nEl estimador de esta varianza su contrapartida muestral, sustituyendo probabilidades por proporciones y el índice kappa por su estimador.\n\\[\n\\displaylines{\n\\hat{\\sigma}^2_{\\kappa_w} = s^2_{\\kappa_w} = \\frac{\\sum_i \\sum_j p_{ij} \\left[ w_{ij} - (\\hat{w}_{i \\cdot} + \\hat{w}_{\\cdot j})(1 - \\hat{\\kappa}_w) \\right]^2 - \\left[ \\hat{\\kappa}_w - P_c (1 - \\hat{\\kappa}_w) \\right]^2}{n (1 - P_c)^2} \\\\\n\\hat{w}_{i \\cdot} = \\sum_{j=1}^t p_{\\cdot j} w_{ij} \\\\\n\\hat{w}_{\\cdot j} = \\sum_{i=1}^t p_{i \\cdot} w_{ij}\n}\n\\] Límites de confianza para el índice kappa con nivel de confianza \\(\\alpha\\):\n\\[ \\hat{\\kappa}_w +- z_\\alpha s_{\\hat{\\kappa}_w}\\]\nEscala de valoración del índice Kappa.\n\\[\n\\begin{array}{c|c}\n\\textbf{Kappa} & \\textbf{Grado de concordancia} \\\\\n\\hline\n&lt; 0.00 & \\text{Sin concordancia} \\\\\n0.00 - 0.20  & \\text{Insignificante} \\\\\n0.20 - 0.40  & \\text{Discreta} \\\\\n0.40 - 0.60  & \\text{Moderada} \\\\\n0.60 - 0.80  & \\text{Sustancial} \\\\\n0.80 - 1  & \\text{Casi perfecta} \\\\\n\\end{array}\n\\]\nEjemplo 1.3.\nDos radiólogos independientes informan de la presencia o ausencia de neumonía en 100 radiografías, siendo los resultados los siguientes.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Neumonía} & \\textbf{No neumonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Neumonía} & 4 & 6 & 10 \\\\\n                      & \\textbf{No neumonía} & 10 & 80 & 90 \\\\\n\\hline\n& \\textbf{Total} & 14 & 86 & 100 \\\\\n\\end{array}\n\\]\n\\(P_0 = \\frac{4+80}{100} = 0.84 \\quad \\quad P_c = \\frac{10*14 + 90*86}{100^2} = 0.788\\)\\(\\hat{\\kappa_w} = \\frac{0.84 - 0.788}{1 - 0.788} = 0.245\\)\\(s^2_{\\kappa_w} = 0.018 \\quad \\quad z_{0.05}s_{\\kappa_w} = 1.65 \\sqrt{0.018} = 0.22\\)\nSe podría hacer también un aproximación asintótica pero conlleva arrastrar una simulacion más.\nLa estimación puntual la tengo. Como no tengo una muestra más grande puedo remuestrear.\nLos estimadores por intervalo miden la precision de la estimación puntual. Cómo de buena es esa estimación puntual para representar otra posible muestra que podríamos haber obtenido.\nUna estimación puntual que se encuentra en alguno de los intervalos de Landis y Koch.\nLa validez de la estimacion hay que refrentarla con un IC. La estimación puntual de 0.245 se considera discreta y el \\(IC_90 = (0.025, 0.465)\\) abarca varios niveles, desde discreta hasta sustancial. Si el IC estuviera solo en el mismo intervalo que la estimación puntual diríamos que el IC respalda/valida la estimacion puntual. Al abarcar más de un nivel deberemos explicarlo.\nEn el 90 % de estudios similares a este tenemos una concordancia que iría desde insignificante a moderada. La validaz de nuestra estimación no es super cool, es cuestionable, ya que hay otros estudios donde podemos encontrar una concordancia diferente a la que hemos calculado en esta muestra.\nH0: las opiniones son independientes (que no hay relación entre las opiniones) (si hay una concordancia que sea casual) AKA \\(\\rho\\) = 0.\nPara poder admitir \\(\\rho\\) = 0 el IC tendría que darlo como un valor posible y en este caso no lo da. Rechazamos que haya un criterio diferente. (juzgamos que los dos puntúen por igual)\nEjemplo 1.4.\nDos psiquiatras evaluaron a 129 pacientes que habían sido diagnosticados previamente como clínicamente deprimidos. Las categorías de clasificación fueron: 0 para no deprimido, 1 para moderadamente deprimido y 2 para clínicamente deprimido. La tabla siguiente muestra los resultados de la clasificación realizada por los dos psiquiatras.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Psiquiatra 1} \\\\\n& & \\textbf{0} & \\textbf{1} & \\textbf{2} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Psiquiatra 2} & \\textbf{0} & 11 & 2 & 19 & 32 \\\\\n                      & \\textbf{1} & 1 & 3 & 3 & 7 \\\\\n                      & \\textbf{2} & 0 & 8 & 82 & 90 \\\\\n\\hline\n& \\textbf{Total} & 12 & 13 & 104 & 129 \\\\\n\\end{array}\n\\]\nDatos.\n\nCódigolibrary(psych)\n(A &lt;- matrix(c(11,2,19,1,3,3,0,8,82),ncol=3,byrow=TRUE))\n\n     [,1] [,2] [,3]\n[1,]   11    2   19\n[2,]    1    3    3\n[3,]    0    8   82\n\n\nÍndice kappa en R por defecto.\nEn la diagonal pone peso 0 (yo quiero que en la diagonal tiene 1 y conforme me alejo lo voy bajando).\nÍndice kappa sin ponderar.\nLe damos importancia a todo lo que coincide pero no le damos peso a la cercanía entre dos diagnósticos. La elección por el kappa ponderado es crucial cuando los niveles de clasificación son mayores a dos niveles.\nPesos Ciccetti y Allison.\n\nCódigo(w_CA&lt;- matrix(c( 1,0.5,0, 0.5,1,0.5,0,0.5,1),ncol=3))\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.5  0.0\n[2,]  0.5  1.0  0.5\n[3,]  0.0  0.5  1.0\n\n\nEstimaciones de kappa.\n\nCódigokappa1 &lt;- cohen.kappa(A, w=w_CA, n.obs=sum(A), alpha=0.1)\nstr(kappa1)\n## List of 11\n##  $ kappa         : num 0.375\n##  $ weighted.kappa: num 0.402\n##  $ n.obs         : num 129\n##  $ agree         : num [1:3, 1:3] 0.08527 0.00775 0 0.0155 0.02326 ...\n##  $ weight        : num [1:3, 1:3] 1 0.5 0 0.5 1 0.5 0 0.5 1\n##  $ var.kappa     : num 0.00622\n##  $ var.weighted  : num 0.00688\n##  $ confid        : num [1:2, 1:3] 0.245 0.265 0.375 0.402 0.504 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:2] \"unweighted kappa\" \"weighted kappa\"\n##   .. ..$ : chr [1:3] \"lower\" \"estimate\" \"upper\"\n##  $ plevel        : num 0.1\n##  $ bad           : logi FALSE\n##  $ Call          : language cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels,      w.exp = w.exp)\n##  - attr(*, \"class\")= chr [1:2] \"psych\" \"kappa\"\n\n\n\nkappa1$kappa\n## [1] 0.3745225\nkappa1$weighted.kappa\n## [1] 0.4018192\nkappa1$var.kappa\n## [1] 0.006221038\nkappa1$var.weighted\n## [1] 0.006884677\nkappa1$confid\n##                      lower  estimate     upper\n## unweighted kappa 0.2447870 0.3745225 0.5042579\n## weighted kappa   0.2653391 0.4018192 0.5382992\n\nPesos de Fleiss y Cohen.\n\n(w_FC&lt;- matrix(c( 1,0.75,0,0.75,1,0.75,0,0.75,1),ncol=3))\n\n     [,1] [,2] [,3]\n[1,] 1.00 0.75 0.00\n[2,] 0.75 1.00 0.75\n[3,] 0.00 0.75 1.00\n\n\nEstimaciones de kappa.\n\nCódigokappa2 &lt;- cohen.kappa(A, w=w_FC, n.obs=sum(A),alpha=0.1)\nstr(kappa2)\n## List of 11\n##  $ kappa         : num 0.375\n##  $ weighted.kappa: num 0.42\n##  $ n.obs         : num 129\n##  $ agree         : num [1:3, 1:3] 0.08527 0.00775 0 0.0155 0.02326 ...\n##  $ weight        : num [1:3, 1:3] 1 0.75 0 0.75 1 0.75 0 0.75 1\n##  $ var.kappa     : num 0.00622\n##  $ var.weighted  : num 0.00796\n##  $ confid        : num [1:2, 1:3] 0.245 0.274 0.375 0.42 0.504 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:2] \"unweighted kappa\" \"weighted kappa\"\n##   .. ..$ : chr [1:3] \"lower\" \"estimate\" \"upper\"\n##  $ plevel        : num 0.1\n##  $ bad           : logi FALSE\n##  $ Call          : language cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels,      w.exp = w.exp)\n##  - attr(*, \"class\")= chr [1:2] \"psych\" \"kappa\"\n\n\n\nkappa2$weighted.kappa\n## [1] 0.4203694\nkappa2$var.weighted\n## [1] 0.007955659\nkappa2$confid\n##                      lower  estimate     upper\n## unweighted kappa 0.2447870 0.3745225 0.5042579\n## weighted kappa   0.2736575 0.4203694 0.5670813\n\n\\[\n\\begin{array}{c|c}\n\\textbf{Estadístico} & \\textbf{Estimación} & \\textbf{Varianza estimada} & LI_{90} \\\\\n\\hline\n\\text{Kappa sin ponderar} & 0.3745225 & 0.006221038 & 0.2447870 \\\\\n\\text{Kappa ponderado (Ciccetti-Allison)} & 0.4018192 & 0.006884677 & 0.2653391 \\\\\n\\text{Kappa ponderado (Fleiss-Cohen)} & 04203694 & 0007955659 & 0.2736575 \\\\\n\\end{array}\n\\]\nEstamos teniendo resultados 0.3, 0.4. Las estimaciones puntuales nos incitan a concordancias moderadas. No podemos decir que no tengan un criterio común. algo tienen.\n¿Qué pasha cuando tenemos que analizar la validez de una estimación pero tenemos una muestra pequeña? Po a la metodología booststrap\nNos vamos a Tema 3, página 55.\nDepués de remuestrar tengo las 23 radiografías con la clasificación de los dos radiólogos cuando remuestre.\nObtengo muestras bootstrap con 23 radiografias pero la clasificación es la misma para cada radiografía.\nCalculo el kappa \\(\\Rightarrow\\) calculo las diferencias \\(\\Rightarrow\\) Despercio las más alejadas \\(\\Rightarrow\\) Me quedo con el 95 % central.\nSi hay más de 2 observadores \\(\\Rightarrow\\) Índice de Kappa para múltiples observadores (Fleiss JL Statistical Methods for Rates and Proportions, 2003)\n\n\n\n      \n         1. Introducción ✓\n                \n  \n  \n      \n        3. Análisis de concordancia en variables numéricas ✓"
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#clasificaciones-multinomiales",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#clasificaciones-multinomiales",
    "title": "2. Análisis de concordancia en variables categóricas ✓",
    "section": "Clasificaciones multinomiales",
    "text": "Clasificaciones multinomiales\n\\[\n\\begin{array}{c|c c c c}\n    & & \\textbf{Y} & & \\\\\n    & \\textbf{Categoría} & \\textbf{1} & \\textbf{...} & \\textbf{t} & \\textbf{Total} \\\\\n\\hline\n\\textbf{X} & \\textbf{1} & \\pi_{11} & ... & \\pi_{1t} & \\pi_{1\\cdot} \\\\\n           & \\textbf{2} & \\pi_{21} & ... & \\pi_{2t} & \\pi_{2\\cdot} \\\\\n           & \\textbf{...} & ... & ... & ... & ... \\\\\n           & \\textbf{t} & \\pi_{2t} & ... & \\pi_{tt} & \\pi_{t\\cdot} \\\\\n\\hline\n& \\textbf{Total} & \\pi_{\\cdot1} & ... & \\pi_{\\cdot t} & 1 \\\\\n\\end{array}\n\\]\n\nMedida de concordancia global: \\(\\Pi_0 = \\sum_{i=1}^{t} \\pi_{ii}\\)\n\n\nMedida de concordancia debida al azar: \\(\\Pi_c = \\sum_{i=1}^{t} \\pi_{i.}\\pi_{.i}\\)\n\n\nÍndice kappa: \\(k = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c}\\)\n\nHay veces que nos interesa saber si la concordancia es mayor en unas categorías o en otras. Por ejemplo, en la categoría grave hay muchas concordancia pero en las categorías leve y media no. En este caso se suele estandarizar para tener dos categorías y medir concordancia para cada categoría.\nEjemplo 1.2.\n\ndf &lt;- data.frame(rbind(c(0.1125, 0.1, 0.0375),\n                       c(0.1125, 0.3625, 0.0625),\n                       c(0, 0.0375, 0.175))\n)\ncolnames(df) &lt;- c(\"Leve\", \"Moderada\", \"Grave\")\nrownames(df) &lt;- c(\"Leve\", \"Moderada\", \"Grave\")\ndf\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\n\n\n\ndf_extended &lt;- rbind(df, \"Total\" = colSums(df))\ndf_extended\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\nTotal    0.2250   0.5000 0.2750\n\n\n\ndf_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\ndf_extended\n\n           Leve Moderada  Grave  Total\nLeve     0.1125   0.1000 0.0375 0.2500\nModerada 0.1125   0.3625 0.0625 0.5375\nGrave    0.0000   0.0375 0.1750 0.2125\nTotal    0.2250   0.5000 0.2750 1.0000\n\n\nMedida de concordancia global\n\n(pi_0 &lt;- sum(diag(as.matrix(df))))\n\n[1] 0.65\n\n\nMedida de concordancia debida al azar\n\npi_1 &lt;- 0\nt &lt;- dim(df_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_extended[i,t]*df_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\n[1] 0.3834375\n\n\nÍndice kappa\n\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] 0.4323365\n\n\nEl índice kappa no mide la distancia entre las discordancias. No es lo mismo decir (X=7, Y=8) que (X=7, Y=2).\nComparando cada categoría con el resto\n\ndf\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\n\n\n\nCódigocat &lt;- \"Leve\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Leve\"\n        Leve  Otras Total\nLeve  0.1125 0.1375  0.25\nOtras 0.1125 0.6375  0.75\nTotal 0.2250 0.7750  1.00\n[1] \"Medida de concordancia global:\"\n[1] 0.75\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.6375\n[1] \"Índice kappa:\"\n[1] 0.3103448\n\n\n\nCódigocat &lt;- \"Moderada\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Moderada\"\n         Moderada Otras  Total\nModerada   0.3625 0.175 0.5375\nOtras      0.1375 0.325 0.4625\nTotal      0.5000 0.500 1.0000\n[1] \"Medida de concordancia global:\"\n[1] 0.6875\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.5\n[1] \"Índice kappa:\"\n[1] 0.375\n\n\n\nCódigocat &lt;- \"Grave\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Grave\"\n      Grave  Otras  Total\nGrave 0.175 0.0375 0.2125\nOtras 0.100 0.6875 0.7875\nTotal 0.275 0.7250 1.0000\n[1] \"Medida de concordancia global:\"\n[1] 0.8625\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.629375\n[1] \"Índice kappa:\"\n[1] 0.6290051\n\n\nHasta ahora hemos hablado de probabilidades ya que estamos con el modelo teórico. Cuando trabajamos con datos de una muestra hablaremos de porcentajes. \\(\\Rightarrow\\) “Este % es consecuencia de un acuerdo de xx y un azar de xx”\n\nEl índice kappa tiene una distribución teórica y lo estimo según una muestra. Con una poca información (la muestra) doy un intervalo. No doy intervalos de confianza para valores desconocidos sino para las estimaciones."
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#inconvenientes-del-índice-kappa",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#inconvenientes-del-índice-kappa",
    "title": "2. Análisis de concordancia en variables categóricas ✓",
    "section": "Inconvenientes del índice kappa",
    "text": "Inconvenientes del índice kappa\nEjemplo 1.1.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.04 & 0.06 & 0.10 \\\\\n                      & \\textbf{No pulmonía} & 0.10 & 0.80 & 0.90 \\\\\n\\hline\n& \\textbf{Total} & 0.14 & 0.86 & 1 \\\\\n\\end{array}\n\\]\nLa prevalencia de pulmonía es baja: 014 para el radiólogo A y 0.1 para el rediólogo B.\nAmbas marginales desequilibradas: la prevalencia observada dista mucho de 0.5 (a favor de los diagnósticos negativos)\nEl bajo valor del índice kappa se explica porque nos encontramos en el peor de los escenarios: baja prevalencia y marginales desequilibradas.\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.788\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.245283\n\nEl valor del índice kappa depende de la prevalencia de la característica observada.\nEjemplo 1.1.a.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.30 & 0.06 & 0.36 \\\\\n                      & \\textbf{No pulmonía} & 0.10 & 0.54 & 0.64 \\\\\n\\hline\n& \\textbf{Total} & 0.40 & 0.60 & 1 \\\\\n\\end{array}\n\\]\nAumentamos la prevalencia: 0.40 para el radiólogo A y 0.36 para el rediólogo B.\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.528\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.6610169\n\n\nCuanto más cercana a 0,5 sea la prevalencia (más equilibradas estén las marginales) mayor es el índice kappa, para igual probabilidad de acuerdos observados.\n\n\\[\\Downarrow\\]\n\nPrevalencias muy altas o muy bajas penalizan el índice kappa.\n\nEl valor del índice kappa depende de la simetría y homogeneidad de las marginales.\nEn el primer caso el comportamiento de los agentes es homogéneo: pues ambos emiten informes positivos con mayor frecuencia.\nEn el segundo caso el comportamiento de los agentes es heterogéneo y asimétrico.\nEn ambos casos los diagnósticos del agente A distan 0.2 de 0.5.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.45 & 0.15 & 0.60 \\\\\n                      & \\textbf{No pulmonía} & 0.25 & 0.15 & 0.40 \\\\\n\\hline\n& \\textbf{Total} & 0.70 & 0.30 & 1 \\\\\n&  & 0.50+0.20 & 0.50-0.20 &  \\\\\n\\end{array}\n\\]\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.54\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.6521739\n\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.25 & 0.35 & 0.60 \\\\\n                      & \\textbf{No pulmonía} & 0.05 & 0.35 & 0.40 \\\\\n\\hline\n& \\textbf{Total} & 0.30 & 0.70 & 1 \\\\\n&  & 0.50-0.20 & 0.50+0.20 &  \\\\\n\\end{array}\n\\]\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.46\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.7037037\n\nEl valor del índice kappa depende del equilibrio de las marginales.\nCuanto mayor sea la diferencia de la prevalencia observada de cada agente respecto de 0.5 mayor es el índice kappa. (incluso para un mismo valor de acuerdos observados)."
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#índice-kappa-ponderado.",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#índice-kappa-ponderado.",
    "title": "2. Análisis de concordancia en variables categóricas ✓",
    "section": "Índice kappa ponderado.",
    "text": "Índice kappa ponderado.\nSólo tiene sentido para variables ordinales.\nEstá diseñado para recoger la idea de que algunas discordancias son más severas que otras y, por tanto, asigna pesos que representan la importancia entre los desacuerdos. El máximo peso se da a la concordancia perfecta y pesos proporcionalmente menores según la importancia del desacuerdo.\nNo tiene la misma importancia un desacuerdo en la clasificación entre las categorías leve y moderada que entre leve y grave, obviamente la última representa un mayor desacuerdo que la primera.\n\nMedida de concordancia global: \\(\\Pi_0 = \\sum_{i=1}^{t} \\sum_{j=1}^{t} w_{ij} \\pi_{ij}\\)\n\n\nMedida de concordancia debida al azar: \\(\\Pi_c = \\sum_{i=1}^{t} \\sum_{j=1}^{t} w_{ij} \\pi_{i.}\\pi_{.j}\\)\n\n\nÍndice kappa: \\(k_{w} = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c}\\)\n\nLos pesos satisfacen:\n\\[\nw_{ij} = \\begin{cases}\nw_{ii} = 1 \\\\\n0 \\le w_{ij} \\le 1 \\\\\nw_{ij} = w_{ji}\n\\end{cases}\n\\]\n\\[\nw_{ij} = \\begin{cases}\n1, \\hspace{1em} i = j \\\\\n0, \\hspace{1em} i \\neq j \\\\\n\\end{cases} \\hspace{1em} \\text{sii} \\hspace{1em} k_{w} = k \\\\\n\\] \\[\n\\text{si t = 2} \\Rightarrow k_{w} = k\n\\]\nPonderaciones.\n\nCicchetti-Allison (1971) \\(w_{ij} = 1 - \\frac{|i-j|}{t-1}\\)\nFleiss-Cohen (1973) \\(w_{ij} = 1 - \\frac{(i-j)^2}{(t-1)^2}\\)"
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#tratamiento-muestral-de-los-índices-kappa",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#tratamiento-muestral-de-los-índices-kappa",
    "title": "2. Análisis de concordancia en variables categóricas ✓",
    "section": "Tratamiento muestral de los índices kappa",
    "text": "Tratamiento muestral de los índices kappa\nSea:\n\nn el número de individuos en la muestra.\n\n\\(p_ij\\) la proporción de individuos asignados a la categoría i por el observador X y a la categoría j por el observador Y, donde i, j =1,2,…,t.\n\n\\(p_{i.}\\) la proporción marginal de que un sujeto sea asignado a la clase i por el observador X (análogo para \\(p_{.j}\\), la clase j y el observador Y), donde i, j =1,2,…,t.\n\n\n\\(p_{i.}\\) = \\(p_{i1}\\) + … + \\(p_{it}\\)\n\n\n\\(p_{.j}\\) = \\(p_{1j}\\) + … + \\(p_{tj}\\)\n\n\n\n\nTenemos:\n\\[E(p_{ij}) = \\pi_{ij}, \\quad E(p_{i.}) = \\pi_{i.}, \\quad E(p_{.j}) = \\pi_{.j}\\]\nEl estimador del índice de kappa ponderado es:\n\\[\n\\kappa_w = \\frac{P_0 - P_c}{1 - P_c}, \\quad P_0 = \\sum_{i=1}^{t}\\sum_{j=1}^{t} w_{ij}p_{ij}, \\quad P_c = \\sum_{i=1}^{t}\\sum_{j=1}^{t} w_{ij}p_{i.}p_{.j}\n\\]\nEs un estimador con una distribución asintótica normal.\n\\[ \\kappa_w ~ N(\\kappa_w, \\sigma_{\\kappa_w})\\]\nTengo una distribucion normal centrada en k y al elegir un dato al azar (o sea, de la muestra aleatoria) puede tener un valor muy cercano o muy lejano de la media.\nLa varianza también es una estimación hecha con lo que yo he visto. Es otra estimación más. Esto hace que vayamos arrastrando aproximaciones AKA posibles errores.\n\\[\n\\displaylines{\n\\sigma^2_{\\kappa_w} = \\frac{\\sum_i \\sum_j \\pi_{ij} \\left[ w_{ij} - (\\overline{w}_{i \\cdot} + \\overline{w}_{\\cdot j})(1 - \\kappa_w) \\right]^2 - \\left[ \\kappa_w - \\Pi_c (1 - \\kappa_w) \\right]^2}{n (1 - \\Pi_c)^2} \\\\\n\\overline{w}_{i \\cdot} = \\sum_{j=1}^t \\pi_{\\cdot j} w_{ij} \\\\\n\\overline{w}_{\\cdot j} = \\sum_{i=1}^t \\pi_{i \\cdot} w_{ij}\n}\n\\]\nEl estimador de esta varianza su contrapartida muestral, sustituyendo probabilidades por proporciones y el índice kappa por su estimador.\n\\[\n\\displaylines{\n\\hat{\\sigma}^2_{\\kappa_w} = s^2_{\\kappa_w} = \\frac{\\sum_i \\sum_j p_{ij} \\left[ w_{ij} - (\\hat{w}_{i \\cdot} + \\hat{w}_{\\cdot j})(1 - \\hat{\\kappa}_w) \\right]^2 - \\left[ \\hat{\\kappa}_w - P_c (1 - \\hat{\\kappa}_w) \\right]^2}{n (1 - P_c)^2} \\\\\n\\hat{w}_{i \\cdot} = \\sum_{j=1}^t p_{\\cdot j} w_{ij} \\\\\n\\hat{w}_{\\cdot j} = \\sum_{i=1}^t p_{i \\cdot} w_{ij}\n}\n\\] Límites de confianza para el índice kappa con nivel de confianza \\(\\alpha\\):\n\\[ \\hat{\\kappa}_w +- z_\\alpha s_{\\hat{\\kappa}_w}\\]\nEscala de valoración del índice Kappa.\n\\[\n\\begin{array}{c|c}\n\\textbf{Kappa} & \\textbf{Grado de concordancia} \\\\\n\\hline\n&lt; 0.00 & \\text{Sin concordancia} \\\\\n0.00 - 0.20  & \\text{Insignificante} \\\\\n0.20 - 0.40  & \\text{Discreta} \\\\\n0.40 - 0.60  & \\text{Moderada} \\\\\n0.60 - 0.80  & \\text{Sustancial} \\\\\n0.80 - 1  & \\text{Casi perfecta} \\\\\n\\end{array}\n\\]\nEjemplo 1.3.\nDos radiólogos independientes informan de la presencia o ausencia de neumonía en 100 radiografías, siendo los resultados los siguientes.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Neumonía} & \\textbf{No neumonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Neumonía} & 4 & 6 & 10 \\\\\n                      & \\textbf{No neumonía} & 10 & 80 & 90 \\\\\n\\hline\n& \\textbf{Total} & 14 & 86 & 100 \\\\\n\\end{array}\n\\]\n\\(P_0 = \\frac{4+80}{100} = 0.84 \\quad \\quad P_c = \\frac{10*14 + 90*86}{100^2} = 0.788\\)\\(\\hat{\\kappa_w} = \\frac{0.84 - 0.788}{1 - 0.788} = 0.245\\)\\(s^2_{\\kappa_w} = 0.018 \\quad \\quad z_{0.05}s_{\\kappa_w} = 1.65 \\sqrt{0.018} = 0.22\\)\nSe podría hacer también un aproximación asintótica pero conlleva arrastrar una simulacion más.\nLa estimación puntual la tengo. Como no tengo una muestra más grande puedo remuestrear.\nLos estimadores por intervalo miden la precision de la estimación puntual. Cómo de buena es esa estimación puntual para representar otra posible muestra que podríamos haber obtenido.\nUna estimación puntual que se encuentra en alguno de los intervalos de Landis y Koch.\nLa validez de la estimacion hay que refrentarla con un IC. La estimación puntual de 0.245 se considera discreta y el \\(IC_90 = (0.025, 0.465)\\) abarca varios niveles, desde discreta hasta sustancial. Si el IC estuviera solo en el mismo intervalo que la estimación puntual diríamos que el IC respalda/valida la estimacion puntual. Al abarcar más de un nivel deberemos explicarlo.\nEn el 90 % de estudios similares a este tenemos una concordancia que iría desde insignificante a moderada. La validaz de nuestra estimación no es super cool, es cuestionable, ya que hay otros estudios donde podemos encontrar una concordancia diferente a la que hemos calculado en esta muestra.\nH0: las opiniones son independientes (que no hay relación entre las opiniones) (si hay una concordancia que sea casual) AKA \\(\\rho\\) = 0.\nPara poder admitir \\(\\rho\\) = 0 el IC tendría que darlo como un valor posible y en este caso no lo da. Rechazamos que haya un criterio diferente. (juzgamos que los dos puntúen por igual)\nEjemplo 1.4.\nDos psiquiatras evaluaron a 129 pacientes que habían sido diagnosticados previamente como clínicamente deprimidos. Las categorías de clasificación fueron: 0 para no deprimido, 1 para moderadamente deprimido y 2 para clínicamente deprimido. La tabla siguiente muestra los resultados de la clasificación realizada por los dos psiquiatras.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Psiquiatra 1} \\\\\n& & \\textbf{0} & \\textbf{1} & \\textbf{2} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Psiquiatra 2} & \\textbf{0} & 11 & 2 & 19 & 32 \\\\\n                      & \\textbf{1} & 1 & 3 & 3 & 7 \\\\\n                      & \\textbf{2} & 0 & 8 & 82 & 90 \\\\\n\\hline\n& \\textbf{Total} & 12 & 13 & 104 & 129 \\\\\n\\end{array}\n\\]\nDatos.\n\nCódigolibrary(psych)\n(A &lt;- matrix(c(11,2,19,1,3,3,0,8,82),ncol=3,byrow=TRUE))\n\n     [,1] [,2] [,3]\n[1,]   11    2   19\n[2,]    1    3    3\n[3,]    0    8   82\n\n\nÍndice kappa en R por defecto.\nEn la diagonal pone peso 0 (yo quiero que en la diagonal tiene 1 y conforme me alejo lo voy bajando).\nÍndice kappa sin ponderar.\nLe damos importancia a todo lo que coincide pero no le damos peso a la cercanía entre dos diagnósticos. La elección por el kappa ponderado es crucial cuando los niveles de clasificación son mayores a dos niveles.\nPesos Ciccetti y Allison.\n\nCódigo(w_CA&lt;- matrix(c( 1,0.5,0, 0.5,1,0.5,0,0.5,1),ncol=3))\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.5  0.0\n[2,]  0.5  1.0  0.5\n[3,]  0.0  0.5  1.0\n\n\nEstimaciones de kappa.\n\nCódigokappa1 &lt;- cohen.kappa(A, w=w_CA, n.obs=sum(A), alpha=0.1)\nstr(kappa1)\n## List of 11\n##  $ kappa         : num 0.375\n##  $ weighted.kappa: num 0.402\n##  $ n.obs         : num 129\n##  $ agree         : num [1:3, 1:3] 0.08527 0.00775 0 0.0155 0.02326 ...\n##  $ weight        : num [1:3, 1:3] 1 0.5 0 0.5 1 0.5 0 0.5 1\n##  $ var.kappa     : num 0.00622\n##  $ var.weighted  : num 0.00688\n##  $ confid        : num [1:2, 1:3] 0.245 0.265 0.375 0.402 0.504 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:2] \"unweighted kappa\" \"weighted kappa\"\n##   .. ..$ : chr [1:3] \"lower\" \"estimate\" \"upper\"\n##  $ plevel        : num 0.1\n##  $ bad           : logi FALSE\n##  $ Call          : language cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels,      w.exp = w.exp)\n##  - attr(*, \"class\")= chr [1:2] \"psych\" \"kappa\"\n\n\n\nkappa1$kappa\n## [1] 0.3745225\nkappa1$weighted.kappa\n## [1] 0.4018192\nkappa1$var.kappa\n## [1] 0.006221038\nkappa1$var.weighted\n## [1] 0.006884677\nkappa1$confid\n##                      lower  estimate     upper\n## unweighted kappa 0.2447870 0.3745225 0.5042579\n## weighted kappa   0.2653391 0.4018192 0.5382992\n\nPesos de Fleiss y Cohen.\n\n(w_FC&lt;- matrix(c( 1,0.75,0,0.75,1,0.75,0,0.75,1),ncol=3))\n\n     [,1] [,2] [,3]\n[1,] 1.00 0.75 0.00\n[2,] 0.75 1.00 0.75\n[3,] 0.00 0.75 1.00\n\n\nEstimaciones de kappa.\n\nCódigokappa2 &lt;- cohen.kappa(A, w=w_FC, n.obs=sum(A),alpha=0.1)\nstr(kappa2)\n## List of 11\n##  $ kappa         : num 0.375\n##  $ weighted.kappa: num 0.42\n##  $ n.obs         : num 129\n##  $ agree         : num [1:3, 1:3] 0.08527 0.00775 0 0.0155 0.02326 ...\n##  $ weight        : num [1:3, 1:3] 1 0.75 0 0.75 1 0.75 0 0.75 1\n##  $ var.kappa     : num 0.00622\n##  $ var.weighted  : num 0.00796\n##  $ confid        : num [1:2, 1:3] 0.245 0.274 0.375 0.42 0.504 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:2] \"unweighted kappa\" \"weighted kappa\"\n##   .. ..$ : chr [1:3] \"lower\" \"estimate\" \"upper\"\n##  $ plevel        : num 0.1\n##  $ bad           : logi FALSE\n##  $ Call          : language cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels,      w.exp = w.exp)\n##  - attr(*, \"class\")= chr [1:2] \"psych\" \"kappa\"\n\n\n\nkappa2$weighted.kappa\n## [1] 0.4203694\nkappa2$var.weighted\n## [1] 0.007955659\nkappa2$confid\n##                      lower  estimate     upper\n## unweighted kappa 0.2447870 0.3745225 0.5042579\n## weighted kappa   0.2736575 0.4203694 0.5670813\n\n\\[\n\\begin{array}{c|c}\n\\textbf{Estadístico} & \\textbf{Estimación} & \\textbf{Varianza estimada} & LI_{90} \\\\\n\\hline\n\\text{Kappa sin ponderar} & 0.3745225 & 0.006221038 & 0.2447870 \\\\\n\\text{Kappa ponderado (Ciccetti-Allison)} & 0.4018192 & 0.006884677 & 0.2653391 \\\\\n\\text{Kappa ponderado (Fleiss-Cohen)} & 04203694 & 0007955659 & 0.2736575 \\\\\n\\end{array}\n\\]\nEstamos teniendo resultados 0.3, 0.4. Las estimaciones puntuales nos incitan a concordancias moderadas. No podemos decir que no tengan un criterio común. algo tienen.\n¿Qué pasha cuando tenemos que analizar la validez de una estimación pero tenemos una muestra pequeña? Po a la metodología booststrap\nNos vamos a Tema 3, página 55.\nDepués de remuestrar tengo las 23 radiografías con la clasificación de los dos radiólogos cuando remuestre.\nObtengo muestras bootstrap con 23 radiografias pero la clasificación es la misma para cada radiografía.\nCalculo el kappa \\(\\Rightarrow\\) calculo las diferencias \\(\\Rightarrow\\) Despercio las más alejadas \\(\\Rightarrow\\) Me quedo con el 95 % central.\nSi hay más de 2 observadores \\(\\Rightarrow\\) Índice de Kappa para múltiples observadores (Fleiss JL Statistical Methods for Rates and Proportions, 2003)"
  },
  {
    "objectID": "tema_01/tema_01_3_concordancia_numericas.html#concordancia-entre-las-puntuaciones-dadas-a-un-conjunto-de-individuos",
    "href": "tema_01/tema_01_3_concordancia_numericas.html#concordancia-entre-las-puntuaciones-dadas-a-un-conjunto-de-individuos",
    "title": "3. Análisis de concordancia en variables numéricas ✓",
    "section": "Concordancia entre las puntuaciones dadas a un conjunto de individuos",
    "text": "Concordancia entre las puntuaciones dadas a un conjunto de individuos\nPara establecer comparaciones necesito medidas que no tengan unidades.\nMedidas absolutas\nLas medidas absolutas tienen las mismas unidades que la variable respuesta.\n\n\nDesviación cuadrática media.\n\n\\(\\symbf{RMSE = \\sqrt{\\frac{1}{n}\\sum_{i}{x_i}^2}}\\)\n\n\n\nÍndice de desviación total.\n\nSe utiliza principalmente cuando se quiere evaluar la precisión y exactitud de un método con respecto a otro.\nResponde a cuál es el valor máximo de la desviación absoluta que se puede esperar entre dos mediciones con un cierto nivel de confianza.\n\n\n\\(\\symbf{TDI = P(|X_1 - X_2| \\leq \\delta) = 1 - \\alpha}\\)\n\n\n\\(X_1, X_2\\) son las mediciones a comparar.\n\n\n\\(\\delta\\) es el valor máximo de desviación absoluta.\n\n\n\\(\\alpha\\) es el nivel de significancia o el complemento del nivel de confianza.\n\n\nSi el TDI es bajo, indica que las dos mediciones (o el método y el valor de referencia) están en buen acuerdo y las desviaciones tienden a ser pequeñas.\n\nSi el TDI es alto, significa que hay una mayor variabilidad o discrepancia entre las dos mediciones.\n\n\n\nProbabilidad de cobertura.\n\nÍndice asociado a un intervalo.\nProbabilidad de que la diferencia entre dos mediciones, \\(|X_1 - X_2\\), esté contenida dentro de un intervalo de longitud \\(\\delta\\) (el TDI).\n\\(\\symbf{P(|X_1 - X_2| \\leq \\delta)}\\)\nProbabilidad de que el intervalo de error entre dos mediciones (o entre una medición y un valor de referencia) se mantenga dentro de un rango específico, con un cierto nivel de confianza.\nLa probabilidad de que la diferencia entre dos mediciones caiga dentro de un intervalo predefinido.\nQuiero estudiar con qué probabilidad tomo valores en ese intervalo. no está tan claro que tenga unidades de medida (porque es una prob).\n\nSi le doy la vuelta, qué intervalo tengo para que el x % de las probabilidades estén en él, estoy pidiendo un superior e inferior que sí tienen unidades de medida. Referida a una prbailidad pero en realdiad dada una prob lo que bsco es un intervalo en la variable respuesta que ocurra con esa probabilidad.\n\n\n\nSon buenos para medir las características de esa respuesta. Pero si quiero medir acuerdos comparando dos respuestas distintas no me valen.\nMedidas relativas\nIdea: semejanza de valores.\n\nCoeficiente de Correlación Intraclase. Cuantifica el acuerdo entre agentes.\nCoeficiente de correlación concordante. Analiza la semejanza. (semejanza \\(\\neq\\) acuerdo)\nCoeficiente de correlación de Pearson. Analiza si existe una función que permite llegar de unos valores a otros.\n\nSi no hay relacion:\n- El ICC dice que no hay acuerdo.\n- El coeficiente de correlación concordante dice que no hay semejanza.\n- El coeficiente de correlación de Pearson dice que no hay forma de a partir de unos valores llegar a los otros.\nEjemplos\n\nEjemplo a.\n\n\n\\(Y_a\\) = medida por el método A.\\(Y_b\\) = medida por el método B.\n\\[\n\\displaylines{\n\\begin{array}{c|c c c c}\n\\textbf{Sujeto} & \\textbf{1} & \\textbf{2} & \\textbf{3} & \\textbf{4}  & \\textbf{5} \\\\ \\hline\n\\symbf{Y_a} & 1 & 2 & 3 & 4 & 5 \\\\ \\hline\n\\symbf{Y_b} & 1 & 2 & 3 & 4 & 5 \\\\ \\hline\n\\end{array}\n}\n\\]\n\n\nICC = 1.\n\nCCC = 1.\nComo \\(Y_a = Y_b\\) =&gt; r = 1.\n\n\n\n\n\n\n\n\n\n\n\nEjemplo b.\n\n\n\\(Y_a\\) = medida por el método A.\\(Y_b\\) = medida por el método B.\n\\[\n\\displaylines{\n\\begin{array}{c|c c c c}\n\\textbf{Sujeto} & \\textbf{1} & \\textbf{2} & \\textbf{3} & \\textbf{4}  & \\textbf{5} \\\\ \\hline\n\\symbf{Y_a} & 5 & 6 & 7 & 8 & 9 \\\\ \\hline\n\\symbf{Y_b} & 0 & 1 & 2 & 3 & 4 \\\\ \\hline\n\\end{array}\n}\n\\]\n\nComo \\(Y_b = Y_a - 5\\) =&gt; r = 1. (no sé si hay acuerdo pero sé que puedo hacer una asociación entre uno y otro)\nLas puntuaciones son diferentes pero el orden de los sujetos es el mismo. Además la diferrencia entre puntuaciones es siempre de una unidad. CCC = 1.\naquí frase no he escuchado. ICC = 0.94. No hay independencia, hay un criterio común para los dos. Buscamos si las valoraciones dadas responden a un criterio común (que uno sea más estricto que otro no nos interesa ahora)\n\n\n\n\n\n\n\n\n\n\n\nEjemplo c.\n\n\n\\(Y_a\\) = medida por el método A.\\(Y_b\\) = medida por el método B.\n\\[\n\\displaylines{\n\\begin{array}{c|c c c c}\n\\textbf{Sujeto} & \\textbf{1} & \\textbf{2} & \\textbf{3} & \\textbf{4}  & \\textbf{5} \\\\ \\hline\n\\symbf{Y_a} & 1 & 2 & 3 & 4 & 5 \\\\ \\hline\n\\symbf{Y_b} & 1/2 & 1 & 3/2 & 2 & 5/2 \\\\ \\hline\n\\end{array}\n}\n\\]\n\nAnalizo la relación entre las dos respuestas y veo que con conocer la primera obtengo la segunda. Al exister esa relacion y ser perfecta sé que r=1. \\(Y_b = 1/2 * Y_a\\)\nICC = 0.5.\nCCC &lt; 1. Aunque la puntuación sea diferente, el orden sigue siendo el mismo en ambas valoracioes. Pero la escala de calibracion no es la misma: la diferencia en los primeros es de un punto y de los segundos es de medio punto."
  },
  {
    "objectID": "tema_01/tema_01_3_concordancia_numericas.html#coeficientes-de-correlación-intraclase-icc",
    "href": "tema_01/tema_01_3_concordancia_numericas.html#coeficientes-de-correlación-intraclase-icc",
    "title": "3. Análisis de concordancia en variables numéricas ✓",
    "section": "Coeficientes de correlación intraclase (ICC)",
    "text": "Coeficientes de correlación intraclase (ICC)\nEl ICC cuantifica el ajuste (la concordancia) entre varias valoraciones de agentes de una variable numérica. Un conjunto de agentes/jueces dan valoraciones a una serie de individuos.\nProblema\nHay muchos ICC. Y R da todos. \\(\\Rightarrow\\) Tendremos muchas salidas y hay que interpretar la situación de nuestros datos para saber qué ICC es el acorde a nuestro modelo teórico.\nEn la situación donde la evaluación era la asignación de categorías no nos afectaba saber cómo se han recodigo los datos.\nEl ICC es la proporción de variabilidad debida a la variabilidad de los “sujetos”.\nhttps://github.com/cran/psych/blob/master/R/ICC.R\nTendremos un ICC para cada modelo ANOVA.\n\nMedir la concordancia:\n\nMedir el modelo que va por detrás.\nEn el diseño de experientos medir la variabilidad total.\nMedir la variabilidad del modelo / explicada por el modelo.\n\n\nAl coeficiente desde el punto de vista teórico lo llamamos rho, \\(\\rho\\).\n\nEste coeficiente es un valor desconocido y lo aproximamos muestralmente.\n\nCuando hablemos del estimador hablaremos del ICC, \\(\\hat{\\rho}\\).\n\nPosibles situaciones. (en los modelos unifactorial y bifactorial)\nTenemos una muestra de n sujetos valorados por k agentes.\n\n\nCada sujeto es valorado por un conjunto diferente de k jueces seleccionados aleatoriamente. (al sujeto 1 le evalúan k jueces, al sujeto 2 otros k jueces distintos, etc.)\n\n“cada vez que hago un juicio (cada vez que hago una valoración sobre un sujeto) escojo un juez al azar”\n\n\n\nSe selecciona una muestra aleatoria de k jueces y cada uno valora a todos los sujetos.\n\n“tengo muchos métodos y como no puedo estudiar todos selecciono una muestra de jueces al azar” (introducir incertidumbre de haber podido elegir otros cuatro)\n\n\n\nLos k jueces son fijos (la población, no son una muesta) y cada uno valora a todos los sujetos.\n\n“tengo solo cuatro jueces y cojo los cuatro para medir la concordancia entre todos ellos.”\n\n\n\nModelo teórico.\n\nSea \\(y_{ij}\\) = la puntuación del juez i al sujeto j. (al revés de como lo habría hecho yo y cualquier persona normal)\nj, sujeto. i = 1…n\ni, juez. j = 1…k\n\n\\[\ny_{ij} \\text{ depende de} = \\begin{cases}\n\\text{parte común} \\\\\n\\text{influencia del sujeto i} \\\\\n\\text{influencia del juez j} \\\\\n\\text{interacción sujeto/juez} \\\\\n\\text{azar (lo que no hemos incluido en el modelo)} \\\\\n\\end{cases}\n\\]\nPara analizar cómo de concordantes son esas valoraciones tengo que identificar el modelo de la recogida de datos. No es lo mismo estudiar la concordancia de los jueces si tengo la opinión de todos, si he tenido que elegir cuatro de ellos, si me imponen cuáles elegir, si tengo que elegir cada vez uno distinto, etc.\n\\(\\rho = \\frac{Cov(y_{ij},y_{i'j})}{Var(y_{ij})} = \\frac{\\sigma_{\\beta}^2}{\\sigma_{\\beta}^2 + \\sigma_{e}^2} = \\text{variabilidad de los sujetos respecto a la variabilidad total del modelo}\\)\n\n\n\\(Cov(y_{ij},y_{i'j})\\): puntuación de un individuo dada por diferentes jueces.\n\n\n\\(Var(y_{ij})\\): toda la variabilidad de lo observado.\n\nVarianza.\nVarianza de los datos = varianza del modelo + varianza de todo lo demás.\nCaso 1. ANOVA unifactorial\nCada sujeto ha sido evaluado por un número k de jueces distinto y desconocido para cada sujeto. No sé si la primera puntuación de un individuo y la primera puntuación de otro individuo la ha dado el mismo juez. Si no tengo la influencia del juez medida tampoco puedo medir la interacción.\nFuentes de variabilidad no controladas:\n\nvariabilidad debida a los jueces\nvariabilidad debida a la interacción entre juez y sujeto\nvariabilidad debida al error\n\nModelo teórico.\n\\[\ny_{ij} = \\mu + \\beta_j + e_{ij}, \\quad i = 1, \\dots, k, \\quad j = 1, \\dots, n\n\\]\n\\[\n\\left.\n\\begin{aligned}\n  \\{\\beta_j\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{\\beta}^2) \\\\\n  \\{e_{ij}\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{e}^2)\n\\end{aligned}\n\\right\\}\n\\quad \\text{independientes.}\n\\]\n\\[\n\\displaylines{\n& \\textbf {ANOVA y esperanzas de los cuadrados medios} \\\\\n&\\begin{array}\n{|c|c|c|c|}\n\\hline  \\textbf { Fuente de variación } & \\textbf { SC } & \\textbf { g.l. } & \\textbf { CM }  & \\textbf { E(CM) } \\\\\n\\hline\n\\text {Intersujetos} &\\text {SCB = } k\\sum_{j=1}^{n}(\\overline{y}_{·j} - \\overline{y}_{··})^2 & \\text {n-1} & \\text {CMB = } \\frac{SCB}{n-1} & k\\sigma_{\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\text {Intrasujetos} & \\text {SCW = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{·j})^2 & \\text {n(k-1) = N-n} & \\text {CMW = } \\frac{SCW}{n(k-1)} & \\sigma_{e}^2\\\\\n\\hline\n\\text {Total} & \\text {SCT = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{··})^2 & \\text {N-1} & \\text {CMT = } \\frac{SCW}{N-1} & \\sigma^2\\\\\n\\hline\n\\end{array}\n}\n\\]\n\n\nIntersujetos. Entre los sujetos. Variabilidad entre los sujetos que participan. Captura las diferencias naturales entre individuos.\n\nSi hay n sujetos, los g.l. de la variabilidad entre ellos es n-1.\n\n\n\nIntrasujetos. Dentro de los sujetos. Variabilidad que existe dentro de un mismo sujeto debido a los diferentes jueces.\n\nComo es dentro de los sujetos y cada sujeto tiene k evaluaciones, g.l. = k-1.\nComo tenemos n sujetos, g.l. totales son n(k-1).\n\n\n\nICC.\n\\[\n\\displaylines{\\rho = \\frac{Cov(y_{ij},y_{i'j})}{Var(y_{ij})} = \\frac{\\sigma_{\\beta}^2}{\\sigma_{\\beta}^2 + \\sigma_{e}^2} \\Rightarrow \\\\\n\\hat\\rho = ICC(1,1) = \\frac{CMB - CMW}{CMB + (k-1)CMW}}\\]\nContraste de hipótesis.\n\\(H0: \\rho =0. \\quad \\quad\\) Estadístico:\n\\[F_0 = \\frac{CMB}{CMW}, \\quad p-value = p(F_{n-1,N-n} &gt; F_{0})\\]\nIntervalo de confianza.\n\\[IC_{1-\\alpha}(\\hat\\rho) = (\\frac{F_{L}-1}{F_{L}+(k-1)}, \\frac{F_{U}-1}{F_{U}+(k-1)})\\] tal que\n\\[F_{L} = \\frac{F_{0}}{F_{n-1, n(k.1), \\alpha/2}} \\quad \\quad F_{U} = F_{0} * F_{n(k.1), n-1, \\alpha/2}\\]\nEjemplo\nEjemplo 1.5. (Shrout y Fleiss (1979))\nLa siguiente tabla muestra cuatro valoraciones para cada uno de 6 sujetos.\n\\[\n\\begin{array}{c|cccc}\n& \\textit{Juez} \\\\\n\\hline\n\\textit{Sujeto} & {1} & {2} & {3} & {4} \\\\\n\\hline\n1 & {9} & {2} & {5} & {8} \\\\\n2 & {6} & {1} & {3} & {2} \\\\\n3 & {8} & {4} & {6} & {8} \\\\\n4 & {7} & {1} & {2} & {6} \\\\\n5 & {10} & {5} & {6} & {9} \\\\\n6 & {6} & {2} & {4} & {7} \\\\\n\\end{array}\n\\]\n\nCódigoA&lt;- matrix(c(9,2,5,8,\n 6,1,3,2,\n 8,4,6,8,\n 7,1,2,6,\n 10,5,6,9,\n 6,2,4,7),ncol=4,byrow=TRUE)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    9    2    5    8\n[2,]    6    1    3    2\n[3,]    8    4    6    8\n[4,]    7    1    2    6\n[5,]   10    5    6    9\n[6,]    6    2    4    7\n\n\nLas valoraciones para cada sujeto corresponden a jueces diferentes para cada sujeto.\n\\[\n\\displaylines{\n&\\begin{array}\n{|c|c|c|c|}\n\\hline  \\textbf { Fuente de variación }& \\textbf { g.l. } & \\textbf { CM } \\\\\n\\hline\n\\text {Intersujetos / Entresujetos} & 5 & 11.24 \\\\\n\\hline\n\\text {Intrasujetos / Dentro de sujetos} & 18 & 6.26 \\\\\n\\hline\n\\end{array}\n}\n\\]\n\nCódigon &lt;- 6\nk &lt;- 4\n\ndatos &lt;- data.frame(\n  Fuente_de_variación = c(\"Entre sujetos\", \"Dentro de sujetos\"),\n  g_l = c(n-1, n*k-n),\n  Cuadrado_medio = c(11.24, 6.26)\n)\n\ndatos\n\n  Fuente_de_variación g_l Cuadrado_medio\n1       Entre sujetos   5          11.24\n2   Dentro de sujetos  18           6.26\n\n\n\\[\n\\displaylines{\n\\text{ICC} = \\frac{CMB - CMW}{CMB + (k-1)CMW} \\\\\n\\text{ICC} = \\frac{11.24 - 6.26}{11.24 + 3*6.26} = 0.290\n}\n\\]\n\n(ICC &lt;- (11.24 - 6.26) / (11.24 + 3*6.26))\n\n[1] 0.1658894\n\n\n\nCódigolibrary(psych)\nICC &lt;- ICC(A, lmer=FALSE)\nICC$results[\"Single_raters_absolute\",]\n\n                       type       ICC        F df1 df2         p lower bound\nSingle_raters_absolute ICC1 0.1657418 1.794678   5  18 0.1647688  -0.1329323\n                       upper bound\nSingle_raters_absolute   0.7225601\n\n\nEl experimento no se ha diseñado para valorar sobre quienes dan evaluaciones (no se ha diseñado para controlar el efecto de los jueces).\nEstoy mirando la independencia entre las valoraciones de un mismo individuo (para el \\(ind_1\\) las ka valoraciones, para el \\(ind_2\\) las ka valoraciones, etc.)\nICC(1,1) = 0,1657 \\(\\quad \\quad\\) \\(IC_{95}\\) = (-0.13, 0.72)\nMi objetivo es estudiar si \\(\\rho = 0\\).\n\nNo puedo rechazar la H0: independencia entre las valoraciones AKA las puntuaciones dadas al individuo son independientes.\nMedimos si un sujeto tiene medidas concordantes sin poder valorar a los jueces.\nCaso 2. ANOVA bifactorial de efectos aleatorios\n(el caso más complicado precisamente por la aleatoriedad que queremos extrapolar a todos los jueces)\nCada sujeto ha sido evaluado por un número k de jueces distinto y desconocido para cada sujeto. No sé si la primera puntuación de un individuo y la primera puntuación de otro individuo la ha dado el mismo juez. Si no tengo la influencia del juez medida tampoco puedo medir la interacción — CAMBIAR\nBusco unos sujetos al azar y busco unos jueces al azar \\(\\Rightarrow\\) Dos factores.\nLa puntuación puntual viene dada por:\n\\(y_{ij} =\\) un algo común para todos \\(\\quad + \\quad\\) lo que influya factor sujeto \\(\\quad + \\quad\\) lo que influya factor juez \\(\\quad + \\quad\\) la interaccion \\(\\quad + \\quad\\) lo que dependa de todo lo que no he incluido.\nEn este escenario hay un grado de concordancia más alta debido a que el primer juez es elegido al azar pero es el mismo para todos, el segundo juez es elegido al azar pero es el mismo para todos, etc.\nDe un grupo de jueces he elegido unos pocos al azar. como podría ser cualquieras los que elijo, me sirve para representar a todos: “independencia entre las valoraciones de los jueces a un mismo individuo”.\nFuentes de variabilidad no controladas:\n\nvariabilidad debida al error\n\nModelo teórico.\n\\[\ny_{ij} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + e_{ij}, \\quad i = 1, \\dots, k, \\quad j = 1, \\dots, n\n\\]\n\\[\n\\left.\n\\begin{aligned}\n  \\{\\alpha_i\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{\\alpha}^2) \\\\\n  \\{\\beta_j\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{\\beta}^2) \\\\\n  \\{(\\alpha\\beta_{ij}\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{\\alpha\\beta}^2) \\\\\n  \\{e_{ij}\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{e}^2)\n\\end{aligned}\n\\right\\}\n\\quad \\text{independientes.}\n\\]\n\\[\n\\displaylines{\n& \\textbf {ANOVA y esperanzas de los cuadrados medios} \\\\\n&\\begin{array}\n{|c|c|c|c|}\n\\hline  \\textbf { F. variación } & \\textbf { SC } & \\textbf { g.l. } & \\textbf { CM }  & \\textbf { E(CM) } \\\\\n\\hline\n\\text {Intersujetos} &\\text {SCB = } k\\sum_{j=1}^{n}(\\overline{y}_{·j} - \\overline{y}_{··})^2 & \\text {n-1} & \\text {CMB = } \\frac{SCB}{n-1} & k\\sigma_{\\beta}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\text {Intrasujetos} & \\text {SCW = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{·j})^2 & \\text {n(k-1) = N-n} & \\text {CMW = } \\frac{SCW}{n(k-1)} & \\sigma_{\\alpha}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\hfill \\text {Intraagentes} & \\text {SCA = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}(\\overline{y}_{i·} - \\overline{y}_{··})^2 & \\text {(k-1)} & \\text {CMA = } \\frac{SCA}{(k-1)} & n\\sigma_{\\alpha}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\hfill \\text {Residual} & \\text {SCE = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{·i} - \\overline{y}_{·j} + \\overline{y}_{··})^2 & \\text {(n-1)(k-1)} & \\text {CME = } \\frac{SCE}{(n-1)(k-1)} & \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\text {Total} & \\text {SCT = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{··})^2 & \\text {N-1} & \\text {CMT = } \\frac{SCW}{N-1} & \\sigma^2\\\\\n\\hline\n\\end{array}\n}\n\\]\nEstamos haciendo una concordancia de todos los jueces, no oslo de los k de la muestra.\nICC.\n\\[\n\\displaylines{\\rho = \\frac{Cov(y_{ij},y_{i'j})}{Var(y_{ij})} = \\frac{\\sigma_{\\beta}^2}{\\sigma_{\\alpha}^2 + \\sigma_{\\beta}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2} \\Rightarrow \\\\\n\\hat\\rho = ICC(2,1) = \\frac{CMB - CME}{CMB + (k-1)CME + \\frac{k(CMA-CME)}{n}}}\\]\nContraste de hipótesis.\n¿hay un cirterio comun de los jeuces para lar las valoracion? rho=0, no.\n\\(H0: \\rho =0. \\quad \\quad\\) Estadístico:\n\\[F_0 = \\frac{CMB}{CME}, \\quad p-value = p(F_{n-1,v} &gt; F_{0})\\]\nIntervalo de confianza.\n\\[IC_{1-\\alpha}(\\hat\\rho) = (\\frac{n(CMB-F^*)}{F^*[kCMA+(kn-k-n)CME]+nCMB}, \\frac{n(F_*CMB-CME)}{kCMA+(kn-k-n)CME+nF_*CMB})\\] tal que\n\\[F_* = F_{n-1, \\nu, \\alpha/2} \\quad \\quad F_* = F_{\\nu, n-1, \\alpha/2}\\] \\[g.l. = \\nu = \\frac{(n-1)(k-1) \\left\\{ k \\hat{\\rho} \\frac{\\text{CMA}}{\\text{CME}} + n \\left[ 1 + (k-1) \\hat{\\rho} \\right] - k \\hat{\\rho} \\right\\}^2}{(n-1) k^2 \\hat{\\rho}^2 \\left( \\frac{\\text{CMA}}{\\text{CME}} \\right)^2 + \\left\\{ n \\left[ 1 + (k-1) \\hat{\\rho} \\right] - k \\hat{\\rho} \\right\\}^2} \\]\nEl IC no tiene el 0. No rechazo independencia entre las valoracions que los jeuces dan a los suejtos. ahora podemos dar quienes dan las valoraciones\nEjemplo\nEjemplo 1.5 (Shrout y Fleiss (1979))\n\nCódigoA&lt;- matrix(c(9,2,5,8,\n 6,1,3,2,\n 8,4,6,8,\n 7,1,2,6,\n 10,5,6,9,\n 6,2,4,7),ncol=4,byrow=TRUE)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    9    2    5    8\n[2,]    6    1    3    2\n[3,]    8    4    6    8\n[4,]    7    1    2    6\n[5,]   10    5    6    9\n[6,]    6    2    4    7\n\n\nCada uno de los 4 jueces seleccionados aleatoriamente valora a todos y cada uno de los 6 sujetos:\n\\[\n\\displaylines{\n&\\begin{array}\n{|c|c|c|c|}\n\\hline  \\textbf { Fuente de variación }& \\textbf { g.l. } & \\textbf { CM } \\\\\n\\hline\n\\text {Intersujetos / Entresujetos} & 5 & 11.24 \\\\\n\\hline\n\\text {Intrasujetos / Dentro de sujetos} & 18 & 6.26 \\\\\n\\hline\n\\hfill \\text {Intraagentes / Entre jueces } & 3 & 32.49 \\\\\n\\hline\n\\hfill \\text {Residual} & 15 & 1.02\\\\\n\\hline\n\\end{array}\n}\n\\]\n\nCódigodf &lt;- data.frame(\n  \"Fuente de variación\" = c(\"Intersujetos / Entresujetos\", \"Intrasujetos / Dentro de sujetos\", \"Entre jueces\", \"Residual\"),\n  \"g.l.\" = c(5, 18, 3, 15),\n  \"Cuadrado medio\" = c(11.24, 6.26, 32.49, 1.02)\n)\n\ndf\n\n               Fuente.de.variación g.l. Cuadrado.medio\n1      Intersujetos / Entresujetos    5          11.24\n2 Intrasujetos / Dentro de sujetos   18           6.26\n3                     Entre jueces    3          32.49\n4                         Residual   15           1.02\n\n\n\\[\n\\displaylines{\n\\text{ICC} = \\frac{CMB - CME}{CMB + (k-1) \\cdot CME + \\frac{k \\cdot (CMA - CME)}{n}} \\\\\n\\text{ICC} = \\frac{11.24 - 1.02}{11.24 + 3 \\cdot 1.02 + \\frac{4 \\cdot (32.49 - 1.02)}{6}} = 0.290\n}\n\\]\n\n(ICC &lt;- (11.24 - 1.02) / (11.24 + 3*1.02 + 4*(32.49 - 1.02)/6))\n\n[1] 0.2896825\n\n\n\nCódigolibrary(psych)\nICC &lt;- ICC(A, lmer=FALSE)\nICC$results[\"Single_random_raters\",]\n\n                     type       ICC        F df1 df2            p lower bound\nSingle_random_raters ICC2 0.2897638 11.02725   5  15 0.0001345665  0.01878651\n                     upper bound\nSingle_random_raters   0.7610844\n\n\n¿De quién depende ICC? la experimentacion conduce a comparar resultados, comparar puntuaciones de un individuos según diferentes jueces/criterios.\nCada sujeto es valorado por un conjunto diferente de k jueces, seleccionados aleatoriamente. Un juez puede evaluar más de una vez a un sujeto (¿creo que ha dicho eso?).\nCojo k jueces y evalúan al sujeto 1. Cojo otros k jueces y evalúan al sujeto 2. Selecciono otros k jueces y evalúan al siguiente sujeto. Etc. Un juez ha podido salir para evaluar a más de un sujeto.\nDiferencia entre esta resolución del ejercicio y el antetior (diferencia entre ICC_1 e ICC_2): en este método medimos cómo o cuánto fluyen las puntuaciones, no tanto la concordancia entre los jueces.\nCaso 3. ANOVA bifactorial de efectos mixtos.\n(24/10/2024)\nModelo teórico.\n\\[\ny_{ij} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + e_{ij}, \\quad i = 1, \\dots, k, \\quad j = 1, \\dots, n\n\\] \\[\n\\sum_{i=1}^{k}{\\alpha_i} = 0 \\quad\\quad \\text{ojo, eh, esto es la suma de unas ctes, es decir, son efectos fijos}\n\\]\n\\[\n\\left.\n\\begin{aligned}\n  \\{\\beta_j\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{\\beta}^2) \\\\\n  \\{e_{ij}\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{e}^2)\n\\end{aligned}\n\\right\\}\n\\quad \\text{independientes.}\n\\]\n\\[\n\\{(\\alpha\\beta_{ij}\\} \\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{\\alpha\\beta}^2)\n\\quad \\text{independientes.}\n\\] \\[\n\\sum_{i=1}^{k}{\\alpha\\beta_{ij}} = 0 \\quad \\Rightarrow \\quad Cov(\\alpha\\beta_{ij}, \\alpha\\beta_{i'j}) = -\\frac{\\sigma^2_{\\alpha\\beta}}{k-1}\n\\]\n\\[\n\\displaylines{\n& \\textbf {ANOVA y esperanzas de los cuadrados medios} \\\\\n&\\begin{array}\n{|c|c|c|c|}\n\\hline  \\textbf { F. variación } & \\textbf { SC } & \\textbf { g.l. } & \\textbf { CM }  & \\textbf { E(CM) } \\\\\n\\hline\n\\text {Intersujetos} &\\text {SCB = } k\\sum_{j=1}^{n}(\\overline{y}_{·j} - \\overline{y}_{··})^2 & \\text {n-1} & \\text {CMB = } \\frac{SCB}{n-1} & k\\sigma_{\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\text {Intrasujetos} & \\text {SCW = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{·j})^2 & \\text {n(k-1) = N-n} & \\text {CMW = } \\frac{SCW}{n(k-1)} & \\frac{1}{k}\\sum_{i=1}^{k}\\alpha^2_i + \\frac{k}{k-1}\\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\hfill \\text {Intraagentes} & \\text {SCA = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}(\\overline{y}_{i·} - \\overline{y}_{··})^2 & \\text {(k-1)} & \\text {CMA = } \\frac{SCA}{(k-1)} & \\frac{n}{k}\\sum_{i=1}^{k}\\alpha^2_i + \\frac{k}{k-1}\\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\hfill \\text {Residual} & \\text {SCE = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{·i} - \\overline{y}_{·j} + \\overline{y}_{··})^2 & \\text {(n-1)(k-1)} & \\text {CME = } \\frac{SCE}{(n-1)(k-1)} & \\frac{k}{k-1}\\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\text {Total} & \\text {SCT = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{··})^2 & \\text {N-1} & \\text {CMT = } \\frac{SCW}{N-1} & \\sigma^2\\\\\n\\hline\n\\end{array}\n}\n\\]\n¿Por qué alphabeta son DEPENDIENTES?\nYo le pregunté eso a ChatGPT y me dijo:\nEn un modelo ANOVA bifactorial de efectos mixtos, como el que se muestra en la imagen, los términos \\((\\alpha\\beta_{ij})\\), que representan la interacción entre los efectos fijos \\(\\alpha{i}\\)y los efectos aleatorios \\(\\beta_{j}\\), son dependientes porque la varianza de las interacciones está ligada tanto a la variabilidad de los efectos aleatorios como a los efectos fijos.\nEl efecto \\((\\alpha\\beta_{ij})\\) refleja cómo la combinación de un nivel del factor fijo iii con un nivel del factor aleatorio \\(\\beta_{j}\\) puede influir en la variable de respuesta \\(y_{ij}\\). Los efectos aleatorios \\(\\beta_{j}\\) y el término de interacción están correlacionados porque ambos se ven influenciados por las mismas fuentes de variabilidad del factor aleatorio.\nEsto ocurre debido a que el factor aleatorio introduce variabilidad que no es completamente explicada por los efectos fijos \\(\\alpha{i}\\), generando una dependencia en el término de interacción. Esta dependencia puede modelarse con una distribución de varianza conjunta, como se indica en la imagen: \\(N(0, \\sigma_{\\alpha \\beta}^2)\\).\nPor otro lado, los términos \\(\\beta_{j}\\) y \\(e_{ij}\\) son independientes entre sí, ya que representan fuentes de variabilidad distintas: \\(\\beta_{j}\\) describe la variabilidad entre niveles del factor aleatorio, mientras que \\(e_{ij}\\) representa el error experimental o residual.\nICC.\n\\[\n\\displaylines{\\rho = \\frac{Cov(y_{ij},y_{i'j})}{Var(y_{ij})} = \\frac{\\sigma_{\\beta}^2 - \\frac{\\sigma^2_{\\alpha\\beta}}{k-1}}{\\sigma_{\\alpha}^2 + \\sigma_{\\beta}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2} = \\frac{\\sigma_{\\beta}^2 - \\frac{\\sigma^2_{\\alpha\\beta}}{k-1}}{\\sigma_{\\beta}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2} \\Rightarrow \\\\\n\\hat\\rho = ICC(3,1) = \\frac{CMB - CME}{CMB + (k-1)CME}}\\]\nContraste de hipótesis.\n\\(H0: \\rho =0. \\quad \\quad\\) Estadístico:\n\\[F_0 = \\frac{CMB}{CME}, \\quad p-value = p(F_{n-1,(n-1)(k-1)} &gt; F_{0})\\]\nIntervalo de confianza.\n\\[IC_{1-\\alpha}(\\hat\\rho) = (\\frac{F_{L}-1}{F_{L}+(k-1)}, \\frac{F_{U}-1}{F_{U}+(k-1)})\\] tal que\n\\[F_{L} = \\frac{F_{0}}{F_{n-1, (n-1)(k-1), \\alpha/2}} \\quad \\quad F_{U} = F_{0} * F_{(n-1)(k-1), n-1, \\alpha/2}\\]\nEl IC no tiene el 0. No rechazo independencia entre las valoracions que los jeuces dan a los suejtos. ahora podemos dar quienes dan las valoraciones\nEjemplo\nEjemplo 1.5 (Shrout y Fleiss (1979)).\n\nCódigoA&lt;- matrix(c(9,2,5,8,\n 6,1,3,2,\n 8,4,6,8,\n 7,1,2,6,\n 10,5,6,9,\n 6,2,4,7),ncol=4,byrow=TRUE)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    9    2    5    8\n[2,]    6    1    3    2\n[3,]    8    4    6    8\n[4,]    7    1    2    6\n[5,]   10    5    6    9\n[6,]    6    2    4    7\n\n\nCada uno de los 4 jueces valora a todos y cada uno de los 6 sujetos:\n\\[\n\\displaylines{\n&\\begin{array}\n{|c|c|c|c|}\n\\hline  \\textbf { Fuente de variación }& \\textbf { g.l. } & \\textbf { CM } \\\\\n\\hline\n\\text {Intersujetos / Entresujetos} & 5 & 11.24 \\\\\n\\hline\n\\text {Intrasujetos / Dentro de sujetos} & 18 & 6.26 \\\\\n\\hline\n\\hfill \\text {Intraagentes / Entre jueces } & 3 & 32.49 \\\\\n\\hline\n\\hfill \\text {Residual} & 15 & 1.02\\\\\n\\hline\n\\end{array}\n}\n\\]\n\nCódigodf &lt;- data.frame(\n  \"Fuente de variación\" = c(\"Intersujetos / Entresujetos\", \"Intrasujetos / Dentro de sujetos\", \"Entre jueces\", \"Residual\"),\n  \"g.l.\" = c(5, 18, 3, 15),\n  \"Cuadrado medio\" = c(11.24, 6.26, 32.49, 1.02)\n)\n\ndf\n\n               Fuente.de.variación g.l. Cuadrado.medio\n1      Intersujetos / Entresujetos    5          11.24\n2 Intrasujetos / Dentro de sujetos   18           6.26\n3                     Entre jueces    3          32.49\n4                         Residual   15           1.02\n\n\n\\[\n\\displaylines{\n\\text{ICC} = \\frac{CMB - CME}{CMB + (k-1) \\cdot CME} \\\\\n\\text{ICC} = \\frac{11.24 - 1.02}{11.24 + 3} = 0.7148\n}\n\\]\n\n(ICC &lt;- (11.24 - 1.02) / (11.24 + 3*1.02))\n\n[1] 0.7146853\n\n\n\nCódigolibrary(psych)\nICC &lt;- ICC(A, lmer=FALSE)\nICC$results[\"Single_fixed_raters\",]\n\n                    type       ICC        F df1 df2            p lower bound\nSingle_fixed_raters ICC3 0.7148407 11.02725   5  15 0.0001345665   0.3424648\n                    upper bound\nSingle_fixed_raters   0.9458583\n\n\nTenemos:\n\nICC, la ESTIMACIÓN del rho.\nEl estadístico del contraste.\nEl IC: refleja algo sobre la H0."
  },
  {
    "objectID": "tema_01/tema_01_3_concordancia_numericas.html#método-gráfico-de-bland-y-altman",
    "href": "tema_01/tema_01_3_concordancia_numericas.html#método-gráfico-de-bland-y-altman",
    "title": "3. Análisis de concordancia en variables numéricas ✓",
    "section": "Método gráfico de Bland y Altman ✗\n",
    "text": "Método gráfico de Bland y Altman ✗\n\nMétodo gráfico, sin análisis estadístico de apoyo.\nSe ha extendido su uso para analizar la concordancia entre dos métodos que utilizan las mismas unidades de medida. Consiste en representar gráficamente el promedio de las dos observaciones frente a su diferencia.\nPermite examinar la magnitud de las discrepancias y su relación con la magnitud de la medida.\nSolamente permite comparar dos observaciones por individuo. Si k medidas con k instrumentos sobre un individuo (k&gt;2) se necesitan hacer comparaciones dos a dos.\n¿Hay una concordancia entre medidas de un mismo individuo?\nSi hubiese concordancia y fuese perfecta el resultado del método gráfico tendría que ser exactamente igual que el resultado con el método no gráfico.\nSea \\((X_1, Y_1), (X_2, Y_2), (X_3, Y_3)\\),…, tal que:\n\\(n_i\\) ~ individuo i.\\(X_i\\) ~ resultado de la variable en el individuo i con el método 1.\\(X_i\\) ~ resultado de la variable en el individuo i con el método 2.\nSe realiza un cambio de referencia.$ \\[\n\\left( \\frac{X_i + Y_i}{2}, \\quad \\quad  X_i - Y_i \\right)\n\\] Gráficamente, se pasa de un diagrama de dispersión a un diagrama de Bland-Altman.\n\\[\\left\\{(X_j, Y_j), \\quad j=1,…n \\right\\} \\quad \\rightarrow \\quad \\left\\{{\\left( \\frac{X_i + Y_i}{2},X_i - Y_i \\right), \\quad j=1,…n}\\right\\}\\]\nSi X e Y fueran iguales (CONCORDANTES) tendríamos medias muy cerca del 0.\nEjemplo 1.6.\nslide 41. pegar.\n¿Sirve hacer un estudio de regresión lineal?\nUn modelo lineal es capaz de encontrar una ecuacion que relacione ambas valoraciones X e Y (se podría considerar incluso como un cambio de escala). En el estudio de concordancia no se pretende ver la relacion entre valoraciones, sino si existe concordancia.\nValdría exclusivamente si al ecuación obtenido fuese Y = X.\nFigura 1.\nCuando damos una medida con el monitor esta suele ser superior a la dada por el esfigmomanómetro.\nFigura 2.\n¿Sigmomanómetro - Monitor?¿Monitor - Sigmomanómetro? Con al figura 1 sabemos y el eje de las diferencias de la figura 2 sabemos que ha sido Sigmomanómetro - Monitor.\nIncluye una recta de la referencia promedio y dos bandas de confianza que calculadas apoyándonos en las propiedades de la media.\nEstamos suponiendo:\n\nQue las difenrencias tiene distribución media (que la van a tener)\nQue hay varianza constante en las diferencias.\n\nLímites de concordancia\nDadas las diferencias entre X e Y, \\(\\left\\{d_j = (X_j, Y_j), \\quad j=1,…n\\right\\}\\), sea \\(\\bar{d} = \\frac{1}{N}\\sum d_j\\) y sea \\(s_d = \\text{desviacion tipica de la muestra de las diferencias de los individuos}\\).\nSuponemos que las diferencias tienen distribución normal y varianza constante. Los límites de concordancia se definen como:\n\\[\\bar{d} \\pm 1.96s_d\\] El \\((1-\\alpha) %\\) muestras de los individuos tienen diferencia muestras que van desde \\(\\bar{d} + 1.96s_d\\) hasta \\(\\bar{d} - 1.96s_d\\).\nSe pueden calcular los errores típicos e intervalos de confianza para los límites de concordancia.\nIC de los límites de concordancia\nAquí entra la suposición de varianza constante. Ya que damos por hecho que tenemos media gaussiana.\nDados unos límites les añadimos una medida de concentración.\nEstos límites siguen una distribucion de t-Student.\nconsigo acotar .\nen media mi diff de mediadas era 16. cuando haría los intervalos tenia -55 y 22. en el caso más optimista tengo -14 29.\ntengo una diferncia que como poco -14+29, y como mucho -61+47\nhay que poner el IC y los intervalos sobre los IC.\n-55,22: intervalo para decir que tengo concentrado el 95 de los datos ahí. este Ic no me dice pracitcamente nada, a esos dos límites les doy una holgura con sus respectivos IC.\nEjemplo 1..\nel 95 % muestras de los individuos tienen diferencia muestras que van de -55 a +22.\nla diff entre un proc y el otro se mueve en promedio en 16 unidades (sobre estima el monitor, o el otro, vamos) a favor del monitor y las diff entre los dos para el 95 % de los individuos se mueve entre 55 y +22.\na parte del Ic tenemos una banda para el IC.\nEjemplo 1.7.\nComparan el volumen de plasma con la medida normal.\nvemos que el movlumen de plasma está por encima de la diagonal, claramente una de las medidas está por encima siempre.\nsi las dos medidas fieran concordantesdeberían estar al rededor del cero.\nen el eje_x bajo las diferencias son más pequeñas y cuando x es grande las diferencias son más grandes. las diferncias resultasn que de donde estén las medidas van creciendo (las diferrencias se abren) oh oh parece que tenemos una varianza que no es cote. las medidas con valores más bajo se dierencian menos que las medidas de cuando tienen altos valores.\nestamos trabajando con la medidas de un individuo sobre una medida estándar. lo que hacemos es trasnformar. Repasar las transofrmciones de Cox: son una familia de transformaciones potenciales usadas en estadística para corregir sesgos en la distribución de errores. Cuando tenemos que transformar medidas que hacen referencia a un estandar lo que se recomienda es hacer medias logar´timocas.\n\\[\n\\log(\\frac{a}{b}) = \\log(a) - \\log(b)\n\\]\ncambiamos la escala y las unidades de medida. obviamente para comunicar los resultados tengo que volver a la escala original\nlo que quiero es darle un intervalo a las medias iniciales.\nla media de media(log) no la estamos usando, sobre todo la usamos para ver dónde tengo los datos, pero la medida que realmente uso para sacar conclusiones es la diferencia (la dif de logaritmos en este caso)\n1.058 &lt; N_i / H_i &lt; 1.153\nes un cociente, la medida de nerder como mínimo es mayor en un 5 % que la de harley.\nla medida de N siempre es superior a la de H, con una superioridad que va de un 5.8 % a un 15.3 %\ndiapo54\ncuando tengamos jeuces desconocidos que dna puntuaciones y jueces perfectamente conocidos y vemos cómo concuerdan o no (caso 1 y caso 3)\nusamos un método recursivo que busca n reiteradamente\ntodos los intervalos tiene la misma estructura. partiendo de mi intervalo incial voy a obtener mi F_0 (ya viene en la salida del ICC). Con ello identifico los gl (dependiendo del caso en el que estemos tendremos unos grados de libertad distintos)\npara un n, saco mis estadísticos y mi IC. Si mi IC no cumple los requisitos del longitud del intervalo que me piden, aumento mi n en una unidad. calculo mis estadísticos y mi IC. Si mi IC no cumple los requisitos del longitud del intervalo que me piden, aumento mi n en una unidad.\npara determinar el Ic seguimos un procedimiento iterado. partiendo del tamaño inicial n de la muestra y del valor del estadístoco F_0 aumentamos el tamaño de la muestra en una unida y calculamos los límites del nuevo IC. si la longitud de ete IC satistace las condcciones ya tenemos determinado n, si no, volvemos a aumentar en una unidad el tamño muestral."
  },
  {
    "objectID": "tema_01/tema_01_4_tamanyo_muestral.html#determinación-del-tamaño-muestral-para-el-coeficiente-kappa",
    "href": "tema_01/tema_01_4_tamanyo_muestral.html#determinación-del-tamaño-muestral-para-el-coeficiente-kappa",
    "title": "4. Cálculo del tamaño muestral ✓",
    "section": "Determinación del tamaño muestral para el coeficiente kappa",
    "text": "Determinación del tamaño muestral para el coeficiente kappa\n\\[\n\\displaylines{\n\\text{Quiero una precisión más pequeña.} \\\\\n{\\Downarrow} \\\\\n\\text{La precisión depende de la varianza.} \\\\\n{\\Downarrow} \\\\\n\\text{La varianza depende del tamaño muestral.}\n}\n\\]\nSupongo que n muestral es grande. ¿Puedo suponerlo? Pos claro amego, n es lo que estoy buscando calcular así que puedo desear que n sea grande.\nPara buscar esta nueva n pongo una condición: que el valor máximo de la varianza sea menor a un valor dado. La varianza con el nuevo tamaño \\(n\\) debe ser menor a la varianza con el tamaño de la muestra \\(n_{0}\\).\nEjemplo.\nSupongamos tengo n=20. Calculo el IC y tengo la amplitud del intervalo. Voy a querer un IC tal que la amplitud sea menor a cierto valor \\(k\\) (puedo tener la suerte de que con mi estudio piloto ya cumpla esa condición). Por lo que quiero un n condicioando a un valor máximo de la varianza (que depende de n).\nFijando el valor máximo de la varianza puedo despejar el valor de n.\n\nCaso dicotómico: dos observadores y dos categorías\n\nDefinicion de la varianza en el modelo teórico.\n\\[\n{\\bf\nV(k) = \\frac{A+B+C}{n(1-\\Pi_c)^4}\n}\n\\]\n\ntal que\n\\[\n\\displaylines{\nA = \\pi_{11}[1- \\pi_c - (\\pi_{1·}+\\pi_{·1})(1-\\pi_0)]^2 + \\pi_{22}[1- \\pi_c - (\\pi_{2·}+\\pi_{·2})(1-\\pi_0)]^2 \\\\\nB = (1-\\pi_0)^2[\\pi_{12}(\\pi_{·1}+\\pi_{2·})^2 + \\pi_{21}(\\pi_{·2}+\\pi_{1·})^2] \\\\\nC = (\\pi_0 - 2\\pi_c+\\pi_0\\pi_c)^2\n}\n\\]\nTodos los valores quedan determinados a partir de \\(\\pi_{1·}\\), \\(\\pi_{·1}\\) y \\(k\\).\n\\[\n\\displaylines{\n\\pi_{2·} = 1 - \\pi_{1·} \\\\\n\\pi_{·2} = 1 - \\pi_{·1} \\\\\n\\pi_{c} = \\pi_{1·}\\pi_{·1} + (1 - \\pi_{1·})(1 - \\pi_{·1}) \\\\\n\\pi_{0} = k(1-\\pi_{c}) + \\pi_{c} \\\\\n\\pi_{22} = (\\pi_{0} - \\pi_{1·} + \\pi_{2·}) / 2 = (\\pi_{0} + 1) / 2 \\\\\n\\pi_{11} = 1 - \\pi_{22} = 1 - (\\pi_{0} + 1) / 2 \\\\\n\\pi_{12} = \\pi_{1·} - \\pi_{11} \\\\\n\\pi_{21} = \\pi_{·1} - \\pi_{11}\n}\n\\]\n\n\nCaso dicotómico: dos observadores y t categorías (kappa sin ponderar)\n\n\\[\n{\\bf\nV(k) = \\frac{A+B+C}{n(1-\\Pi_c)^4}\n}\n\\]\ntal que\n\\[\n\\displaylines{\n\\bf{A = \\sum_{i=1}^{t} \\pi{ii}[1-\\pi_c-(\\pi_{i·}+\\pi_{·i})(1-\\pi_0)^2]} \\\\\n\\bf{B = (1-\\pi_0)^2 \\sum_{i=1}^{t} \\sum_{\\substack{i=1 \\\\ i \\neq j}}^{t} \\pi_{ij}(\\pi_{·i}+\\pi_{j·})^2} \\\\\n\\bf{C = (\\pi_0 - 2\\pi_c+\\pi_0\\pi_c)^2}\n}\n\\]"
  },
  {
    "objectID": "tema_01/tema_01_4_tamanyo_muestral.html#n-óptimo",
    "href": "tema_01/tema_01_4_tamanyo_muestral.html#n-óptimo",
    "title": "4. Cálculo del tamaño muestral ✓",
    "section": "n óptimo",
    "text": "n óptimo\n¿Cuál sería el n óptimo partiendo de una muestra incial y un IC incial para conseguir reducir la amplitud del intervalo?\n\nTengo un n que me da la posibilidad de estimar un n neuvo dada una condición.\n\n\nFijando la longitud del intervalo\n\nSupongamos que tenemos información de una muestra de tamaño \\(n_0\\) para la cual se tiene que \\[IC_{\\hat{k}}^{1-\\alpha} \\hspace{1em} = \\hspace{1em} (\\hat{k_0} - z_{\\frac{\\alpha}{2}} * \\sqrt{Var(k)}, \\hspace{1em} \\hspace{1em} \\hat{k_0} + z_{\\frac{\\alpha}{2}} * \\sqrt{Var(k)}\\]\nLlamamos \\[l_{0} \\hspace{1em} = \\hspace{1em} \\text{longitud del intervalo} = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{Var(k)}\\]\nBusco un tamaño muestral \\(n\\) tal que el IC para \\(\\hat{k}\\) con la muestra de tamaño n tenga una longitud \\(w\\). \\[w = \\text{longitud del intervalo de la muestra de tamaño n} = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{Var(k)} = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{A+B-C}{n_0(1-\\Pi_{c})^4}}\\]\nA la estimacion de \\(n\\) le asignamos el mismo error que tendría con una muestra más pequeña (la peor de las situaciones que podríamos tener). \\[w = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{A+B-C}{n_0(1-\\Pi_{c})^4}} = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{A+B-C}{n(1-\\Pi_{c})^4}}\\]\nBusco un \\(n\\) tal que su longitud sea menor igual a \\(w\\).\n\n\\[\n\\begin{cases}\nl_{0} = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{1}{n_0}} * \\sqrt{\\frac{A+B-C}{(1-\\Pi_{c})^4}} \\\\\nw = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{1}{n}} * \\sqrt{\\frac{A+B-C}{(1-\\Pi_{c})^4}}\n\\end{cases}\n\\]\n\nDespejando en \\(l_{0}\\).\n\n\\[\\frac{l_{0} * \\sqrt{n_0}}{2*z_{\\frac{\\alpha}{2}}} = \\sqrt{\\frac{A+B-C}{(1-\\Pi_{c})^4}}\\]\n\nSustituyo en \\(w\\).\n\n\\[w = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{1}{n}} * \\sqrt{\\frac{A+B-C}{(1-\\Pi_{c})^4}} = \\frac{2*z_{\\frac{\\alpha}{2}}}{\\sqrt{n}} * \\frac{l_{0} * \\sqrt{n_0}}{2*z_{\\frac{\\alpha}{2}}} = \\frac{l_{0} * \\sqrt{n_0}}{\\sqrt{n}}\\]\n\nDespejando en \\(w\\).\n\n\\[w = \\frac{l_{0} * \\sqrt{n_0}}{\\sqrt{n}} \\Longleftrightarrow w^2 = \\frac{l_{0}^2 * n_0}{n}\\]\n\\[n = (\\frac{l_{0}}{w})^2*n_0\\]\n\n\nResolver el contraste de hipótesis sobre \\(k\\) ✗\nla condicion que hemos puesto para fijar n es que la longitud del intervalo la fijamos\ntambien pueddo fijar una hipótesis de un crontraste de hipótesis, pe, diapo52, una hipótesis sobre el índico k_0."
  },
  {
    "objectID": "tema_02/tema_02.html",
    "href": "tema_02/tema_02.html",
    "title": "TEMA 2",
    "section": "",
    "text": "M.S. Pepe.\nThe statistical evaluation of medical tests for classification and predictor.\nValidez y prueba diagnóstica.\nPrueba diagnóstica: prueba que trata de clasificar a nun individuo en un grupo adecuado.\nLas prueba diagnóstica las manejamos en un contexto clínico pero la idea es extrapolable a otros contexto (prueba diagnóstica de un conocimiento adquirido durante la carrera).\nLa validez de una prueba diagnóstica es cuando refleja aquello para lo que se ocntruyó. Si hago un examen, este debe estar hecho para comprobar si un estudiante domina o no los conceptos.\nValidez en el sentido de representatividad, “y este estimador es válido/representativo para tal”.\n¿La prueba diagnóstica sirve para lo que fue creada?\nPrevalencia = probabilidad. Calcularé la prevalencia estimada."
  },
  {
    "objectID": "tema_02/tema_02_1_introduccion.html#tests-binarios",
    "href": "tema_02/tema_02_1_introduccion.html#tests-binarios",
    "title": "1.1. Conceptos básicos ✓",
    "section": "Tests binarios",
    "text": "Tests binarios\nDada una gold standar, sea \\(D\\) v.a. dicotómica que denota el estado de la enfermedad: variable que dice el estado real del paciente respecto a la enfermedad.\n\\[\nD =\n\\begin{cases}\n1, & \\text{si el sujeto tiene la enfermedad} \\\\\n0, & \\text{si el sujeto no tiene la enfermedad} \\\\\n\\end{cases}\n\\]\n\\[\\rho = \\text{prevalencia} = P(D=1) = \\text{probabilidad real de tener la enfermedad.}\\]\nSea \\(Y\\) v.a.que modeliza el resultado de la prueba diagnóstica: variable que dice el resiltado de la prueba diagnóstica.\n\\[\nY =\n\\begin{cases}\n1, & \\text{si el resultado del test diagnóstico es positivo} \\\\\n0, & \\text{si el resultado del test diagnóstico es negativo} \\\\\n\\end{cases}\n\\]\nClasificación\n\n\n\n\n\n\n\n\n\n\n\n\nVerdadero estado de la enfermedad\n\n\n\nResultado del test\nD = 1\nD = 0\n\n\n\n\nY = 1\nVerdadero positivo\nFalso positivo\n\n\nY = 0\nFalso negativo\nVerdadero negativo\n\n\n\n\n Y, D ∈ {0,1}\n\n\n\n\n\n\nEl test da el resultado para la enfermedad, no el diagnóstoco. Queremos medir si la prueba diagnóstica es válida para que, cuando esta de positivo o negativo, una persona 100tífika pueda tomar una decisión en base a esa prueba.\nA partir de las probabilidades de estos sucesos se definen las siguientes probabilidades.\n\n\nProbabilidades de acierto. Probabilidades que determinan cómo de válidas son las pruebas que estamos midiendo\n\nProbabilidades de fallo. Ya que creamos las probabiliades de error\n\n\nProbabilidades de acierto.\nProbabilidades que determinan cómo de válidas son las pruebas que estamos midiendo: la sensibilidad (Se) y la especificidad (Sp). Ambas son probabilides de acertar.\nSensibilidad. Probabilidad de clasificar correctamente a un enfermo. (“enfermo” y no “positivo” porque se que la realidad es enfermo - por ejemplo, gracias a la gold standar )\n\\[Se = p(Y = 1 | D = 1)\\]\nEspecificidad. Probabilidad de clasificar correctamente a un sano.\n\\[Sp = p(Y = 0 | D = 0)\\]\n\n\nProbabilidades de fallo.\nTambién están relacionadas con las probabilidades de fallo ya que creamos las probabiliades de error falso positivo (FP) y falso negativo (FN).\nProbabilidad de falsos negativos. Probabilidad de clasificar erróneamente a un enfermo.\n\\[p(FN) = 1 - Se = 1 - p(Y = 1 | D = 1) = p(Y = 0 | D = 1)\\]\nProbabilidad de falsos positivos. Probabilidad de clasificar erróneamente a un sano.\n\\[p(FN) = 1 - Sp = 1 - p(Y = 0 | D = 0) = p(Y = 1 | D = 0)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nVerdadero estado de la enfermedad\n\n\n\nResultado del test\nD = 1\nD = 0\n\n\n\n\nY = 1\nTP = Se\nFP = 1- Se\n\n\nY = 0\nFN = 1 - Sp\nTN = Sp\n\n\n\n\n Y, D ∈ {0,1}\n\n\n\n\n\n\nQué paxa para ser un test ideal. \\(\\rightarrow\\) Las probabilidades de acertar son máximas \\(\\rightarrow\\) Se = Sp = 1.Qué paxa para ser un test inutil. \\(\\rightarrow\\) La probabilidad de que el test de positivo es la misma cuando el sujeto está enfermo que cuando está sano. \\(\\rightarrow\\) Se = 1 – Sp.\nQué más paxa para ser un test inútil.\n\n\nDar positivo a todo el mundo. \\(\\rightarrow\\) Dar positivo tenga o no la enfermedad.\n\nMaximiza el acierto \\(Se\\) (todos los positivos están identificados).\npero maximiza también el error \\(1-Sp\\) (todos los negativos se asigna como positivos).\n\n\n\nDar negativo a todo el mundo. \\(\\rightarrow\\) Dar negativo esté sano o no.\n\nMaximiza el acierto \\(Sp\\) (todos los negativos están identificados).\npero maximiza también el error \\(1-Se\\) (todos los positivos se asigna como negativos).\n\n\n\n\nMedidas asociadas a un test.\nmedidas de clasificacion, probabilidades de clasificar bien a un individuo.\nProbabilidades de clasificación (correcta a un individuo).\n- Se - 1-Sp\nProbabilidad de emitir un diagnóstico erróneo (contexto no médico).\nValores predictivos.\nRazón de verosimilitudes.\n- Odds pre-test - Odds post test\n\nSi quiero hacer una compartaiva con varias pruebas diagnósticas, al tener dos medidas, ¿cuál priorizo para la comprativa? (novio guapo y pobre vs novio feo y rico)\nNecesitamos una escala doble. Así podemos pasar nuestra decision en una comparativa donde las dos comparativas iniciales están reducidas a una única dimensión.\n\\[\n\\begin{aligned}\np(\\text{emitir un diagnóstico erróneo}) &= p(Y \\neq D) \\\\\n&= p(FP) + p(FN) \\\\\n&= p(Y=1 \\cap D=0) + p(Y=0 \\cap D=1) \\\\\n&= p(Y=1 | D=0) \\cdot p(D=0) + p(Y=0 | D=1) \\cdot p(D=1) \\\\\n&= (\\text{un error}) \\cdot p(D=0) + p(\\text{otro error}) \\cdot \\rho \\\\\n&= (1 - \\text{Sp}) \\cdot (1-\\rho) + (1 - \\text{Se}) \\cdot \\rho\n\\end{aligned}\n\\]\nEsto me dice cuál de los dos errores tiene más “importancia”.\nPensar en el significado que tiene una prevalencia alta o baja en relación con cada uno de los errores. A mayor prevalencia da más peso a tal error porque…\nEjemplo.\nImaginemos una una prueba diagnóstica que siempre da positivo a cualquier individuo.\n\\(p(Y=1 | D=0) = p(Y=1 | D=1) = 1.\\)\n\nSi la prevalencia es grande la prueba es fantástica.\nSi la prevalencia es pequeña la prueba apesta.\nLa prueba no me sirve para nada porque no discrimina.\n\nCuando comparamos entre varias prueba diagnóstica tenemos dos medidas que evalúan la calidad de cada una de las pruebas. Para lidiar ese problema usamos otra medida mediante los valores predictivos y las razones de verosimilitudes.\nVale, mijo, pero cuando yo hago un test y tengo un positivo, ¿qué hago?\nDesde un punto de vista clínico, la utilidad de una test recae en tener el resultado de la prueba y hacer un diagnóstico.\nLa utilidad de una prueba diagnóstica puede cuantificarse analizando las posibilidades de que el resultado de la prueba prediga el verdadero estado del sujeto.\nValor predictivo positivo: \\(PPV = P(D=1 | Y=1)\\)Valor predictivo negativo: \\(NPV = P(D=0 | Y=0)\\)\nEs decir:\n\nSi da un resultado positivo, ¿cuál es la probabilidad de que realmente esté enfermo?\n\nSi da un resultado negativo, ¿cuál es la probabilidad de que realmente esté sano?\n\nTest ideal: PPV = NPV = 1\nTest sin utilidad: PPV = \\(\\rho\\) y NPV = 1 - \\(\\rho\\)\nLos valores predictivos dependen de la prevalencia.\nQue un test sea más sensible o más específico no depende de la prevalencia.\nEjemplo 2.1.\nSe considera una prueba para diagnosticar el cáncer de mama. Supongamos que la prevalencia en la población femenina es del 1%. En un estudio caso-control las probabilidades de resultados positivos del test fueron 0,85 dentro de los casos y 0,03 dentro de los controles. ¿Cuáles son los valores predictivos positivo y negativo?\nQue sea de caso-control ahora me vale verga no me sirve para nada en este punto de evaluacion del modelo.\n\\(0.01 = p(D=1) = \\text{prevalencia}\\)\\(0.85 = p(Y=1 | D=1) = Se\\)\\(0.03 = p(Y=1 | D=0) = 1-Sp\\)\nCálculo del PPV.\n\\[\\begin{aligned} PPV &= P(D=1 | Y=1) \\\\ &= \\frac{p(D=1 \\cap Y=1)}{p(Y=1)} \\\\ &= \\frac{p(Y=1 | D=1) · p(D=1)} {p(Y=1 | D=1) · p(D=1) + p(Y=1 | D=0) · p(D=0)} \\\\ &= \\frac{0.85 · 0.01}{0.85 · 0.01 + 0.03 · 0.99} \\end{aligned}\\]\n\n((0.85 * 0.01) / (0.85 * 0.01 + 0.03 * 0.99))\n\n[1] 0.2225131\n\n\nEl 22.25 % de los resultados positivos padecerán la enfermedad.\nCálculo del NPV.\n\\[\\begin{aligned} NPV &= P(D=0 | Y=0) \\\\ &= \\frac{p(D=0 \\cap Y=0)}{p(Y=0)} \\\\ &= \\frac{p(Y=0 | D=0) · p(D=0)} {p(Y=0 | D=0) · p(D=0) + p(Y=0 | D=1) · p(D=1)} \\\\ &= \\frac{(1-0.03) · (1-0.99)}{(1-0.03) · (1-0.99) + (1-0.85) · 0.99} \\end{aligned}\\]\n\n((1-0.03) * 0.99)/((1-0.03) * (1-0.01) + (1-0.85) * 0.01)\n\n[1] 0.9984404\n\n\nEl 99.84 % de los resultados negativos no padecerán la enfermedad.\nLa prueba diagnóstica perfecta no es. Si fuera perfecta tendríamos que haber obtenido un 1 en ambos valores predictivos (que dependen de los resultados de la prueba). Ya que eso significaría:\n\nPPV=1: que cuando la prueba da positivo la probabilidad de que el individuo sea un positivo es 1 AKA todos los positivos son TP.\n\nNPV=1: que cuando la prueba da negativo la probabilidad de que el individuo sea un negativo es 1 AKA todos los negativos son TN.\n\nUtilidad: ¿podemos hacer una criba?\nPara una persona que da un resultado negativo tenemos una probabilidad muy alta de no tener la enfermedad (0.9984). Es importante asegurarse de que si a una paciente se le dice que está sano realmente lo esté.\nLa prueba no sirve para diagnosticar un positivo de la muestra. Para los individuos donde la prueba dé positivo o repito la prueba o hago otro test diferente.\n¿Qué miramos en esta prueba o qué utilidad le damos? Aquello que nos conviene: los resultados o las probabilidades altas.\nOdds\nDependen de la prevalencia.\nRepresentan la razón entre la probabilidad de que ocurra un evento y la probabilidad de que no ocurra.\n\\[Odds = \\frac{p(D=1)}{1-p(D=1)}\\]\nSe interpretan como _por cada vez que ocurre el evento, hay x veces en las que el evento no ocurre_.\nRazones de verosimilitud (Diagnostic Likelihood Ratio)\nNo dependen de la prevalencia: son medidas inherentes a la prueba diagnóstica.\nSe trata de razones de verosimilitud utilizadas en el contexto de pruebas diagnósticas para medir la capacidad de una prueba para diferenciar entre personas con y sin enfermedad.\nIndican cómo de plausible/admisible es algo.\n\\(DLR^{???} = \\frac{P(Y=??? | D=111)}{P(Y=??? | D=000)}, \\quad DLR^{???} \\in (0, \\infty)\\)\n\nRazón de verosimilitud de diagnóstico positivo: se trata de una comparación de dos situaciones donde tengo resultados positivos. \\(DLR^+ = \\frac{P(Y=1 | D=1)}{P(Y=1 | D=0)} = \\frac{Se}{1 - Sp}, \\quad DLR^+ \\in (0, \\infty)\\)\nCuánto más probable es obtener un resultado positivo en alguien con la enfermedad comparado con alguien sin la enfermedad.\n\nComparo cómo de creible es tener un positivo en los sanos respecto a tener un positivo en los enfermos. Lo idóneo es tener unas razones de verosimilutd altas, ya que refleja que hay mucha más probabilidad de tener positivos en los enfermos de tener positivos en los sanos.\n\nRazón de verosimilitud de diagnóstico negativo: se trata de una comparación de dos situaciones donde tengo resultados negativos. \\(DLR^- = \\frac{P(Y=0 | D=1)}{P(Y=0 | D=0)} = \\frac{1 - Se}{Sp}, \\quad DLR^- \\in (0, \\infty)\\)\nCuánto más probable es obtener un resultado negativo en alguien con la enfermedad comparado con alguien sin la enfermedad.\n\nTest ideal:\n\nPensemos:\n\nQuiero que la probabilidad de \\(P(Y=1 | D=1)\\) sea mayor a \\(P(Y=1 | D=0) \\Rightarrow P(Y=1 | D=1) &gt;&gt;&gt; P(Y=0 | D=1) \\Rightarrow\\) \\(DLR^+ = +\\infty\\)\n\nQuiero que la probabilidad de \\(P(Y=0 | D=1)\\) sea menor a \\(P(Y=0 | D=0) \\Rightarrow P(Y=0 | D=1) &lt;&lt;&lt; P(Y=0 | D=0) \\Rightarrow\\) \\(DLR^- = 0\\)\n\n\n\nSi no llegamos a ese punto, al menos:\n\n\n\\(DLR^+ \\ge 1\\)\n\n\\(DLR^- \\le 1\\)\n\n\n\nTest no informativo:\n\nEl test tiene la misma probabilidad de un resultado positivo en el grupo de los enfermos que en el grupo de los sanos.\n\nEl test tiene la misma probabilidad de un resultado negativo en el grupo de los sanos que en el grupo de los enfermos.\nAmbos puntos se resumen en que un test no informativo es aquel que: \\(DLR^+ = DLR^- = 1\\)\n\n\nTest que no quiero: - El test no tiene mayor probabilidad de resultado positivo en el grupo de los enfermos que en el grupo de los sanos. \\(\\Rightarrow DLR^+ \\lt 1\\).\n- El test no tiene mayor probabilidad de resultado negativo en el grupo de los sanos que en el grupo de los enfermos. \\(\\Rightarrow DLR^- \\gt 1\\).\nLas razones de verosimilitud cuantifican el conocimiento que se ha ganado acerca de la presencia de la enfermedad tras realizar el test AKA sirven para evaluar la utilidad de una prueba diagnóstica.\nPasamos te comparar probabilidad de tener la enfermedad vs probabilidad de no tenerla a comparar probabilidad de tener la enfermedad sabiendo el resultados del test vs. probabilidad de no tener la enfermedad sabiendo el resultado del test.\n\n\n\n\n\n\n\nInterpretación\nValor deseado\nValores útiles\n\n\n\nDLR+ &gt; 1: un resultado positivo aumenta la probabilidad de enfermedad.\nCuanto más alto es el DLR+, mayor es la utilidad de la prueba para confirmar la enfermedad.\nValores típicamente considerados útiles: DLR+ &gt; 10.\n\n\nDLR− &lt; 1: Un resultado negativo disminuye la probabilidad de enfermedad.\nCuanto más cerca de 0 es el DLR−, mayor es la utilidad de la prueba para descartar la enfermedad.\nValores típicamente considerados útiles: DLR− &lt; 0.1.\n\n\n\nUtilidad\nVer cuánta información añade el resultado de una prueba diagnóstica.\nTras realizar el test, cómo cambia el resultado esa percepción sobre la presencia de la enfermedad.\n\\[\\textbf{En ausencia de test} \\Rightarrow \\quad \\text{Odd pre-test} = \\frac{p(D=1)}{p(D=0)}\\]\n\\[\\textbf{Después de realizar el test} \\Rightarrow \\quad \\text{Odd post-test} = \\frac{p(D=1 | Y)}{p(D=0 | Y)}\\]\n\\[\\text{Odd post-test} = \\text{Odd pre-test} × DLR\\] - El DLR cuantifican el cambio en las odds de la enfermedad por el conocimiento del resultado del test.\n- A estas razones de verosimilitud de diagnóstico también se les conoce como factores Bayes.\n\\[\\text{Odd post-test (con Y=1)} = \\text{Odd pre-test} × DLR^+\\]\n\\[\\text{Odd post-test (con Y=0)} = \\text{Odd pre-test} × DLR^-\\]\nLa forma de interpretar las razones de verosimilitud no es probabilistas.\nDiferencias entre Odds y DLR\n\n\n\n\n\n\n\nAspecto\nOdds\nRazones de verosimilitud\n\n\n\nDefinición\nRazón entre probabilidad de éxito y fracaso.\nRelación entre la sensibilidad/especificidad y la probabilidad de resultados positivos/negativos.\n\n\nAplicación\nDescribe la probabilidad relativa de un evento.\nEvalúa la capacidad de una prueba diagnóstica para discriminar entre personas con y sin enfermedad.\n\n\nUnidades\nNúmero puro.\nNúmero puro.\n\n\nRelación con pruebas diagnósticas\nSe usa en la conversión entre probabilidad y razones de verosimilitud.\nSe usa para ajustar la probabilidad pre-prueba y calcular la probabilidad post-prueba.\n\n\n\nEjemplo 2.1. (cont.)\n\\(DLR^+ = \\frac{Se}{1-Sp} = \\frac{0.85}{1-0.97} = 28.3333\\) \\(DLR^- = \\frac{1-Se}{Sp} = \\frac{1-0.85}{0.97} = 0.1546\\)\nEstas razones de verosimilitud indican que la prueba tiene una buena capacidad para confirmar la enfermedad (\\(DLR^+ &gt; 10\\)) y una moderada capacidad para descartarla (\\(DLR^- &lt;0.2\\)).\nLa estimacion para las razones dependen de las estimaciones de Se y Sp. Los valores predictivos no.\n\\[\\text{Odd pre-test} = \\frac{p(D=1)}{p(D=0)} = \\frac{0.01}{1-0.01} = 0.0101\\] Antes de hacerme la prueba: sin hacerme la prueba la únnica informacion que tengo la prevalencia de la enfermedad\n\\[\\text{Odd post-test (para un resultado positivo)} = \\frac{p(D=1|Y=1)}{p(D=0|Y=1)} = \\frac{0.2225}{1-0.2225} = 0.28617.\\] Cuando tengo un positivo en la prueba: tener ese positivo en la prueba diagnóstica aumenta la credebilidade de que tengo la enfermedad. Aumenta la credebilidad de tener la enfermedad, no la probabilidad. Ha aumentado en casi 28.83 %. 28 veces es la relación que existe entre las dos situaciones.\n\\[\\text{Odd post-test (para un resultado positivo)} = \\text{Odd pre-test} · DLR^+ = 0.0101 · 28.33 = 0.28613\\]\nSi pensamos en los resultados negativos:\n\nPartimos igual (sin hacerme la pruebea): 1 a favor de tenerlo vs 99 a favor de no tenerlo (el odd pre-test).\nAl tener un resultado negativo: \\(\\text{Odd post-test (para un resultado negativo)} = \\frac{p(D=1|Y=0)}{p(D=0|Y=0)} = \\frac{1-0.9984}{0.9984} = \\frac{1}{629} = 0.0016.\\) Pasamos de una credebilidad de tener 1 a 99 a una credebilidad de 1 de tener la enfermedad a 629 de no tenerla.\n\n¿Un resultado negativo es seis veces más verosimil en un paciente sano que en uno enfermo?\nLa \\(DLR^−\\) mide cuántas veces más probable es obtener un resultado negativo en personas sin la enfermedad comparado con personas con la enfermedad.\nComo \\(DLR^− = 0.157 \\approx \\frac{1}{6}\\), entonces sí. Un resultado negativo es aproximadamente 6 veces más probable en personas sanas que en personas con la enfermedad."
  },
  {
    "objectID": "tema_02/tema_02_1_2_cohortes_y_casos_control.html#estudios-cohorte",
    "href": "tema_02/tema_02_1_2_cohortes_y_casos_control.html#estudios-cohorte",
    "title": "1.2 Estudios de cohorte y casos-control ✓",
    "section": "Estudios cohorte",
    "text": "Estudios cohorte\nSupongamos que realizamos un experimento con resultados {éxito, fracaso} y sea \\(\\text{p = p(éxito)}\\). Repetimos el experimento n veces en condiciones independientes. Sea \\(X \\sim \\text{\"número de éxitos en n pruebas\"}\\) bajo el modelo teórico B(n,p). Sea \\(\\hat p\\) = número de éxitos observados en el total de muestras = \\(\\frac{x}{n}\\)\n\\(N\\) = nº de sujetos seleccionados aleatoriamente de la población\\(n_{{D}}\\) = nº de sujetos con la enfermedad de los N seleccionado\\(n_{\\overline{D}}\\) = nº de sujetos sin la enfermedad de los N seleccionados\\(n^+\\) = nº de sujetos que han dado positivo en el test de los N\\(n^-\\) = nº de sujetos que han dado negativo en el test de los N\n\n\n\n\\[\n\\scriptsize\n\\begin{array}{cc|c|c|c}\n& & \\text{prueba +} & \\text{prueba -} & \\\\\n& & D = 0 & D = 1 & \\\\\n\\hline\n\\text{persona enferma} & \\text{Y = 0} & n_{\\overline{D}}^{-} & n_{D}^{-} & n^{-} \\\\\n\\text{persona sana} & \\text{Y = 1} & n_{\\overline{D}}^{+} & n_{D}^{+} & n^{+} \\\\\n\\hline\n&& n_{\\overline{D}} & n_{D}\n\\end{array}\n\\]\n\n\n\\[\n\\scriptsize\n\\begin{array}{c|c|c|c}\n& \\text{persona enferma} & \\text{persona sana} & \\\\\n& \\text{Y = 0} & \\text{Y = 1} & \\\\\n\\hline\n\\text{prueba +} \\ D = 0 & n_{\\overline{D}}^{-} & n_{\\overline{D}}^{+} & n_{\\overline{D}} \\\\\n\\text{prueba -} \\ D = 1 & n_{D}^{-} & n_{D}^{+} & n_{D} \\\\\n\\hline\n& n^{-} & n^{+}\n\\end{array}\n\\]\n\n\n\n\nDetrás de cada una de las probabilidades hay una distribucion binomial. Los intervalos de confianza de cada proporción binomial \\(\\hat p\\) se puede calcular a través de:\n\n\nIntervalos de confianza exactos \\(\\Rightarrow\\) Intervalo de Clopper-Pearson (se basa en la función de distribución binomial acumulativa invertida)\n\n\\[\n\\text{IC}_{1-\\alpha} = \\left[\n\\frac{x}{x + (n - x + 1)F_{1-\\alpha/2, 2x, 2(n-x+1)}}, \\,\n\\frac{(x+1)F_{\\alpha/2, 2(x+1), 2(n-x)}}{(n-x) + (x+1)F_{\\alpha/2, 2(x+1), 2(n-x)}}\n\\right]\n\\]\n\n\nLa distribución binomial o métodos asintóticos \\(\\Rightarrow\\) Cuando \\(n\\) es grande, las observaciones son independientes y \\(\\hat p\\) no es cercano a 0 o 1 (eventos raros)\n\n\\[\n\\left( \\frac{x}{m} \\pm z_{\\alpha/2} \\sqrt{\\frac{ x\\left(1 - \\frac{x}{m}\\right)}{m}} \\right)\n\\]\n\nEstimadores (I)\n\nPara estimar la probabilidad de verderos negativos \\(\\widehat{Se}\\) se usa la v.a.:\n\\[\nX \\sim \\text{\"número de negativos en el grupo de enfermos\"} \\sim Bin(n_{D}^{+}, Se) = Bin(n_{D}^{+}, n_D)\n\\]\nPara estimar la probabilidad de falsos positivos \\(\\widehat{(1-Sp)}\\) se usa la v.a.:\n\\[\nX \\sim \\text{\"número de enfermos en el grupo de sanos\"} \\sim Bin(n_{\\overline{D}}^{+}, (1-Sp)) = Bin(n_{\\overline{D}}^{+}, n_{\\overline{D}})\n\\]\nPara estimar el valor predictivo \\(\\widehat{PPV}\\) se usa la v.a.:\n\\[\nX \\sim \\text{\"número de individuos enfermos en sujetos con test postitivo.\"} \\sim Bin(n_{\\overline{D}}, (1-Sp)) TERMINAR\n\\]\n\\[\nX \\sim Bin(n^+, ppv), \\quad \\quad x =  n_{\\overline{D}}^+\n\\]\nPara estimar el valor predictivo \\(\\widehat{NPV}\\) se usa la v.a.:\n\\[\nX \\sim \\text{número de individuos enferos en los que tienen test postitivo} ~ Bin(n^-, npv), \\quad \\quad x =  n_{\\overline{D}}^- TERMINAR\n\\]\n\nEjemplo del intervalo de confianza exacto para la sensibilidad.\n\\[\nIC_{1-\\alpha} (\\widehat{\\text{Se}}) =\n\\left(\n\\frac{1}{1 + \\frac{n_D^+ F_{2n_D^+ , 2(n_D - n_D^+ + 1); 1-\\alpha/2}}{n_D^+ - n_D^+ + 1}} , \\\n\\frac{1}{1 + \\frac{(n_D^+ + 1) F_{2(n_D^+ + 1), 2(n_D - n_D^+); \\alpha/2}}{n_D - n_D^+}}\n\\right)\n\\]\nEjemplo del intervalo de confianza por el método asintótico para la sensibilidad.\n\\[\nIC_{1-\\alpha} (\\widehat{Se}) =\n\\left(\n\\frac{n^+_D}{n_D} - z_{\\alpha/2} \\cdot \\sqrt{\\frac{1}{n_D} \\cdot \\frac{n^+_D \\left( n_D - n^+_D \\right)}{n_D}}, \\;\n\\frac{n^+_D}{n_D} + z_{\\alpha/2} \\cdot \\sqrt{\\frac{1}{n_D} \\cdot \\frac{n^+_D \\left( n_D - n^+_D \\right)}{n_D}}\n\\right)\n\\]\nEjemplo 2.2.\nEl objetivo es determinar qué constituye un éxito y en qué subgrupo estamos evaluando la cantidad de éxitos.\nEn el cálculo de estimaciones e IC, en cada IC que genero cometo un error y a mayor númeor de estimaciones e IC el error total de mi proyecto aumenta o la precision total disminuye.\n- Probabiidad de que la sensibilidad esté dentro de mi intervalo, 0.9.\n- Probabiidad de que la especificidad esté dentro de mi intervalo, 0.9.\n- Probabiidad de que la sensibilidad esté dentro de mi intervalo y de que la especificidad esté dentro de mi intervalo, 0.9*0.9. (es un cálculo teórico para entenderlo)\nLos estimadores de la probabilidades de clasificación son independientes. Esto nos motiva a buscar una region de confianza de tal forma que ambas estimaciones estén dentro de esa región con una probabilidad del 0.9.\nRegión de confianza para (\\(\\widehat{Se}, (\\widehat{1-Sp})\\))\n\nConfianza bidimensional \\(1-\\alpha = \\text{\"prob de caer en el IC * prob de caer en el otro IC\"} = \\beta^2 : \\beta = \\sqrt{1-\\alpha}\\).\n\nLa RC es una zona de confort en dos dimensiones. En caso de trabajar en dos dimensiones separadas consigo una confianza menor.\nEjemplo 2.2. (cont.)\nLos siguientes datos corresponden a un estudio cohorte realizado en el que se tenían 1465 hombres con posible enfermedad coronaria para los que se tiene el resultado de una prueba diagnóstica de esfuerzo (EST). La determinación de la enfermedad se realiza con arteriografía, la medida gold standard. Los datos obtenidos son los siguientes:\n\nlibrary(dplyr)\n\ndf &lt;- data.frame(col1 = c(327, 115),\n                 col2 = c(208, 815)\n)\ncolnames(df) &lt;- c(\"D=0\", \"D=1\")\nrownames(df) &lt;- c(\"Y=0\", \"Y=1\")\n\ndf_extended &lt;- rbind(df, \"Total\" = colSums(df))\ndf_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended))\ndf_extended\n##       D=0  D=1 Total\n## Y=0   327  208   535\n## Y=1   115  815   930\n## Total 442 1023  1465\n\nEn medcalc por defecto te calcula la asitótica (independientemente del tamaño que tengas). (para estudio de cohortes no hace falta indicarle la prevalencia porque da igual en este caso)\n\\(\\widehat{PPV}\\), estimo los casos positivos: 930 + cuántos lo son de verdad: 815 = \\(\\frac{815}{930}\\)\n\\(\\widehat{NPV}\\), casos negativos: 327 + cuántos lo son de verdad: 535 = \\(\\frac{327}{535}\\), un resultado positivo en la prueba estaría asociado a detectar un 87,63% de los casos\n\nsource(\"calculo_estimadores_pruebas_diagnosticas.R\")\n\n\nn_D &lt;- 1023\nn_D_pos &lt;- 815\nn_D_neg &lt;- 208\nn_noD &lt;- 442\nn_noD_pos &lt;- 115\nn_noD_neg &lt;- 327\nn_pos &lt;- n_D_pos + n_noD_pos\nn_neg &lt;- n_D_neg + n_noD_neg\n\n\nlibrary(tibble)\ncalculo_estimadores(n_D, n_D_pos, n_D_neg, n_noD, n_noD_pos, n_noD_neg, n_pos, n_neg, tipo_estudio = \"cohorte\") |&gt; as.data.frame() |&gt; rownames_to_column(var = \"rownames\") %&gt;% filter(rownames %in% c('Se', 'Sp', 'Uno_menos_Sp', 'PPV', 'NPV'))\n##       rownames   Puntual  IC.lower  IC.upper\n## 1           Se 0.7966764 0.7720135 0.8213394\n## 2           Sp 0.7398190 0.6989177 0.7807203\n## 3 Uno_menos_Sp 0.2601810 0.2192797 0.3010823\n## 4          PPV 0.8763441 0.8551872 0.8975010\n## 5          NPV 0.6112150 0.5699080 0.6525219\n\nUn resultado positivo en la prueba estaría asociado a detectar un 87,63% de los casos.\nEstimamos que al 79,67 % de los enfermos se les detectará la enfermedad mediante la prueba de esfuerzo. Estimamos que un 26,02 % de los sanos también se les diagnosticará como enfermos.\nRegión de confianza rectangular: \\(\\displaylines{IC_{95} (\\widehat{1-Sp}, \\widehat{Se}) = (0.2193, 0.3011) x (0.772, 0.821) : \\\\ (1−\\alpha^{*}) * (1−\\alpha^{*}) = (1−\\alpha) \\Rightarrow (1−\\alpha^{*}) = \\sqrt{1−\\alpha} = \\sqrt{0.95} = 0.9747}\\)\nEstimadores (II) - DLR\nComparación de casos positivos dentro del grupo de enfermos y casos positivos dentro del grupo de sanos \\(\\Rightarrow\\) En funcion del cociente se puede decididr cuál es más verosímil.\nSe estiman las razones de verosimilitud a través de las estimaciones de las probabilidades de clasificaciones.\n\n\n\n\\(\\widehat{DLR^+} = \\frac{\\widehat{Se}}{\\widehat{1-Sp}} = \\frac{{\\frac{n_{D}^{+}}{n_D}}}{\\frac{n_{\\overline{D}}^{+}}{n_{\\overline{D}}}}\\)\n\n\n\\(\\widehat{DLR^-} = \\frac{\\widehat{1-Se}}{\\widehat{Sp}} = \\frac{{\\frac{n_{D}^{-}}{n_D}}}{\\frac{n_{\\overline{D}}^{-}}{n_{\\overline{D}}}}\\)\n\n\n\nIC para las razones de verosimilutes\nLos intervalos de confianza de cada razón de verosimilitud estimada se determinan a partir del resultado asintótico.\nPasamos los DLR a escala logarítmica y separamos las componentes sensibilidad y especificidad. Trabajamos con el método delta.\n\\[\n\\left( \\log DLR^{+}, \\log DLR^{-} \\right) \\approx \\mathcal{N} \\left( \\left( \\log DLR^{+}, \\log DLR^{-} \\right);\n\\begin{pmatrix}\n\\frac{1 - Se}{n_D Se} + \\frac{Sp}{n_D (1 - Sp)} & - \\left( \\frac{1}{n_D} + \\frac{1}{n_D} \\right) \\\\\n\\left( \\frac{1}{n_D} + \\frac{1}{n_D} \\right) & \\frac{Se}{n_D (1 - Se)} + \\frac{1 - Sp}{n_D Sp}\n\\end{pmatrix}\n\\right)\n\\]\nEjemplo.\nDado el \\(IC(log(\\widehat{DLR^+})) = (-0.6, 2) : e^-0.6 &lt; \\widehat{DLR+} &lt; e^2\\). Se obtiene el \\(IC(\\widehat{DLR^+}) = (e^{-0.6}, e^2) = (0.55, 0.79)\\)\n“Las dos razones de verosimilitud no son independientes sino que cuando trabajamos con sus logaritmos tienen un comportamiento normal bidimensional. La covarianza negativa nos indica que una razón aunmenta la otra disminuye.”\n\nlibrary(MASS)\nlibrary(ggplot2)\n\n# Definimos la media y la matriz de covarianza\nmedia &lt;- c(0, 0)\ncovarianza &lt;- matrix(c(1, 0.5, 0.5, 1), nrow = 2)\n\n# Generamos los puntos con una distribución normal multivariada\ndatos &lt;- mvrnorm(n = 1000, mu = media, Sigma = covarianza)\ndatos &lt;- as.data.frame(datos)\ncolnames(datos) &lt;- c(\"X\", \"Y\")\n\nlibrary(plot3D)\nx &lt;- seq(-3, 3, length = 50)\ny &lt;- seq(-3, 3, length = 50)\nz &lt;- outer(x, y, function(x, y) mclust::dmvnorm(cbind(x, y), mean = media, sigma = covarianza))\npersp3D(x, y, z, theta = 30, phi = 20, expand = 0.6, col = \"lightblue\", main = \"Distribución Gaussiana Bidimensional\")\nmtext(\"Esto lo hago porque me aburro\", side = 3, line = 0.5, cex = 0.8)  # side = 3 coloca el texto arriba, line ajusta la distancia\n\n\n\n\nEjemplo 2.2. (cont. 21)\n\ncalculo_estimadores(n_D, n_D_pos, n_D_neg, n_noD, n_noD_pos, n_noD_neg, n_pos, n_neg, tipo_estudio = \"cohorte\") |&gt; as.data.frame() |&gt; rownames_to_column(var = \"rownames\") %&gt;% filter(rownames %in% c('DLR_pos', 'DLR_neg'))\n\n  rownames   Puntual  IC.lower  IC.upper\n1  DLR_pos 3.0620086 2.6086910 3.5941001\n2  DLR_neg 0.2748288 0.2405299 0.3140186\n\n\n\\(DLR^{+}\\): un resultado positivo es 3 veces más creíble en el grupo de enfermos que en el grupo de sanos\n\\(DLR^{-}\\): comparamos negativos en el grupo de sanos y en el grupo de enfermos. En el grupo de sanos deben ser pocos, por que lo que \\(DLR^{+}&lt;&lt;&lt;1\\). Para facilitar la comparativa se le suele dar la vuelta (al parecer es más fácil interpretar un 2 que un 0.5)\nEn el 95 % de las muestras es entre 2.6 y 3.5 veces más creíble un resultado positivo en el grupo de los enfermos que en los sanos.\nEl test de esfuerzo da positivo tres veces más en los enfermos que en sanos. Es tres veces más versosímil encontrar un positivo en la prueba de esfuerzo en un individuo enfermo que en un individio sano.\nCuando tenemos un resultado negativo en la prueba es 3.7 (1/0.2748) más verosímil un negativo en un individuo sano que en un individuo enfermo."
  },
  {
    "objectID": "tema_02/tema_02_1_2_cohortes_y_casos_control.html#estudios-caso-control",
    "href": "tema_02/tema_02_1_2_cohortes_y_casos_control.html#estudios-caso-control",
    "title": "1.2 Estudios de cohorte y casos-control ✓",
    "section": "Estudios caso control",
    "text": "Estudios caso control\nEl número de sujetos enfermos seleccionados y el de sujetos sanos seleccionados son predeterminados.\nEsta situación equilibrada no suele ser representativa de la realidad. La frecuencia relativa de sujetos enfermos es normalmente mucho mayor que en la población, al menos cuando la prevalencia es relativamente baja.\nLa prevalencia muestral no representativa de la prevalenciapoblacional no afecta a las estimaciones de sensibilidada, especificidad ni a las razones. Pero sí afectaa los valores predictivos.\nTrabajamos suponiendo que tenemos una estimación para la prevalencia real.\nEn estudios de caso-control la prevalencia de la población no se conserva en la muestra (los sujetos se elige en función de si tienen o no la enfermedad). En los estudios de cohortes ese problema no existe ya que se ha buscado un grupo de individuos que se someten a un riesgo y se espera a ver si tenían o no la enfermedad.\nDefinimos los valores predictivos en función de la prevalencia. Con el teorema de Bayes se expresan los valores predictivos en función de la sensibilidad y la especificidad.\n\\[\n\\scriptsize\n\\begin{aligned} PPV &= P(D=1 | Y=1) \\\\ &= \\frac{p(D=1 \\cap Y=1)}{p(Y=1)} \\\\ &= \\frac{p(Y=1 | D=1) · p(D=1)} {p(Y=1 | D=1) · p(D=1) + p(Y=1 | D=0) · p(D=0)} \\\\ &= \\frac{\\rho * Se}{\\rho * Se + (1-\\rho)(1-Sp)} \\end{aligned}\n\\begin{aligned} NPV &= P(D=0 | Y=0) \\\\ &= \\frac{p(D=0 \\cap Y=0)}{p(Y=0)} \\\\ &= \\frac{p(Y=0 | D=0) · p(D=0)} {p(Y=0 | D=0) · p(D=0) + p(Y=0 | D=1) · p(D=1)} \\\\ &= \\frac{(1-\\rho) · Sp}{(1-\\rho) · Sp + \\rho · (1-Sp)} \\end{aligned}\n\\]\n\\[\\hat{PPV} = \\frac{\\rho * \\hat{Se}}{\\rho * \\hat{Se} + (1-\\rho)(1-\\hat{Sp})} \\quad \\quad \\quad \\hat{NPV} = \\frac{(1-\\rho) · \\hat{Sp}}{(1-\\rho) · \\hat{Sp} + \\rho · (1-\\hat{Sp})}\\]\nSupongamos que desconocemos el valor exacto de la prevalencia:\n\nSi la prevalencia es máxima, \\(\\text{si }\\rho=1 \\Rightarrow PPV = 1 \\text{ y } NPV = 0\\).\nSi la prevalencia es mínima, \\(\\text{si }\\rho=0 \\Rightarrow PPV = 0 \\text{ y } NPV = 1\\).\n\n\n\n\n\n\n\nDefinimos los IC valores predictivos en función de las DLR.\n\\[\\scriptsize \\frac{PPV}{1-PPV} = DLR^+ (\\frac{\\rho}{1-\\rho}) \\Rightarrow ln(\\frac{PPV}{1-PPV}) = ln(DLR^+) + ln(\\frac{\\rho}{1-\\rho}) = ln(DLR^+) + logit(\\rho) = ln(DLR^+) + H\\]\n\n\\(\\begin{aligned} \\text{Sea } IC\\left(\\ln\\left(\\widehat{DLR^-}\\right)\\right) = (A, B) &\\Rightarrow A &lt; ln(\\widehat{DLR^-}) &lt; B \\\\ &\\Rightarrow -B &lt; -ln(\\widehat{DLR^-}) &lt; -A \\\\ &\\Rightarrow - H - B &lt; logit(NPV) &lt; - H - A \\\\ &\\Rightarrow - (H + B) &lt; logit(NPV) &lt; - (H + A) \\\\ &\\Rightarrow - (H + B) &lt; ln(\\frac{NPV}{1-NPV}) &lt; - (H + A) \\\\ &\\Rightarrow e^{- (H-B)} &lt; \\frac{NPV}{1-NPV} &lt; e^{- (H+A)} \\\\ &\\Rightarrow e^{- (H-B)} * (1-NPV) &lt; NPV &lt; e^{- (H+A)} * (1-NPV) \\\\ &\\Rightarrow \\text{Como } \\widehat{ppv} &gt; 0, exp^{b+H} &gt; exp^{a+H}(1-\\widehat{ppv}) &gt; \\frac{exp^{b+H}}{1+exp^{b+H}}, \\\\ & \\quad\\quad \\frac{e^{- (H-B)}}{1 -e^{- (H-B)}} &lt; NPV &lt; \\frac{e^{- (H+A)}}{1 -e^{- (H+A)}} \\end{aligned}\\)\nEjemplo 2.3.\nSupongamos que en un estudio caso-control, para una enfermedad con una prevalencia de 2%, el intervalo de confianza para el logaritmo de la razón de verosimilitud de diagnóstico positivo es \\(IC_{95} \\left( log(\\widehat{DLR^+}) \\right) = (1, 2.5)\\)) ¿Cuál sería el intervalo de confianza para el valor predictivo positivo?\n\\(IC_{95}(logit(\\widehat{PPV^+})) = ( log \\left( \\frac{0.02}{0.98} \\right) + 1.5, log \\left( \\frac{0.02}{0.98} \\right) + 2.5 ) = (-2.39, -1.39)\\)\n\\(IC_{95}(\\widehat{PPV^+}) = \\left( \\frac{e^{-2.39}}{1+e^{-2.39}}, \\frac{e^{-1.39}}{1+e^{-1.39}} + 2.5 \\right) = (0.08, 0.2)\\)\nEl IC indica que la prueba diagnóstica no sirve para diagnosticar la enfermedad. Dando positivo el test solo acierta la enfermedad de 0.08 a 0.2. La probabilidad de que dando positivo estemos ante un individuo enfermo es muy baja entre el 8 % y el 20 %. Otra cosa sería el NPV. No lo sabemos pero por lo menos podría ser buena para determinar que con un resultado negativo el paciente estuviera sano.\nLa prueba diagnóstica no es buena ya que solo acertaría entre un 0.08 y un 0.2 de las veces basándome en haber visto un valor positivo en la prueba. Si tomáramos esta prueba diagnóstica para hacer un diagnóstico en función del resultado los aciertos son muy pocos."
  },
  {
    "objectID": "tema_02/tema_02_1_2_cohortes_y_casos_control.html#comparación-entre-pruebas-diagnósticas",
    "href": "tema_02/tema_02_1_2_cohortes_y_casos_control.html#comparación-entre-pruebas-diagnósticas",
    "title": "1.2 Estudios de cohorte y casos-control ✓",
    "section": "Comparación entre pruebas diagnósticas",
    "text": "Comparación entre pruebas diagnósticas\nEl objetivo es comparar 2 test diagnósticos: test A y test B. Podemos comparar tres medidas (cada una de ellas enfocadas a un objetivo distinto): - Probabilidades de clasificación (cómo refleja la prueba el estado real del paciente.)\n\nValores predictivos (cómo refleja la prueba el estado real del paciente)\nRazones de verosimilitud de diagnóstico (cómo de creible es un valor postivio o negativo)\n\nEn caso de no encontrar una prueba diagnóstica “superior” a otra basándonos en las probabilidades de clasificación, podemos intentar encontrarla por medio de los valores predictivos.\nMétricas para la comparación.\n\nDiferencias absolutas (\\(\\Delta\\)) (p.e. \\(\\Delta Se(A,B) = Se(A) - Se(B))\\)\n\nOddratios (ο) (p.e. \\(οSe(A,B) = \\frac{\\frac{Se(A)}{1-Se(A)}}{\\frac{Se(B)}{1-Se(B)}} = \\frac{Se(A)·(1-Se(B))}{Se(B)·(1-Se(A))}\\)\n\nCocientes o medidas relativas (r) (p.e. \\(rSe(A,B) = \\frac{Se(A)}{Se(B)}\\))\n\nAunque se puede elegir cualquiera de ellas para comparar dos pruebas, los cocientes son más fáciles de interpretar que los odd ratios. Intenamos encontrar diferencias absolutas y en caso de no haberlas lo intentamos con los cocientes relativos.\nEn caso de tener dos pruebas diagnósticas basadas en unidades distintas, no podemos hacer difrencia de una contra la otra, pero sí puedo hacer comparaciones haciendo comparaciones entre ambas pruebas.\nProbabilidades de clasificación\nSe puede utilizar cualquiera métrica de comparación. Aunque la más utilizada es la métrica de cocientes o medidas relativas debido a su fácil interpretación.\nEjemplo:\n- \\(rSe(A;B) = \\frac{Se(A)}{Se(B)} =25\\): el test A tiene 25 veces más verdaderos positivos que el test B.\n- \\(rSe(A,B) = 0.3\\) : la cantidad de verdaderos positivos del test A es el 30 % de los observados en el test B\nValores predictivos\nLa métrica utilizada son los odds ratios o cocientes.\nSi la prevalencia de la enfermedad es baja:\n\n\\(rPPV(A,B) \\approx oPPV(A,B)\\)\n\\(rNPV(A,B) \\approx 1\\)\n\nSi la prevalencia no es baja es preferible utilizar cocientes\nEjemplo:\n- \\(rPPV(A;B)=2.0\\): un resultado positivo en el test A es 2 veces más indicativo del riesgo de enfermedad que el que tiene un resultado positivo en el test B.\nRazones de verosimilitud de diagnóstico:\nSe comparan con la métrica de cocientes.\nExiste una relación con los odds ratios de los valores predictivo:\n\n\\(rDLR+(A,B) = oPPV(A,B)\\)\n\\(rDLR-(A,B) = \\frac{1}{oNPV(A,B)}\\)\n\nEjemplo: \\(DLR^+(A)=3.05\\), \\(DLR^+(B)=1.71\\).\n- Ambos test indican que es más verosímil un resultado positivo en enfermos que en sanos.\n- \\(rDLR^+(A,B) = 1.78\\): el resultado positivo del test A es más informativo del riesgo de enfermedad que el positivo del test B."
  },
  {
    "objectID": "tema_02/tema_02_2_comparacion_test_y_tamanyos_muestrales.html#qué-test-es-mejor",
    "href": "tema_02/tema_02_2_comparacion_test_y_tamanyos_muestrales.html#qué-test-es-mejor",
    "title": "2. Comparación de test y tratamiento del tamaño mmuestral ✗",
    "section": "¿Qué test es mejor?",
    "text": "¿Qué test es mejor?\nA es preferible a B, en la métrica del cociente, al comparar las probabilidades de clasificación: rSe(A,B) &gt; 1 y r(1-Sp(A,B)) &lt; 1, (i)\nA es preferible a B, en la métrica del cociente, al comparar los valores predictivos: rPPV(A,B) &gt; 1 y rNPV(A,B) &lt; 1, (ii)\nA es preferible a B, en la métrica del cociente, al comparar las razones de verosimilitud del diagnóstico: rDLR+(A,B) &gt;1 y rDLR-(A,B) &lt;1, (iii)\nSe tiene que:\n(i) \\(\\iff\\) (ii)\n(i) \\(\\implies\\) (iii)\nSi un test es mejor en una dimensión y peor en otra (no sé a qué dimensiones se refiere), entonces es aconsejable tomar una medida única construida a partir de ambas dimensiones:\n\nMedida compuesta basada en costes:\n\nBoyko (Med Dec Making, 1994)\nGold et al. (1996). Cost-Effectiveness in Health and Medicine\n\n\nMedida compuesta de Chock et al. (J Clin Epidemiol, 1997):\n\n(TPFA-TPFB)/(FPFA-FPFB)=(Se(A)-Se(B)/(Sp(B)-Sp(A))\n\n\n\nLa validez de la preferencia está relacionada con la validez de los estimación de la métrica empleada. Es preciso realizar un estudio de la comparativa desde el punto de vista de la muestra. Se necesita:\n\nDeterminar estimadores de las métricas\nValidar dichos estimadore\n\nCon las estimaciones puntuales para la coparativa y sus validaciones, podemos tomas decisiones sobre qué prueba diagnóstica es mejor con la comparativa adecuada."
  },
  {
    "objectID": "tema_02/tema_02_2_comparacion_test_y_tamanyos_muestrales.html#tratamiento-muestral-de-la-comparativa",
    "href": "tema_02/tema_02_2_comparacion_test_y_tamanyos_muestrales.html#tratamiento-muestral-de-la-comparativa",
    "title": "2. Comparación de test y tratamiento del tamaño mmuestral ✗",
    "section": "Tratamiento muestral de la comparativa",
    "text": "Tratamiento muestral de la comparativa\nEn la comparativa de dos pruebas diagnósticas, es importante la forma en que se han elegido los individuos y cómo se ha construido el estudio comparativo.\nDiseño pareado. Cada sujeto se somete a todas las pruebas que se quieren comparar. - Son válidos cuando las pruebas diagnósticas no interfieren entre ellas - Se debe diseñar el experimento de forma que las pruebas se realicen en distinto orden para cada sujeto.\nDiseño no pareado. Un sujeto solo se somete a una prueba. Cada test se aplica a un conjunto de sujetos diferente. - Implican un protocolo detallado del estudio. - Se aplican en pruebas diagnósticas invasivas, que causan malestar en el paciente, que consumen tiempo o que conllevan un riesgo significativo. - En pruebas que interfieren en los resultados de la otra. - En estudios observacionales.\nEs preferible el diseño pareado ya que genera menos variabildiad inter sujetos.\nNotación.\n\\[\n\\begin{aligned}\n\\textbf{No pareado}\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n\\textbf{D=0}\n\\end{aligned}\n\\begin{array}{c|c|c|c|}\n& Y = 0 & Y = 1 \\\\\n\\hline\n\\text{Test A} & n_{D}^{-}(A) & n_{D}^{+}(A) & n_{D}(A)\\\\\n\\text{Test B} & n_{D}^{-}(B) & n_{D}^{+}(B) & n_{D}(B)\\\\\n\\hline\n\\end{array}\n\\]\n\n\n\\[\n\\begin{aligned}\n\\textbf{D=1}\n\\end{aligned}\n\\begin{array}{c|c|c|c|}\n& Y = 0 & Y = 1 \\\\\n\\hline\n\\text{Test A} & n_{\\overline{D}}^{-}(A) & n_{\\overline{D}}^{+}(A) & n_{\\overline{D}}(A)\\\\\n\\text{Test B} & n_{\\overline{D}}^{-}(B) & n_{\\overline{D}}^{+}(B) & n_{\\overline{D}}(B)\\\\\n\\hline\n\\end{array}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n\\textbf{Pareado}\n\\end{aligned}\n\\]\n\n\n\n\\[\n\\begin{aligned}\n\\textbf{D=1}\n\\end{aligned}\n\\begin{array}{c|c|c|c|}\n& \\text{(Test A) Y = 0} & \\text{(Test A) Y = 1} \\\\\n\\hline\n\\text{(Test B) Y = 0} & a & b & n_{D}^-(B)\\\\\n\\text{(Test B) Y = 1} & c & d & n_{D}^+(B)\\\\\n\\hline\n& n_{D}^-(A) & n_{D}^+(A) & n_{D}\n\\end{array}\n\\]\n\n\n\\[\n\\begin{aligned}\n\\textbf{D=0}\n\\end{aligned}\n\\begin{array}{c|c|c|c|}\n& \\text{(Test A) Y = 0} & \\text{(Test A) Y = 1} \\\\\n\\hline\n\\text{(Test B) Y = 0} & e & f & n_{\\overline{D}}^-(B)\\\\\n\\text{(Test B) Y = 1} & g & h & n_{\\overline{D}}^+(B)\\\\\n\\hline\n& n_{\\overline{D}}^-(A) & n_{\\overline{D}}^+(A) & n{\\overline{D}}\n\\end{array}\n\\]\n\n\n\nEstimación de medias de comparación en datos no paradeados\nSe observa la situacion real de cada conjunto de personas (grupo con la caracteristica y grupo sin la caracteristica; grupo con un test y grupo con otro test, etc.). Se contabiliza y realizan estimaciones de manera independiente.\nCohortes\n\nCaso-control\nNo se puede estimar las medidas comparativas para los valores predictivos puesto que involucran a la prevalencia.\n\nResultados para muestras grandes\n\n\n\n\n\n\nEjemplo 2.4.\nLos siguientes datos corresponden a dos pruebas diagnósticas aplicadas a mujeres embarazadas en el primer trimestre de embarazo para diagnosticar anormalidades cromosómicas fetales. Estas pruebas son CVS (Chorionic villus sampling) y EA (Early amniocentesis). Las mujeres fueron aleatorizadas para recibir CVS o EA. Los datos obtenidos se resumen como:\n\nlibrary(dplyr)\n\ndf_1 &lt;- data.frame(col1 = c(6, 13),\n                 col2 = c(116, 111)\n)\ncolnames(df_1) &lt;- c(\"Y=0\", \"Y=1\")\nrownames(df_1) &lt;- c(\"EA\",\"CVS\")\n\ndf_1_extended &lt;- rbind(df_1, \"Total\" = colSums(df_1))\ndf_1_extended &lt;- df_1_extended |&gt; mutate(\"Total\" = rowSums(df_1_extended))\ndf_1_extended\n##       Y=0 Y=1 Total\n## EA      6 116   122\n## CVS    13 111   124\n## Total  19 227   246\n\n\nlibrary(dplyr)\n\ndf_0 &lt;- data.frame(col1 = c(4844, 4765),\n                 col2 = c(34, 111)\n)\ncolnames(df_0) &lt;- c(\"Y=0\", \"Y=1\")\nrownames(df_0) &lt;- c(\"EA\",\"CVS\")\n\ndf_0_extended &lt;- rbind(df_0, \"Total\" = colSums(df_0))\ndf_0_extended &lt;- df_0_extended |&gt; mutate(\"Total\" = rowSums(df_0_extended))\ndf_0_extended\n##        Y=0 Y=1 Total\n## EA    4844  34  4878\n## CVS   4765 111  4876\n## Total 9609 145  9754\n\nprueba EA: 122 + 4.878 = 5.000 prueba CVS: 124 + 4.876 = 5.000\ngente con problemas: 122 + 124 = 246 gente sin problemas: las otras = 5.000 + 5.000 - 246 = 9.754\nAlgo ha pasado que ha dicho nuestro gozo en un pozo\n\nSe_EA = 116 / 122\nSe_CVS = 111 / 124\n# rSe_est_EA_sobre_CVS\n(rSE_est = Se_EA / Se_CVS)\n## [1] 1.062177\n\n\nUno_menos_Sp_EA = 1 - 4844 / 4878\nUno_menos_Sp_CVS = 1 - 4765 / 4876\n# rSe_est_EA_sobre_CVS\n(rUno_menos_Sp_est = Uno_menos_Sp_EA / Uno_menos_Sp_CVS)\n## [1] 0.3061807\n\n\n(IC_log_rSe_est_EA_CVS = c( log(rSE_est) - 1.96*sqrt(0.00137), log(rSE_est) + 1.96*sqrt(0.00137) ))\n## [1] -0.01222597  0.13286699\n\n\n(IC_rSe_est_EA_CVS =  exp(IC_log_rSe_est_EA_CVS))\n## [1] 0.9878485 1.1420981\n\n\n(IC_log_rUno_menos_Sp_est_EA_CVS = c( log(rUno_menos_Sp_est) - 1.96*sqrt(0.038), log(rUno_menos_Sp_est) + 1.96*sqrt(0.038) ))\n## [1] -1.5656541 -0.8015054\n\n\n(IC_rUno_menos_Sp_est_EA_CVS =  exp(IC_log_rUno_menos_Sp_est_EA_CVS))\n## [1] 0.2089513 0.4486530\n\n\n(RC_log_rSe_est_log_rUno_menos_Sp_est_EA_CVS = c(IC_log_rSe_est_EA_CVS, IC_log_rUno_menos_Sp_est_EA_CVS))\n## [1] -0.01222597  0.13286699 -1.56565410 -0.80150543\n\n\n(RC_rSe_estrUno_menos_Sp_est_EA_CVS = exp(c(IC_log_rSe_est_EA_CVS, IC_log_rUno_menos_Sp_est_EA_CVS)))\n## [1] 0.9878485 1.1420981 0.2089513 0.4486530\n\nEn el 95 % de los casos con una muestra de igual tamaño y mismo comportamiento encontraríamos razón de sensibilidades de 0.98 a 1.14 por lo que no se puede asumir que ninguna prueba sea mejor que otra ya que el 1 está contenido en el intervalo de confianza.\nCon estas razones no podríamos decir que una prueba sea mejor que otra, no se cumple la condición i y consecuentemente la ii tampoco. Hay que mirar la iii.\nAl hacer un IC para ambas medidas conjuntamente debo subir levemente la confianza y construir una region como producto de los intervalos. Esta región de confianza no nos conviene ya que es aún más ancha debido a que hemos subido esa confianza.\nRespecto a la sensibilidad no tengo diferencias de comprtamiento entre ambas pruebas al 95 %. En una confianza del 90 % el intervalo será más pequeño y nos estaremos arriesgando más. (Calcularlo el 90 % a ver si sirve o no, tal vez podamos dar una respuesta clara con una confianza más pequeña)\n95 % en esa región vs 95 % en una prueba y 95 % en otra prueba.\nEstimación de medias de comparación en datos paradeados\n\n\n\n\n\n\nEficiencia de diseños pareados frente no pareados\n— nuevo día, 21 / 11 / 2024\ndiapo44\nsuponemos en que el mismo número de individuos en a que en b\nel doble en el diseño no pareado que en el parado (que es lo normal)\npara compara la precision de las estiamciones, la única meddad que tengo es el cociente entre las probablidades de clasificacion (la Se y la Sp)\nal comparar la var en no pareado y en pareado tenemos una cantidad.\nla cual cuenta con las probabilidade de los indivios de estar enfermo condicionado a D=1 (que los dos esten enfermos en ambas pruebas) vs tener eso mismor pero por separado. comparo una cierta condicion de independencia pq es como una dist conjunta vs as dos distribuciones independientes.\nno comparo las pruebas, sino que tengo dos pruebas para comparra y tengo un diseño para compararlas pareado o no parado. quiero elegir un diseño que me introduzca la menor variabilidad posible para poder compararlas mejor. teoricame\nprob de tenre un resultado positivo en la misma prueba vs ver por separado esa orobabilidad y como son independientes las multiplico.\nsi son condicionalmente indeendientes me da igual un pareado que un no parado para compararlas (si puede aplicar las ods pruebas a un mismo individuo, claro). no estoy juzgando qué tipo de prueba clasifica mejor.\nriesgo relativo de tener un resultado positivo en una prueba cuando ya se tiene un resultado positivo en la otra prueba. si es mayor a uno decimos que el pareado tiene mayor varianza y nos da resulados menos precisos que un resultado paraeado.\n(es la varianza de un caso asintótico)\naunque n no grande sigue siendo valido cuando al menos tenemos cinco obs en los falsos negativos y cinco en los falsos positivos (que conste que ha dicho “cinco falsos positivos Y cinco falsos negativos”, que luego me acusa de ser literal y decirlo mal), y podemos aplicar metodología asintótica basasdo en resultados recientes.\nEjemplo 2.2.\nen enfermos y no enfermos.\nestimacion sensibilidad en cada prueba: en pareados: tengo 1.023 enfermos. (voy a enfermos y ver que tienen resultados positivos en una y en otra), de 1023 hay 815 enfermos\nen no pareado: tengo 2.046 enfermos. sensibilidad de la prueba de esfuerzo: 815/1023.\npositivos en la prueba de esfuerzo: 815 positivos con el dolor de pecho:\nDISEÑO PAREADO r(Se(CPH,EST)) = 969 / 815 IC_r(Se^) = (1.151, 1.228) DISEÑO NO PAREADO r(Se(CPH,EST)) = 969 / 815 IC_r(Se^) = (1.149, 1.30)\nesa mediada de asociacion es pracitcamente 1, tenemos dos pd que son condicionalmente independientes\nA_pareado = \\(\\hat{A}_{D}\\)\nA_no_pareado = \\(\\hat{A}_\\overline{D}\\)\nA_no_pareado(CPH,EST) = 1.018 =&gt; usar una misma muestra para realaizar las dos pruebas o dos pruebas independeiente que realice cada una una de las pruebas, es a fectos de estimacion y de variabildiad la misma. no hay ventaja en buscar un únio grupo que realice ambas pruebas.\n\\(\\hat{A}_{D}\\) = 1.018 = freq de individuos sanos con dos pruebas positivas vs individuos sanos con priemra positiva * individuos sanos con segunda positiva\n\\(\\hat{A}_\\overline{D}\\) = 1.018\nsi hubiera salido 3.25, la rpeba ch detecta 3.25 veces mas enfermos que la prueba de enfermos\nr(Se(EST, CPH)) = 815/969 = 0.84. la primera detecta un 20 % más de qué, de enfermos.\nsi es el cociente entre las especifidades (esto conviene)"
  },
  {
    "objectID": "tema_02/tema_2_28_11_2024.html#estimación-no-paramétrica-de-curvas-roc",
    "href": "tema_02/tema_2_28_11_2024.html#estimación-no-paramétrica-de-curvas-roc",
    "title": "tema_2_28_11_2024",
    "section": "Estimación no paramétrica de curvas ROC",
    "text": "Estimación no paramétrica de curvas ROC\nSe trata de casos donde solo tenemos información muestral: una muestra de resultados para los casos sanos y otra muestra de resultados para los casos enfermos.\nMétodoempírico\nEs el método no paramétrico más usado tradicionalmente para pruebas diagnósticas continuas y se basa en las distribuciones empíricas de las muestras de sanos y enfermos.\nPara cada una de las muestras miro la proporción de casos que se encuentran por encima del punto de corte.\nEn casos donde la variable respuesta es un biomarcador con unidades de medida, muchas veces la máquina correspondiente da pocos decimales o incluso ninguno. En esta situación estamos chocando con la hipótesis de que es imposible que dos individuos tengan el mismo valor para una variable continua. (esto se refleja en la curva ROC y ya veremos cómo impacta)\nSea \\(\\{Y_{D_i}, i = 1, \\dots, n_D\\} \\; \\text{m.a.s.} \\; Y_D \\; m.a.s Y_D\\) y sea \\(\\{Y_\\overline{D_i}, i = 1, \\dots, n_\\overline{D_i}\\} \\; \\text{m.a.s.} \\; Y_\\overline{D}\\).\nPara cada punto de corte c estimamos: \\[\\hat{\\text{Se}}(c) = \\frac{1}{n_D} \\sum_{i=1}^{n_D} I\\{Y_{D_i} \\geq c\\} = \\frac{\\# I\\{Y_{D_i} \\geq c\\}}{n_D}\\]\n\\[1 - \\hat{\\text{Sp}}(c) = \\frac{1}{\\bar{n}_D} \\sum_{i=1}^{\\bar{n}_D} I\\{Y_{\\bar{D}_i} \\geq c\\} = \\frac{\\# I\\{Y_{\\bar{D}_i} \\geq c\\}}{\\bar{n}_D}\\]\nEjemplo. (análogo al 2.6. así que ese no le incluyo.\nSupongamos que queremos estimar con el método empiricio la curva ROC asociada a un biomarcador y que se ha observado la siguiente muestra de resultados del biomarcador en sanos y enfermos.\n\\(Y_{\\overline{D}} = (1, 1, 2, 4, 4)\\)\\(Y_{D} = (2, 3, 4, 5, 7)\\)\n\nY_overD &lt;- c(1, 1, 2, 4, 4)\nY_D &lt;- c(2, 3, 4, 6, 7)\n\nLos valores observados se encuentran entre 2 y 7. Asigno c € (0.5, 7.5).\n\\(1 - \\hat{Sp}(7.5) = \\text{\"proporcion de individuos sanos que tienen valores por encima de c = 7.5\"} = 0\\)\n\\(1 - \\hat{Sp}(7.5) = \\text{\"proporcion de individuos enfermos que tienen valores por encima de c = 7.5\"} = 0\\)\nSea \\(c=6\\) el umbral de positivos,\n\n# sensibilidad\nsum(Y_D &gt;= 6) / length(Y_D)\n## [1] 0.4\n# especificidad\nsum(Y_overD &lt; 6) / length(Y_overD)\n## [1] 1\n\n\nlibrary(ggplot2)\n\ndf &lt;- data.frame(c=double(),\n                 `1-Sp`=double(), \n                 Se=double()) \nfor (c in seq(7.5, 0.5, by=-0.5)){\n  se &lt;- sum(Y_D &gt; c) / length(Y_D)\n  sp &lt;- 1 - (sum(Y_overD &lt; c) / length(Y_overD))\n  df &lt;- rbind(df,c(c,sp,se))\n}\ncolnames(df) &lt;- c('c', '1-Sp(c)^', 'Se(c)^')\n\nggplot(df, aes(x = `1-Sp(c)^`, y = `Se(c)^`)) +\n  geom_point(color = \"red\", size = 2) +\n  geom_line(color = \"red\", size = 1, alpha = 0.2) +  # Agrega líneas para unir los puntos\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray\", size = 1) +\n  coord_fixed(ratio = 1) +\n  scale_x_continuous(limits = c(0, 1)) +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(\n    title = \"ESTIMACIÓN DE LA CURVA ROC\",\n    x = \"1 - Especificidad (Tasa de Falsos Positivos)\",\n    y = \"Sensibilidad (Tasa de Verdaderos Positivos)\"\n  ) +\n  theme_minimal()\n\n\n\n\nOrdenada la muestra de mayor a menor, tenemos saltos horizontales de longitud proporcional a la inversa del tamaño muestral de sanos y saltos verticales de longitud proporcional a la inversa del tamaño muestral de enfermos.\n¿Dónde la distancia vertical es máxima? En (0, 2/5) \\(\\rightarrow\\) Esto es la estiamcion del índice de Youden.\n¿a qué corresponde esa poligonal? a los empates. ¿eing?\nSi miras la teoria que hay detras de todo esto pues ves que esas cosas dependen de los empates.\nel AUC estimado es 0.78, sacado a mano sumando el área de los polígonos del dibujo.\n¿Los valores observados están en algún sitio en la curva? Nop, la curva no representa los valores (si en lugar de números hubieran sido letras ordenadas alfabéticamente hubiera sido el mismo resultado). Solo es necesario saber el orden de los valores y saber si los valores se repiten. Ni los valores ni el punto del corte a partir del cual decimos que está enfermo se ven reflejados en la curva ROC.\nLa curva ROC nos refleja a nivel global cómo se comporta el biomarcador pero no indica nada sobre si discrimina bien o no. Para ver si discrimina bien o no se tiene que mirar el AUC. (cuanto más cercano a 1 mejor sirve para discriminar)\n\\[\n\\widehat{\\text{ROC}}(\\cdot) = \\left\\{ (1 - \\widehat{\\text{Sp}}(c), \\widehat{\\text{Se}}(c)), \\, \\forall c \\in (-\\infty, +\\infty) \\right\\} = \\left\\{ (t, \\widehat{\\text{ROC}}(t)), \\, t \\in (0,1) \\right\\},\n\\]\n\\[\n\\widehat{\\text{ROC}}(t) = \\widehat{S}_D \\left( \\widehat{S}_{\\bar{D}}^{-1}(t) \\right),\n\\]\nDonde \\(\\widehat{S}_D\\) y \\(\\widehat{S}_{\\bar{D}}\\) son las funciones de supervivencia empíricas.\nLa curva ROC empírica es una función escalonada creciente que se aproxima a la curva ROC teórica.\nÍndices o medidas resumen.\n\\(\\widehat{\\text{AUC}} = \\int_0^1 \\widehat{\\text{ROC}}(t) \\, dt = \\frac{1}{n_D \\bar{n}_D} \\sum_{j=1}^{\\bar{n}_D} \\sum_{i=1}^{n_D} \\left\\{ I[Y_{D_i} &gt; Y_{\\bar{D}_j}] + \\frac{1}{2} I[Y_{D_i} = Y_{\\bar{D}_j}] \\right\\}\\)\n\\(\\widehat{pAUC}(t_0) = \\int_0^{t_0} \\widehat{\\text{ROC}}(t) \\, dt\\)\n\\(\\widehat{YI} = \\max \\left| \\widehat{ROC}(t) - t \\right| = \\max \\left| \\widehat{Se} + \\widehat{\\text{Sp}} - 1 \\right|\\)\nEjemplo.\n\nset.seed(602)\nn &lt;- 100\nYsan3 &lt;- round(rgamma(n, 10, 3), digits=3)\nYenf3 &lt;- round(rgamma(n, 12, 3), digits=3)\nroc3 &lt;- pROC::roc(controls = Ysan3, cases = Yenf3, partial.auc = c(0,0.2))\nroc3 # pos se supone que debería dar 0.686\n\n\nCall:\nroc.default(controls = Ysan3, cases = Yenf3, partial.auc = c(0,     0.2))\n\nData: 100 controls &lt; 100 cases.\nPartial area under the curve (specificity 0.2-0): 0.197\n\n\nCambiando la semilla, AKA cambiando la muestra, las estimaciones de AUC cambian. Aumentando el tamaño muestral obvio que también.\nEjemplo 2.7.\nSe disponen de datos sobre las intensidades relativas de la expresión de un determinado gen para una muestra de 23 tejidos de ovarios sanos y para 30 de tejidos con tumor .\n\nlibrary (pROC)\ncancer &lt;- read.csv(\"~/Master_Bioestadistica/Simulacion/tema_02/EjemploROCcancer.csv\", header=TRUE, sep=\";\")\n\nroc1 &lt;- roc(cancer$var2,cancer$var1,plot=TRUE)\n\n\n\nstr(roc1)\n\nList of 15\n $ percent           : logi FALSE\n $ sensitivities     : num [1:46] 1 1 1 1 0.967 ...\n $ specificities     : num [1:46] 0 0.0435 0.087 0.1304 0.1304 ...\n $ thresholds        : num [1:46] -Inf 0.471 0.505 0.526 0.555 ...\n $ direction         : chr \"&lt;\"\n $ cases             : num [1:30] 0.543 0.571 0.602 0.609 0.628 0.641 0.666 0.694 0.769 0.8 ...\n $ controls          : num [1:23] 0.442 0.5 0.51 0.568 0.571 0.574 0.588 0.595 0.595 0.595 ...\n $ fun.sesp          :function (thresholds, controls, cases, direction)  \n $ auc               : 'auc' num 0.809\n  ..- attr(*, \"partial.auc\")= logi FALSE\n  ..- attr(*, \"percent\")= logi FALSE\n  ..- attr(*, \"roc\")=List of 15\n  .. ..$ percent           : logi FALSE\n  .. ..$ sensitivities     : num [1:46] 1 1 1 1 0.967 ...\n  .. ..$ specificities     : num [1:46] 0 0.0435 0.087 0.1304 0.1304 ...\n  .. ..$ thresholds        : num [1:46] -Inf 0.471 0.505 0.526 0.555 ...\n  .. ..$ direction         : chr \"&lt;\"\n  .. ..$ cases             : num [1:30] 0.543 0.571 0.602 0.609 0.628 0.641 0.666 0.694 0.769 0.8 ...\n  .. ..$ controls          : num [1:23] 0.442 0.5 0.51 0.568 0.571 0.574 0.588 0.595 0.595 0.595 ...\n  .. ..$ fun.sesp          :function (thresholds, controls, cases, direction)  \n  .. ..$ auc               : 'auc' num 0.809\n  .. .. ..- attr(*, \"partial.auc\")= logi FALSE\n  .. .. ..- attr(*, \"percent\")= logi FALSE\n  .. .. ..- attr(*, \"roc\")=List of 8\n  .. .. .. ..$ percent      : logi FALSE\n  .. .. .. ..$ sensitivities: num [1:46] 1 1 1 1 0.967 ...\n  .. .. .. ..$ specificities: num [1:46] 0 0.0435 0.087 0.1304 0.1304 ...\n  .. .. .. ..$ thresholds   : num [1:46] -Inf 0.471 0.505 0.526 0.555 ...\n  .. .. .. ..$ direction    : chr \"&lt;\"\n  .. .. .. ..$ cases        : num [1:30] 0.543 0.571 0.602 0.609 0.628 0.641 0.666 0.694 0.769 0.8 ...\n  .. .. .. ..$ controls     : num [1:23] 0.442 0.5 0.51 0.568 0.571 0.574 0.588 0.595 0.595 0.595 ...\n  .. .. .. ..$ fun.sesp     :function (thresholds, controls, cases, direction)  \n  .. .. .. ..- attr(*, \"class\")= chr \"roc\"\n  .. ..$ call              : language roc.default(response = cancer$var2, predictor = cancer$var1, plot = TRUE)\n  .. ..$ original.predictor: num [1:53] 0.442 0.5 0.51 0.568 0.571 0.574 0.588 0.595 0.595 0.595 ...\n  .. ..$ original.response : int [1:53] 0 0 0 0 0 0 0 0 0 0 ...\n  .. ..$ predictor         : num [1:53] 0.442 0.5 0.51 0.568 0.571 0.574 0.588 0.595 0.595 0.595 ...\n  .. ..$ response          : int [1:53] 0 0 0 0 0 0 0 0 0 0 ...\n  .. ..$ levels            : chr [1:2] \"0\" \"1\"\n  .. ..- attr(*, \"class\")= chr \"roc\"\n $ call              : language roc.default(response = cancer$var2, predictor = cancer$var1, plot = TRUE)\n $ original.predictor: num [1:53] 0.442 0.5 0.51 0.568 0.571 0.574 0.588 0.595 0.595 0.595 ...\n $ original.response : int [1:53] 0 0 0 0 0 0 0 0 0 0 ...\n $ predictor         : num [1:53] 0.442 0.5 0.51 0.568 0.571 0.574 0.588 0.595 0.595 0.595 ...\n $ response          : int [1:53] 0 0 0 0 0 0 0 0 0 0 ...\n $ levels            : chr [1:2] \"0\" \"1\"\n - attr(*, \"class\")= chr \"roc\"\n\nYouden &lt;- roc1$sensitivities+roc1$specificities-1\nYouden &lt;- abs(Youden)\nsummary(Youden)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.1431  0.3297  0.2977  0.4453  0.5696 \n\nYI &lt;- max(Youden)\n\nplot.roc(roc1,print.auc = TRUE, legacy.axes =TRUE)\n\n\n\n\nEjemplo 2.8.\n¿Es el porcentaje de protombina un buen marcador de cirrosis? Los datos disponibles corresponden a una muestra de 145 individuos sanos y 39 cirróticos.\n\nlibrary (pROC)\ncirroticos &lt;- read.csv(\"~/Master_Bioestadistica/Simulacion/tema_02/Protom_SanCirr.csv\", header=TRUE, sep=\";\")\n\nRepresentamos histogramas y distribuciones de densidad de los datos muestrales para hacernos una idea de si vamos a tener un biomarcador que nos ayude a discriminar o no.\n\ndensidad_cirroticos &lt;- density(cirroticos$X.protombina[cirroticos$cirroticos.sanos == 1])\ndensidad_sanos &lt;- density(cirroticos$X.protombina[cirroticos$cirroticos.sanos == 0])\n\nplot(densidad_cirroticos, \n     main = \"Distribuciones de densidad de X.protombina\", \n     xlab = \"X.protombina\", \n     ylab = \"Densidad\", \n     col = \"blue\", \n     lwd = 2, \n     ylim = range(0, densidad_cirroticos$y, densidad_sanos$y))\n\nlines(densidad_sanos, col = \"red\", lwd = 2)\n\nlegend(\"topright\", legend = c(\"Cirróticos\", \"Sanos\"), \n       col = c(\"blue\", \"red\"), lwd = 2)\n\n\n\n#Ref: Estimación no paramétrica de la función de densidad. Antonio Miñarro 1998\n\nAparentemente los sanos tienen valores superiores a los cirróticos y esto es al revés que muchos default. Intercambaimos los valores de casos y controles.\nLos “casos” siempre son los que tienen mayor valor, en este caso los “casos” son los sanos.\nMe parecía poco intuitivo ese valor del área bajo la curva (ROC) al observar cómo se solapan las distribuciones. Sin embargo, es cierto que lo que realmente se solapan son los rangos en los que se distribuyen los datos. Por debajo del punto donde las distribuciones se cruzan, la densidad de los datos en el grupo de los sanos es muy baja.\nSuavizado tipo núcleo\nHa dicho algo de nucleos y suavizado.\nLa curva ROC empírica es una función escalonada a pesar de que la curva ROC es continua. Además, parece razonable construir estimadores que también sean continuos. Estos se pueden obtener de forma no paramétrica mediante técnicas de suavización, por ejemplo de tipo núcleo:\n\\[\nF_{D, h_D}(y) = \\frac{1}{n_D} \\sum_{i=1}^{n_D} K \\left( \\frac{y - y_{D_i}}{h_D} \\right)\n\\]\n\\[\nF_{\\bar{D}, h_{\\bar{D}}}(y) = \\frac{1}{n_{\\bar{D}}} \\sum_{i=1}^{n_{\\bar{D}}} K \\left( \\frac{y - y_{\\bar{D}_i}}{h_{\\bar{D}}} \\right)\n\\]\ndonde \\(F_D\\) y \\(F_{\\bar{D}}\\) son las funciones de distribución de enfermos y sanos, y \\(K(t) = \\int_{-\\infty}^t k(u) \\, du\\) es la función kernel.\n\\[\n\\widehat{\\text{ROC}}(t) = 1 - F_{D, h_D} \\left( F_{\\bar{D}, h_{\\bar{D}}}^{-1} (1 - t) \\right), \\quad t \\in (0, 1).\n\\]\nEn R nos deja elegir el nucleo pero no la ventana. la ventana tiene aplicacion en diagnostico por imagen\nExisten varios métodos para el suavizado.\n\nBinormal. Se utiliza una aproximación normal para la densidad de los datos. La curva ROC se estima bajo la suposición de que los datos provienen de distribuciones normales (binormalidad), con medias y varianzas estimadas a partir de la muestra.\nDensidad. Se estima la densidad de los datos utilizando un núcleo considerado adecuado. Aunque se ha mencionado que es “perfecto”, no queda claro a qué se refiere exactamente dicha afirmación.\nBest-Fit Distribution. Es la distribución que mejor se ajusta a los datos, determinada mediante el método de máxima verosimilitud.\nlognormal: …\n\n\nlibrary(readr)\ndatos &lt;- read_delim(\"Protom_SanCirr.csv\", delim = \";\", escape_double = FALSE, trim_ws = TRUE)\n\nroc1 &lt;- roc(datos$`cirroticos/sanos`,datos$`%protombina`)\n str(roc1)\n\nList of 15\n $ percent           : logi FALSE\n $ sensitivities     : num [1:48] 1 1 0.974 0.949 0.949 ...\n $ specificities     : num [1:48] 0 0.421 0.441 0.455 0.476 ...\n $ thresholds        : num [1:48] Inf 99 97 95.5 94.5 ...\n $ direction         : chr \"&gt;\"\n $ cases             : num [1:39] 36 71 50 50 45 42 37 40 42 52 ...\n $ controls          : num [1:145] 100 73 73 80 57 90 80 75 78 100 ...\n $ fun.sesp          :function (thresholds, controls, cases, direction)  \n $ auc               : 'auc' num 0.897\n  ..- attr(*, \"partial.auc\")= logi FALSE\n  ..- attr(*, \"percent\")= logi FALSE\n  ..- attr(*, \"roc\")=List of 15\n  .. ..$ percent           : logi FALSE\n  .. ..$ sensitivities     : num [1:48] 1 1 0.974 0.949 0.949 ...\n  .. ..$ specificities     : num [1:48] 0 0.421 0.441 0.455 0.476 ...\n  .. ..$ thresholds        : num [1:48] Inf 99 97 95.5 94.5 ...\n  .. ..$ direction         : chr \"&gt;\"\n  .. ..$ cases             : num [1:39] 36 71 50 50 45 42 37 40 42 52 ...\n  .. ..$ controls          : num [1:145] 100 73 73 80 57 90 80 75 78 100 ...\n  .. ..$ fun.sesp          :function (thresholds, controls, cases, direction)  \n  .. ..$ auc               : 'auc' num 0.897\n  .. .. ..- attr(*, \"partial.auc\")= logi FALSE\n  .. .. ..- attr(*, \"percent\")= logi FALSE\n  .. .. ..- attr(*, \"roc\")=List of 8\n  .. .. .. ..$ percent      : logi FALSE\n  .. .. .. ..$ sensitivities: num [1:48] 1 1 0.974 0.949 0.949 ...\n  .. .. .. ..$ specificities: num [1:48] 0 0.421 0.441 0.455 0.476 ...\n  .. .. .. ..$ thresholds   : num [1:48] Inf 99 97 95.5 94.5 ...\n  .. .. .. ..$ direction    : chr \"&gt;\"\n  .. .. .. ..$ cases        : num [1:39] 36 71 50 50 45 42 37 40 42 52 ...\n  .. .. .. ..$ controls     : num [1:145] 100 73 73 80 57 90 80 75 78 100 ...\n  .. .. .. ..$ fun.sesp     :function (thresholds, controls, cases, direction)  \n  .. .. .. ..- attr(*, \"class\")= chr \"roc\"\n  .. ..$ call              : language roc.default(response = datos$`cirroticos/sanos`, predictor = datos$`%protombina`)\n  .. ..$ original.predictor: num [1:184] 100 73 73 80 57 90 80 75 78 100 ...\n  .. ..$ original.response : num [1:184] 0 0 0 0 0 0 0 0 0 0 ...\n  .. ..$ predictor         : num [1:184] 100 73 73 80 57 90 80 75 78 100 ...\n  .. ..$ response          : num [1:184] 0 0 0 0 0 0 0 0 0 0 ...\n  .. ..$ levels            : chr [1:2] \"0\" \"1\"\n  .. ..- attr(*, \"class\")= chr \"roc\"\n $ call              : language roc.default(response = datos$`cirroticos/sanos`, predictor = datos$`%protombina`)\n $ original.predictor: num [1:184] 100 73 73 80 57 90 80 75 78 100 ...\n $ original.response : num [1:184] 0 0 0 0 0 0 0 0 0 0 ...\n $ predictor         : num [1:184] 100 73 73 80 57 90 80 75 78 100 ...\n $ response          : num [1:184] 0 0 0 0 0 0 0 0 0 0 ...\n $ levels            : chr [1:2] \"0\" \"1\"\n - attr(*, \"class\")= chr \"roc\"\n\n #Dibujamos\nplot(roc1,legacy.axes =TRUE)\n\nrs &lt;- smooth(roc1, method=\"binormal\")\nplot(rs, add=TRUE, col=\"green\")\n\nrs2 &lt;- smooth(roc1, method= \"density\")\nplot(rs2, add=TRUE, col=\"blue\")\n\nrs3 &lt;- smooth(roc1, method=\"fitdistr\", density=\"lognormal\")\nplot(rs3, add=TRUE, col=\"red\")\n\nlegend(\"bottomright\", legend=c(\"Empírica\",\"Binormal\",\"Densidad\",\"Lognormal\"), col=c(\"black\",\"green\",\"blue\",\"red\"),lwd=2)\n\n\n\n\nEl AUC es el mismo porque se estima utilizando la distribución empírica. La versión normalizada se emplea únicamente para representar gráficamente la curva ROC, ya que proporciona una visualización más intuitiva. Lo que hace es trazar la curva de manera más “estética”, generando una forma más continua y suave, en contraste con la versión empírica, que tiende a ser más poligonal y escalonada."
  },
  {
    "objectID": "tema_02/tema_2_28_11_2024.html#intervalos-y-bandas-de-confianza",
    "href": "tema_02/tema_2_28_11_2024.html#intervalos-y-bandas-de-confianza",
    "title": "tema_2_28_11_2024",
    "section": "Intervalos y bandas de confianza",
    "text": "Intervalos y bandas de confianza\nCuando la muestra es pequeña, solo es posible “repetir el experimento” utilizando el método bootstrap, generando nuevas muestras (tengo muestas de sanos y de enfermos) de igual tamaño que las originales, respetando la proporción entre los grupos de sanos y enfermos.\nFijado, se puede calcular la sensibilidad y la especificidad correspondientes para cada muestra bootstrap, lo que permite estimar una región de confianza para estos valores.\nSi se mantiene fija una de las dimensiones (tanto la sensibilidad como la especificidad), es posible observar cómo varía la otra dimensión (en este caso, la especificidad) y determinar el rango en el que se encuentra, es decir, “fijada la sensibilidad, ver de qué punto a qué punto varía la especificidad”.\n[aquí ha hecho un dibujito muy boniko que a ver si lo replico.]\nLa diferencia entre las bandas de confianza obtenidas mediante el método bootstrap y las obtenidas a través de la aproximación asintótica es muy pequeña.\nLa variabilidad muestral para la curva ROC empírica puede obtenerse desde varios puntos de vista: dipo76.\nComparación de curvas ROC\nlleva emparejado dos biomarcadores en dos muestras pareaadas o no pareadas, dependiendo del tipo de muestras usaremos un tipo de comparación y otra.\nEl estimador que se suele usar es la diferencia de las AUC, normalizado para tener una distribucion asintotica.\nSean dos pruebas diagnósticas A, B =&gt; H0: ROC_A = ROC_B\nSi las AUC son diferentes automáticamente tenemos curvas distintas (al revés no)\naparece la supervivencia ya que los sanos\nprob acumulada : densidad prob de colas : prob de supervivencia\ndiapo83\n2a.\nsi miramos las curvas (a parte de los resultados) nos gustaría saber si las consideramos la misma o no\nel p value para contrastar la hipotesis es p-value = 0.03075. si hacemos una comparacoin entre dos curvas podemos decir que no son la misma. si usamos DeLongs, el estadistico es diferente con otra dist asintotica pero decimos lo mismo. Y con Venkatraman pues eso.\nbootstrap aqui es inutil pq es mecanico y tal… no sé, que computacionalmente no tiene sentido, ¿creo?\npara esta especi de 0.9, ¿las dos pruebas me dan sensibilidades que son distintas?\ncaramba, pvalor 0.2, con lo cual las dos pruebas (partiendo de una especi de 0.9) conducen a la misma sensibilidad. y si aplico bootstrap (para hacerlo al reves, fijando sensi) me dice que no: si fijo sensi de 0.9 no admito que las dos rpeubas tenga la misma especi.\nno tengo evidencia que para una especi de 0.9 las dos sensi sean iguales\nen diapo85, para una especi de 0.9 se contrasta si la sensi_ = sensi_B. con pvalor 0.2 admito la hipóteis. no tnemos evidencias de que la sensi de ambas pruebas conese nivel de especi sea diferente.\nla segunda salida es fijo la sensi a 0.9 y contrasto si la especi de esas dos pruebas coinciden o no. con pvalor infimo puedo descartar que la especi sea igual.\nesto es un argumento de autoridad (test y un apoyo teorico que me garantiza las cosas) pero visualmente viendo las curvas ROC debemos tener unas primeras impresiones.\n“casualmete” la especi 0.9 es donde se cruzan las curvas ROC, por eso no se pueden diferencias las sensi.\npero cuando voy a mirar la seni 0.9 SÍ que hay diferencias entre as especifi\nElección del punto de corte c\nValor que hace que la pd asigne como positivo o negativo un resultado.\nsegún algún criterio.\na la hora de tener una estimacion del AUC y sobre todo su Ic nos va a dar una pista para decidir qué tipo de prueba tenemos.\nAUC – categoria &lt;0.6 – prueba caca y no merece la pensa hacer nada con ella 0.7-0.8 – buena 0.8-0.9 – buena 0.9 - 1 – excelente\nsi el AUC estimado con sus intervalos del IC es claramente menor a 0.6 la verdad es que podemos obviar esa curva ROC y no hace falta hacer ninguna comparativa con otra (alargar esta frase para resolver todos los casos pero vamos, que está entendido)\ndiapo87\nindice de youden: buscar test que tiene mayor sensi y especi\nen otro es buscar que tenga misma sensi y misma especi\nen el tercer caso queremos maximizar una de ellas\ntodas basadas en tener maxima sensi y maxima especi, dentro de lo que ofrece la prueba. si la sensi no sube de 0.6, por más que intentemos maximizarla no podremos (esto no lo entiendo, porque al final si disminuyes el pc debería poder llegar a 1)\ndiapo89. consideramos positivo si mayor a 37, pero eso no me dice nada sober la prueba. miramos el auc y es 0.74. no puedo descartar esa prueba, ya que con el IC a veces llega a ser buena.\ninterpretacion de AUC. asignando e mayor de los valores a los enfermos aciertarias estimadamente en el 74 % de las veces pudiendo variasr entre un 66 y un 62.\nacierto en una proporcion del 68 %. el ic marca 0.58 qye es muy poco pq 0.5 ya es puro azar y va hasta el 0.77 aunque muy lejos del 1. no pinta bien esta prueba eh\nla descriptiva nos hubiera ayudado mucho.\nantes estábamos mirando todo junto. ahora separamos entre muejres y hombres."
  },
  {
    "objectID": "tema_02/tema_02_3_curvas_roc.html#medidas-de-calidad-para-la-curva-roc",
    "href": "tema_02/tema_02_3_curvas_roc.html#medidas-de-calidad-para-la-curva-roc",
    "title": "3. Validación de biomarcadores. Curvas ROC ✗",
    "section": "Medidas de calidad para la curva ROC:",
    "text": "Medidas de calidad para la curva ROC:\nAUC\nCaso de uso. Eligiendo un par de resultados (uno de la muestra de sanos y otro de la muestra de enfermos) la probabilidad de clasificar correctamente cuando se asigna como sano aquel con menor valor en la prueba y como enfermo aquel que tiene mayor valor en la en la prueba es igual a AUC.\n\\(AUC = p(Y_D &gt; Y_\\overline{D})\\)\n\nTest perfecto: \\(AUC=1\\).\nTest noinformativo: \\(AUC=0.5\\).\nDados dos test A y B tales que \\(ROC_A(t) \\ge ROC_(t), \\forall t \\in [0,1] \\Rightarrow AUC_A \\ge AUC_B\\), pero no se cumple en ambas direcciones.\n\nSi una prueba es más sensible que otra solo para algunos umbrales, diremos que la prueba es preferible solo en algunos casos.\npAUC\nEn casos donde las curvas ROC de dos pruebas diagnósticas se cruzan y las áreas bajo las curvas son similares.\n\nEl test B demuestra un mejor desempeño en el intervalo [0,0.25]. Este desempeño superior se debe a que la sensibilidad y especificidad de dicha prueba superan a las del test A en este rango. Sin embargo, a partir de este punto, la interpretación se invierte: el test A pasa a ser superior tanto en sensibilidad como en especificidad.\nEn general, no existe una prueba que supere consistentemente a la otra en todo el rango evaluado. Es útil considerar el área bajo la curva parcial para realizar comparaciones más específicas y establecer puntos de corte que sean coherentes con el comportamiento de ambas curvas en los distintos intervalos.\nÍndice de Youden\nAlgunas publicaciones lo defines como una distancia y otras lo definen como la mayor distancia.\nDistancia entre mi curva ROC y la diagonal.\n\\(YI(c) = |Se(c) + Sp(c) - 1|\\)\n\nTest perfecto: \\(max YI(c)= 1\\).\nTest noinformativo: \\(YI(c)=0 \\quad \\forall \\quad c\\).\nDados dos test A y B tales que \\(ROC_A(t) \\ge ROC_(t), \\forall t \\in [0,1] \\Rightarrow AUC_A \\ge AUC_B\\), pero no se cumple en ambas direcciones.z\n\n\\(YI(c) = 0 \\; \\forall \\; c\\)ç \\(YI(c) = 0 \\, \\forall \\, c\\)\nDependiendo de cuánto se conozca sobre la prueba diagnóstica a nivel estadístico, la forma de trabajar con la curva ROC será de una manera u otra."
  },
  {
    "objectID": "tema_02/tema_02_3_curvas_roc.html#estimación-de-curvas-roc",
    "href": "tema_02/tema_02_3_curvas_roc.html#estimación-de-curvas-roc",
    "title": "3. Validación de biomarcadores. Curvas ROC ✗",
    "section": "Estimación de curvas ROC",
    "text": "Estimación de curvas ROC\nEn caso de conocer el comportamiento de la prueba diagnóstica (la distribución de los resultados) uso una metodología paramétrica (la cual es idonea si los grupos de sanos y enfermos se distribuyen normalmente o se pueden transformar para tener normalidad).\nEficaz con conjuntos de datos desequilibrados: En los conjuntos de datos en los que una clase domina significativamente, las métricas como la precisión pueden no proporcionar una imagen precisa. El AUC, que incorpora tanto el TPR como el FPR, es una métrica más fiable en estos casos.\nLo más habitual es tener una prueba diagnóstica donde solo tenemos una muestra y no tenemos información de la distribución de la prueba (de los resultados) que podamos usar en el modelo teórico.\nCon el método empírico obtenemos curvas escaloanadas. Visualmente es feo y no es lo esperable, ya que estamos trabajando con una variable numerica. En tales casos aplicamos un suavizado."
  },
  {
    "objectID": "tema_02/tema_02_3_curvas_roc.html#estimación-paramétrica-de-curvas-roc",
    "href": "tema_02/tema_02_3_curvas_roc.html#estimación-paramétrica-de-curvas-roc",
    "title": "3. Validación de biomarcadores. Curvas ROC ✗",
    "section": "Estimación paramétrica de curvas ROC",
    "text": "Estimación paramétrica de curvas ROC\nMétodobinormal\nSea \\(Y_D \\sim N(\\mu_D, \\sigma^2_D)\\) y sea \\(Y_\\overline{D} \\sim N(\\mu_\\overline{D}, \\sigma^2_\\overline{D})\\).\nEntonces,\n\\[\nROC(t) = \\Phi\\left( \\frac{\\mu_D - \\mu_{\\overline{D}}}{\\sigma_D} + \\frac{\\sigma_{\\overline{D}}}{\\sigma_D} \\Phi^{-1}(t) \\right)\n\\]\n\\[\nAUC= \\Phi\\left( \\frac{\\mu_D - \\mu_{\\overline{D}}}{\\sqrt{\\sigma^2_D + \\sigma^2_\\overline{D}}} \\right) : \\Phi(a) = p(Z&lt;a), \\; Z \\sim N(0,1)\n\\]\nEn caso de desconocer la media o varianza poblacionales pasamos a tener una estimación de la curva ROC construida a través de la estimación puntual de la media y la varianza con los datos muestrales.\n\\[\n\\widehat{ROC}(t) = \\Phi\\left( \\frac{\\widehat{\\mu}_D - \\widehat{\\mu}_{\\overline{D}}}{\\widehat{\\sigma}_D} + \\frac{\\widehat{\\sigma}_{\\overline{D}}}{\\widehat{\\sigma}_D} \\Phi^{-1}(t) \\right)\n\\]\n\\[\n\\widehat{AUC} = \\Phi\\left( \\frac{\\widehat{\\mu_D} - \\widehat{\\mu}_{\\overline{D}}}{\\sqrt{\\widehat{\\sigma}^2_D + \\widehat{\\sigma}^2_\\overline{D}}} \\right)\n\\]\nDesconocer la distribucion de la prueba diagnóstica no impide poder hacer estimaciones puntuales para la sensibilidad y la especificidad.\nLa curva ROC es invariante ante transformaciones estrictamente crecientes. Decir que la curva ROC es binormal implica que, mediante alguna transformación \\(h\\) estrictamente creciente, \\(h(Y_D)\\) y \\(h(Y_\\overline{D})\\) asumen distribuciones normales. (test de bondad de ajuste donde no rechazo normalidad)\nAlgunas transformaciones que pueden usarse son las de tipo Box-Cox.\n- \\(X^a : a \\ne 0\\), solo si la prueba diagnóstica puede tener solo resultados positivos.\n- \\(log(X)\\)"
  },
  {
    "objectID": "tema_02/tema_02_3_curvas_roc_final.html#determinación-de-tamaños-muestrales",
    "href": "tema_02/tema_02_3_curvas_roc_final.html#determinación-de-tamaños-muestrales",
    "title": "curvas ROC - segunda parte",
    "section": "4. Determinación de tamaños muestrales",
    "text": "4. Determinación de tamaños muestrales\nantes era fijando la anchura de un IC. tb podríamos hacerlo ahora, pero nos interesa una metodlogia que consiste en hacer un contraste de hipotesis y tener un tamaño muestra para cierta potencia dada.\nen estudios clinicos se pone en h1 la que a uno le gustaría. así cuando rechaces h0 dices “estabas protegida y mira, te he conseguido descartar”, “tengo una significatividad suficientemente grande para rechazarte”\na parte de esa confianza, los h tiene una potencia: prob de rechazar H0 cuando es falsa. queremos que sea máxima.\nObuchowski (1998) Li and Fine (2004) revisan la metodología bajo diferentes supuestos\ncualquier tipo de pd que no sea binaria, lo que hacemos con un pto de corte es hacerla binaria.\ndiapo95\ntamaños muestrales pra casos y orta para controles que garanticen niveles mínimos para la sensi y especi de la pd.\n\\(\\hat{Se} \\ge TPF_0 \\quad \\text{(1 - fraccion de falsos negativos)}\\) \\(\\hat{Sp} \\ge 1 - FPF_0 \\quad \\text{(1 - fraccion de falsos positivos)}\\)\nvalor minimo que estoy dispuesto asumir para la sensi\n“busco unos tamaños muestrales pata que la sensi de la prueba sea eta y la especi de la prueba sea esta”\nsi la especi quiero cmoom minimo este valor, entonces 1 - Sp quiero que como mucho sea 1-ese valor. cambiamos el valor minimo por una cota máxima.\n\\[H1: Se &lt; TFP_0 \\quad \\cup \\quad 1 - Sp &gt; FPF_0\\] \\[H1: Se \\ge TFP_0 \\quad \\cap \\quad 1 - Sp \\le FPF_0\\]\ndiapo95\nRegión crítica: {\\(\\text{RegCrít}_{\\alpha} = \\left\\{ FPF_{U}^{\\alpha^*} &lt; FPF_0, \\, TPF_{L}^{\\alpha^*} &gt; TPF_0 \\right\\}\\)}\nrc cuadrada y queremos una confianza de aplha, tendríamos confianza de alpha^2. de ahí sale el alpha estrella.\ndeciamos que todos los estimadores (no se cuaes) se comportaban como una binomial. vamos a trabajar con esa idea\nnumeor de enfermos y sanos se refieren a las pruebas que vamos a hacer: \\(n_D, n_{\\over{D}}\\)\nel comportamiento de estas variables se aproxima sintoticamente a una distribucion normal\n\npara conseguir casos y controles, fijados niveles minimos de se y sp tengo unos valores inciales\ncompruebo si esos valores iniciales me conducen a una potencia mayor o igual a la que quiero.\n\nuna condicion sobre la potencia de un contraste donde se supone que el valor de beta es un valor fijado por nosotros.\nmediante simulacion reproduzco marcando los valoers minimos de sensi y speci con los n_D y n_overD que he marcado antes.\ncreo dist binomial y cuento cuantas veces tengo una etimacion para esos FPF_apha\ndos valores que son los que tomamos asumiendo que la h0 es falsa. tomo dos valores dentro de la RC: \\((FPF_1, TPF_1) \\rightarrow\\) generamos muestras de tamaños \\(n_D\\) y \\(n_{\\over{D}}\\)\nsuponemos que partimos de un valor de la sensi y la especi y ahora simulamos con ese valor.\nsimulo falsos positivos con una binomial de n=casos y prob= falso positivo\nsi estoy en RC es 1, sino 0.\nasí para un n de muestras grande\npara estimar la potencia cuento cuántas muestras me han llevado a estimaciones dentro de la rc partiendo de una H0 falsa,\nsi esa potencia está por encima del requisito esos tamaños muestrales nos sirven.\nsi no, aumentamos los tamaños muestrales y volvemos a calcular.\nEjemplo 2.11.\nlo que yo quiero es: tener sensi de 0.9 y especi de 0.95\nun valor dentro de la RC. y dos valores que coforman los limites de esa RC.\n\\(H0: Se &lt; 0.75 \\cup 1 - Sp &gt; 0.2\\) \\(H0: Se \\ge 0.75 \\cap 1 - Sp \\le 0.2\\)\n\\(\\hat{Se} \\ge TPF_0 = 0.75 -&gt; TPF_1 = 0.9\\) \\(\\hat{Sp} \\ge 1 - FPF_0 = 0.2 -&gt; FPF_1 = 0.05\\)\nRegión crítica: {\\(\\text{RegCrít}_{\\alpha} = \\left\\{ FPF_{U}^{\\alpha^*} &lt; 0.2, \\, TPF_{L}^{\\alpha^*} &gt; 0.75 \\right\\}\\)}\ndetermino asitntoticamente los valores indiales para n_D y para n_overD\nhacemos al calculo de esa normal asintotica y esos valores, y los tamaños de las meustras son 64 y 46.\ngeneramos muestras de tamañaos 64 para los enfermos y 46 para los sanos.\nhacemos la matriz correspondiente y determinamos las expresiones de la diapo95 (donde sale el dibujito de la RC)\ncalculamos beta:´´voy sumando en cuántas de esas muestras los valores estumados de fL y fU está dentro de la RC\n0.87 – hay que repetir e ir incrementando. al llegar a 50 casos y x controles consigue la potencia\n70 y 50 le daban\nmuestra mas grande con misma proporcion de FP y FN.\nel primer paso es usar la dist asintotica normal para tener tañaos de partida\npuede haber más de una solucion, ya que matematicamente podemos ir aumentando de uno en uno tanto los sanos como los controles, pero tb (si estamos cerca de la potencia deseada, claro*) podemos aumentar de dos en dos sanos y de uno en uno enfermo: a criterio de cada uno, tb dependiendo de si es muy difícil encontrar sanos o enfermos, etc.\n\nsi estamos cerca de la potencia mínima deseada seguramente valga con aumentar pocas personas. si estamos muy lejos seguramente necesitemos mucha más muestra y quedaría raro si aumentamos mucho unos y muy pocos otros."
  },
  {
    "objectID": "tema_03/tema_03.html#simulación-numérica-para-desarrollar-pruebas-diagnósticas.",
    "href": "tema_03/tema_03.html#simulación-numérica-para-desarrollar-pruebas-diagnósticas.",
    "title": "TEMA 3",
    "section": "Simulación numérica para desarrollar pruebas diagnósticas.",
    "text": "Simulación numérica para desarrollar pruebas diagnósticas.\nEste es un tema instrumental. Iremos recurriendo a sus contenidos a medida que avance el desarrollo de la asignatura.\nContenidos:\n\nIntroducción (metodología y otros aspectos básicos)\nSelección aleatoria de pacientes y asignación de tratamientos\nObtención de datos simulados\nValidación de estimaciones: Metodología Bootstrap\nValidación del Índice Kappa\nTécnicas de remuestreo aplicadas a la inferencia de curvas ROC\nDeterminar el número de réplicas de simulación\nControlar la calidad de los datos simulados\nAplicaciones"
  },
  {
    "objectID": "tema_03/tema_03_1_introduccion.html#whats-simular",
    "href": "tema_03/tema_03_1_introduccion.html#whats-simular",
    "title": "1. Introducción ✓",
    "section": "What’s simular?",
    "text": "What’s simular?\nSIMULAR. Representar la realidad con un modelo.\nQueremos es una simulación estocástica \\(\\Rightarrow\\) Obtener diferentes “variedades” de una situacion donde el azar interviene. \\(\\Rightarrow\\) Lo que se conoce como estimacion de Monte Carlo.\n\nUn proceso estocástico es aquel cuyo comportamiento no es determinista, en la medida en que el subsiguiente estado del sistema se determina tanto por las acciones predecibles del proceso como por elementos aleatorios.\nUna simulación de Monte Carlo es un modelo probabilístico que puede incluir un elemento de incertidumbre o aleatoriedad en su predicción. https://aws.amazon.com/es/what-is/monte-carlo-simulation/. Técnica numérica basada en conceptos y resultados probabilísticos que consigue IMITAR un fenómeno (situación o sistema) real. Obvio que no tenemos la certeza de cuál va a ser el fenómeno, hay incertidumbre.\n\n\nLa simulación es la antítesis de los modelos teóricos en el sentido de que la simulación no duda nunca porque ya no tiene probabilidades, tiene datos. La simulación no echa cuentas, da resultados (pa’ echar cuentas ya tengo el modelo teórico).\n\nPara simular un modelo tengo que basarme en un modelo teórico. Si quiero hacer simulaciones de la realidad debo conocer el comportamiento teórico del modelo. E identificar qué partes de ese modelo depende del azar y cuáles no. Esos cambios aleatorios impactados por el azar debo definirla como una variable aleatoria.\nPor ello, no hablamos de muestras malas ni muestras buenas. Habrá simulaciones malas o simulaciones buenas.\nLos números aleatorios que nos da el ordenador en realidad son números pseudoalatorios. Dado un valor inicial se consigue el siguiente número, y a partir del segundo el tercero, etc.\nPlanteamiento de un modelo de simulación:\n\nDesarrollar un modelo que represente la situación real que se quiere investigar.\nIdentificar qué partes o fases de la situación real cambian aleatoriamente. (sexo del bebé)\nDescribir los cambios aleatorios con variables aleatorias. (Sexo: {XX, XY}, p(XX)=0.5)\nGenerar observaciones aleatorias del sistema investigado.\nValidar el modelo simulado comparando los valores simulados con las observaciones reales.\n\nLa simulación tiene dos fases:\n1. Simulación de un valor aleatorio.\n2. Dado valor aleatorio asignar el valor simulado.\nCondiciones de una simulación\n\nQue sea rápida (que la generacion de números aletorios sea rápida)\n\nLos números aleatorios generados se distribuyan entre 0-1\n\nLos números aleatorios generados se repartan por igual entre 0-1\n\nLos números aleatorios parezcan independientes\n\nLos números generados no son independientes, ya que dada una semilla siempre recreo la misma sucesión de números aleatorios. H0: necesitamos que esos números pseudoaletorios parezcan independietes (ya que sabemos que no lo son\nEjemplo\nRealizar una asignación aleatoria del sexo de un bebé. Conocido el sexo vamos a simular el peso de cada bebé.\nCriterio: dado un número aleatorio (entre 0-1) elegir el sexo del bebé. Quiero que ambos elementos tengan la misma probabilidad (divido el intervalo en dos partes iguales)\nLa base de mi modelo teorico:\nx = sexo del bebé       y = peso del bebé (kg)\n\nQuiero generar varios bebés.\n(x1, peso_1)\n(x2, peso 2)\n\nPara cada bebé tengo sexo y peso (muestras paradas pero independencia entre las observaciones).\n\np(X=varón) = 0.5 = p(x=hembra)\npeso | x=varón ~ N(3.266, 0.514)\npeso | x=hembra ~ N(3.155, 0.495)\n\n\nGenero un número aleatorio.\nHago una simulación.\nSaco los datos simulados.\nReciclo semilla.\n\n\n\n\n\nTengo dos variables que simular.\n- Un número aleatorio para simular la primera variable.\n- Otra variable que no conozco pero que está condicionada. Otro número para simular para la otra variable.\n\nn=6; set.seed(20175)\n\nU=runif(n,min=0,max=1)\np=0.5;\npeso=numeric(n);\nsexo=character(n);\n\nfor(i in 1:n) {\n  if (U[i]&lt;p){\n    pp=rnorm(1,3.266,0.514)\n    peso[i]=pp\n    sexo[i]=\"varon\"\n } else {\n   pp=rnorm(1,3.155, 0.495)\n   peso[i]=pp\n   sexo[i]=\"mujer\"\n }\n}\n\n\ncat(\"Números aleatorios:\", U, \"\\n\")\n\nNúmeros aleatorios: 0.7621414 0.3668522 0.3111475 0.6270264 0.07654514 0.7816305 \n\ncat(\"Sexo:\", sexo, \"\\n\")\n\nSexo: mujer varon varon mujer varon mujer \n\ncat(\"Peso:\", peso, \"\\n\")\n\nPeso: 3.913199 3.536088 2.989453 3.101755 2.76329 3.636893 \n\n\n\nn=100000; set.seed(20175)\nU=runif(n,min=0,max=1)\np=0.5;\npeso=numeric(n);\nsexo=character(n);\n\nfor(i in 1:n) {\n  if (U[i]&lt;p){\n    pp=rnorm(1,3.266,0.514)\n    peso[i]=pp\n    sexo[i]=\"varon\"\n } else {\n   pp=rnorm(1,3.155, 0.495)\n   peso[i]=pp\n   sexo[i]=\"mujer\"\n }\n}\n\n# Histograma muestral n=100000\nhist(peso,freq=FALSE)\n\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nComo tengo dos muestras, dos variables, necesito que la creación de la simulación de cada una de las variables sea independiente. Necesito que la semilla de cada variable aleatoria, AKA la semilla de cada variable simulada, sea diferente. Al ser números pseudoaleatorios estaría condicionada la primera y segunda variable simulada.\n(esto no tiene nada que ver con que una variable esté condicionada a la otra)\n\n\n\n\n\n\n\n\nAcerca del ejercicio que nos mandó\n\n\n\nPuedo simular un número aleatorio para un hijo y luego otro para el segundo hijo. O un método para simular con un único número aleatorio los dos hijos a la vez (vamos, crear todas las condiciones con un único número aleatorio).\n\n\nMuy bien mijo, ¿pero y si tengo que simular algunas de las distribuciones no conocidas?\nExisten métodos generales para ello. Si tengo que simular distribución conocida estoy ok. Si tengo que generar una variable desconocida entonces tendré que crearla por distintos métodos.\n\n\n\n\nLa pregunta es, ¿qué nrum le pongo?"
  },
  {
    "objectID": "tema_03/tema_03_2_selección_aleatoria.html",
    "href": "tema_03/tema_03_2_selección_aleatoria.html",
    "title": "2. Selección aleatoria de pacientes y asignación de tratamientos ✗",
    "section": "",
    "text": "Advertencia\n\n\n\nQuedan por incluir unos ejemplos por algún lado del tema 3.\n\n\nObjetivo.\nLlegar a una conclusión que sea válida. Valido = representativo.\nAleatorización.\nExigencia teórica impuesta a experimentos y ensayos clínicos con el objetivo de minimizar la variabilidad de las evaluaciones y evitar la distorsión que pueden producir otros factores en las pruebas experimentales.\nCuántas veces debería hacer la simulación para saber que el resultado es verdaderamente cercano al valor desconocido real. La validez de una estimación está ligada al conportamiento de lo que quiero estimar y con la cantidad de información que tenga.\nHipótesis.\n\nLos pacientes se eligen aleatoriamente. Cualquier grupo de n pacientes tiene las mismas posibilidades de ser elegido.\n\nEl tratamiento es asignado aleatoriamente. No hay preferencias en la asignación, cada paciente tiene las mismas oportunidades de recibir uno de los tratamientos.\n\nDebo de partir con la idea de que todas las personas que son similares.\nSi quiero hacer una comparacion voy a procurar que el tamaño entre las personas que reciben cada uno de los tratamientos es similar.\nEjemplo: aplico varias metodologías de aprendizaje en niños, no puedo tener 10k niños con la metodología antigua y 1 unidad de niños con la metodología nueva, aunque todos sabemos que luego esto nunca se lleva a cabo correctamente. (vamos, la diapo14)\nObjetivos de la aleatorización.\n\nAsegurar que cualquier paciente tiene las mismas oportunidades de recibir el tratamiento experimental.\n\nEliminar sesgos en la selección.\n\nEquilibrar el tamaño de los grupos. (en función del objetivo o de las características experimentales)\n\nVerificar o estudiar la eficacia de los tratamientos.\n\nRazones para la aleatorización.\n\nLos sujetos asignados a cada tratamiento tendrán características similares.\n\nSin similitud =&gt; Sesgo en los resultados.\n\n\n\nNi el investigador, ni el paciente tendrán conocimiento del grupo de asignación en el que se va incluir al participante en el estudio.\n\nSesgo en la selección =&gt; Efectos del tratamiento sobredimensionados.\n\n\n\nLa aleatorización garantiza la validez de los test estadísticos utilizados para comparar tratamientos.\n\nLa aleatorización estratificada y la aleatorización adaptada a las covariables controlan la influencia de las covariables.\nSi quiero ver si un medicamento funciona mejor que otro:\n\nNecestiamos trabajar sobre un modelo matematico que me confirme que los individuos no estén relacionados entre sí (si son familia pues les sentará igual de mal o igual de bien cada uno de los tratamietos) y homogeneidad entre los dos grupos (un grupo más sano que otro)\nSi quiero asginar mi muestra en dos grupos la asignación aleatoria de cada individuo uno por uno va mal si tenemos una muestra reducida (elijo una persona y la asigno aleatoriamente a un grupo, luego con la siguiente). Si n es grande a la larga tendré estabilidad de frecuencias.\n\nAleatorización simple\nEstá basada en una única serie de asignaciones aleatorias. Los pacientes se asignan a los grupos de tratamiento del estudio clínico.\nSe puede imponer un control realizando asignaciones de modo que haya el mismo número de individuos en cada grupo.\nDe un grupo de individuo cojo n de ellos sin que se repitan.\n\nset.seed(178900)\ntrat=sample(1:20,10,replace=FALSE)\nindiv=sort(trat)\nindiv\n\n [1]  2  3  4  6  9 10 11 13 17 18\n\n\nSi tengo un n muy grande participando en el estudio no tengo que preocuparme por la selección.\nVentajas:\n\nEs un procedimiento sencillo y fácil de poner en práctica.\n\nEn estudios de muchos pacientes la aleatorización simple conduce a grupos con un número similar de participantes.\n\nDesventajas:\n\nLos resultados de la aleatorización pueden dar lugar a grupos de tamaños muy desiguales cuando el estudio involucra a un número reducido de pacientes.\n\nProblema:\n\nA veces no tengo todos mis pacientes a mis disposición, por ejemplo, a mitad del estudio este se para y tenemos x individuos sin haberle podido dar el tratamiento (ej: se rompe la máquina que da la dosis y muchos pacientes se quedan sin dosis)\n\nSi el suceso de no poder realizar la asignación a todos los individuos es altamente probable debo tenerlo contemplado. Para ello usamos la aleatorización por bloques.\nPor bloques o restringida\nPermite asignar aleatoriamente sujetos en los grupos de tratamiento de igual tamaño dividiendo a los pacientes potenciales en m bloques de tamaño 2n, n&gt;1.\nEn lugar de colocar las n personas al mogollón, voy equilibrando grupos más pequeños. Así si se interrumpe el experimento tengo un experimento más pequeño pero bien repartido.\nEl procedimiento consiste en repartir toda la muestra en x aleternativas igual que antes pero en vez de tener n/2 y m/2, fijo un tamaño del bloque y dentro de cada bloque jeugo con un reparto equitativo.\nEl procedimiento se basa en la construcción de todos los posibles bloques distintos formados con n asignaciones A (tratamiento) y n asignaciones B (placebo). La elección de cada bloque es aleatoria.\nSelección del tamaño de los bloques.\n\nTamaño n múltiplo del número de tratamientos.\n\nTamaño n no muy grande porque precisamente lo que quiero son bloques no grandes para poder tener la asignación equilibrada.\n\nEl n=6 suele ir bien.\n\nTamaño n dividendo del total de la muestra parece no ser una condición, tal vez el último bloque sea de menor tamaño para asignar a los individuos sin tratamiento.\n\nVentajas:\n\nEl método asegura tener grupos de tamaño equilibrado a lo largo del proceso de asignación, siempre que se use el bloque completo.\n\nDesventajas:\n\nHay que ocultar el tamaño del bloque al clínico para que la asignación no sea predecible.\n\nSi el experimento es doble ciego quiero que el médico no sepa que es asignación por blqoues, ya que si estoy asignando el primer bloque y la mitad de los individuos tienen un tratamiento automáticamente sabe que los n siguientes individuos van a tener el otro tratamiento.\nEjemplo\nAsignar tratamiento o placebo a 60 sujetos utilizando bloques de tamaño 6 (2n=6).\nCreo todos los bloques de asignación posible con todos los órdenes de asignación.\n\n\n\n\nSelecciono con reemplazamiento los bloques necesarios.\n\nset.seed(32581)\nsample(1:20,10,replace=TRUE)\n\n [1]  7  4 15  5 19  3  5  2 12 19\n\n\nEstratificada\nSe utiliza para conseguir un equilibrio entre los grupos respecto a otras características (covariables) de los sujetos. Se complica con la cantidad de covariables que quiera incluir para la creación de grupos.\nControla la posible influencia de las covariables en las conclusiones de la investigación.\nLa asignación uno a uno de los individuos no se puede si quiere controlar las covariables. La asignación estratificada debe hacerse de entrada. Necesito conocer qué niveles tengo para cada covariable y definir el tamaño muestral para cada nivel o combinación de niveles.\nEl número de estratos es el múltiplo del número de niveles de cada covariable.\nEn cada estrato se genera una secuencia de asignación mediante aleatorización simple o por bloques.\nVentajas:\n\nEl método asegura tener grupos de tamaño equilibrado teniendo en cuenta los factores influyentes.\n\nDesventajas:\n\nPrecisa conocer todas las características de los sujetos con anterioridad a la asignación en grupos.\n\nLa técnica se complica al aumentar el número de covariables.\n\nNo puede utilizarse si los sujetos se incluyen en el estudio uno a uno.\nAdaptativa o minimización\nTenemos un equilibrio en los bloques en función de los individuos que vamos recibiendo.\nSe utiliza para minimizar las diferencias de los tamaños de los distintos grupos.\nCada nuevo sujeto se asigna secuencialmente a un grupo concreto de tratamiento teniendo en cuenta las covariables y las asignaciones de los sujetos anteriores.\nEl investigador debe elaborar un plan de aleatorización para asignar los tratamientos a los pacientes\nVentajas:\n\nEl método es útil cuando hay muchas covariables y si la muestra de sujetos es pequeña.\n\nDesventajas:\n\nPrecisa recoger todas las características de los participantes con anterioridad a la aleatorización."
  },
  {
    "objectID": "tema_03/tema_03_3_obtener_datos_simulados.html#simulación-paramétrica",
    "href": "tema_03/tema_03_3_obtener_datos_simulados.html#simulación-paramétrica",
    "title": "3. Obtener datos simulados a partir de observaciones reales ✓",
    "section": "Simulación paramétrica",
    "text": "Simulación paramétrica\nEl peor de mis problemas es tener poca muestra ya que me complica cómo validar mis estimaciones.\nA partir de los datos observados (\\(y_1, …, y_n\\)) calculamos el valor del estimador del parámetro del modelo paramétrico, \\(\\hat{\\theta}\\).\n\n\\(\\theta\\) es el verdadero valor del parámetro poblacional.\n\\(\\hat{\\theta}\\) es el estimador que se calcula a partir de los datos muestrales.\n\\(\\hat{\\theta*}\\) es una estimación del mismo parámetro obtenida mediante una técnica de remuestreo, usada para analizar la variabilidad del estimador \\(\\hat{\\theta}\\)\n\nEjemplo\n\\[\n\\theta = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} |z_1 + z_2| e^{-\\frac{z_1^2 + z_2^2}{2}} \\, dz_1 dz_2\n\\]\n\\[\n\\theta = E \\left[ 2\\pi |Z_1 + Z_2| \\right], \\text{ siendo } Z_1 \\text{ y } Z_2 \\text{ v.a. } N(0,1) \\text{ independientes}\n\\]\n\nNsim=10000\nset.seed(5597)\n\nZ1 &lt;- rnorm(Nsim)\nZ2 &lt;- rnorm(Nsim)\nX &lt;- 2*pi*abs(Z1+Z2)\n\nesperanza_X &lt;- mean(X)\nsd_X &lt;- sd(X)\n\nalpha &lt;- 0.05\nz_a2 &lt;- qnorm(1-(alpha/2))\n\nLower &lt;- esperanza_X - (z_a2*sd_X/sqrt(Nsim))\nUpper &lt;- esperanza_X + (z_a2*sd_X/sqrt(Nsim))\nc(Lower, esperanza_X, Upper)\n\n[1] 7.024079 7.129826 7.235572\n\n\nValidez de la estimación\nSe generan nuevas muestras (\\(y*_1, …, y*_n\\)) a partir de la distribución \\(F(\\hat{\\theta})\\).\n\nObjetivo de la simulación.\nConseguir información sobre la distribución del estimador T de interés.\nSi existen resultados teóricos para la distribución de T o la relación entre el estimador y su parámetro es preferible utilizarlos a depender del resultado de la simulación.\n\n¿Cómo procedemos si tenemos problemas?.\nPosibles problemas:\n\nLas propiedades teóricas de T son complicadas.\n\nNo hay resultados asintóticos.\n\nLa muestra observada es pequeña.\n\n\nTécnicas Bootstrap [Man] (remuestreo con reemplazamiento)\n\nTécnica Bootstrap\nCuando no tenemos información de la población, la distribución empírica de una muestra aleatoria es la mejor representación de la distribución de la población ==&gt; La muestra observada se toma como modelo de la distribución desconocida.\nSi hay un resultado teorico, al teorico. Si hay una aproximación, a la apriximación. Si no hay información suficientepara tirar por lo asintótico (asintótico AKA apoximación) y tengo poca muestra: Bootstrap.\nPara mejorar el conocimiento de la distribución real la técnica bootstrap realiza muestreos con reemplazamiento teniendo en cuenta la distribución empírica.\n\nSi las características del estimador no son conocidas o son muy complejas o tengo muestra muy pequeña, uso Bootstrap. Bootstrap no crea ni destruye nada.\n\nRemuestreo. Saco muestras del mismo tamaño que la inicia. “Mi muestra era esta, pero si tomara nuevas muestras del mismo tamaño mi población sería esta”.\nFinalidad.\n- Validar, mediante intervalos de confianza, la estimación del parámetro que se consigue a partir de la muestra observada.\n- Realizar contrastes de hipótesis.\nProcedimiento.\n\nSean \\((y_1, …, y_n)\\) los resultados de una medida X en n sujetos independientes.\n\nSea \\(\\theta\\) una cantidad referida a X (valor medio, mediana, desviación…).\n\nCon los resultados observados podemos calcular el valor estimación de \\(\\theta\\): \\(\\hat{\\theta}\\)\n\n\nValidación de la estimación por IC.\n\nSimulamos una nueva muestra (\\(y*_1, …, y*_n\\)) remuestreando con repetición en los resultados iniciales y calculamos el valor de \\(\\hat{\\theta*}\\).\n\nRepetimos el proceso r-veces obteniendo r estimaciones bootstrap: \\(\\hat{\\theta_i*}\\), i=1,2,…r.\n\n¿Cuántas muestras? Eso es el capítulo final del tema.\n\n\nCalculamos las diferencias entre las estimaciones bootstrap y la estimación conseguida con la muestra inicial: \\(d_i = \\hat{\\theta_i*} - \\hat{\\theta}\\).\n\nObtenemos los cuantiles asociados \\(\\alpha/2\\) y \\(1-\\alpha/2\\): \\(d_b\\), \\(d_u\\).\n\nEl intervalo de confianza bootstrap \\(1-\\alpha\\) es: \\([\\hat{\\theta} + d_b, \\hat{\\theta} + d_u]\\)\n\n\nLa muestra original la guardo y la dejo apartada y trabajo con las muestras de Bootstrap (bueno esto volver a preguntárselo de cara a algún ejercicio pq tampoco creo si me ha contestado lo mismo dos veces seguidas). Trabajamos solo con las r estimaciones Bootstrap."
  },
  {
    "objectID": "tema_03/tema_03_3_obtener_datos_simulados.html#simulación-no-paramétrica",
    "href": "tema_03/tema_03_3_obtener_datos_simulados.html#simulación-no-paramétrica",
    "title": "3. Obtener datos simulados a partir de observaciones reales ✓",
    "section": "Simulación no paramétrica",
    "text": "Simulación no paramétrica\nNo se asume una distribución teórica, se remuestrea los datos originales para simular nuevas muestras.\ndiapo43 es la distribución del estadístico\npero no es la distribucion de p, sino del estadñistico\ncon la simulacion ya no tengo estimaciones, tengo estimaciones de la probabilidades\nlas muestras no tienen probabilidad, pq las muestras están fijas. tienen frecuencias, no dudo. en el modelo teórico tengo algo genérico, ahí sí hablo de probabilidades"
  },
  {
    "objectID": "tema_03/tema_03_3_obtener_datos_simulados.html#ejemplo-validar-índice-kappa",
    "href": "tema_03/tema_03_3_obtener_datos_simulados.html#ejemplo-validar-índice-kappa",
    "title": "3. Obtener datos simulados a partir de observaciones reales ✓",
    "section": "Ejemplo: Validar índice Kappa",
    "text": "Ejemplo: Validar índice Kappa\nSupongamos que se examinan 20 radiografías de la columna con el fin de detectar daños en la misma. Un par de radiólogos examinan las placas y emiten su diagnóstico: N = sin daño, I = daño incipiente, S = daño severo.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{N} & \\textbf{I} & \\textbf{S} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{N} & 6 & 1 & 0 \\\\\n                      & \\textbf{I} & 1 & 3 & 2 \\\\\n                      & \\textbf{S} & 0 & 3 & 4 \\\\\n\\hline\n& & & & & 20 \\\\\n\\end{array}\n\\]\n\\(\\hat\\kappa_0\\): estimacion con la muestra inicial\nn=20 \\(\\Rightarrow\\) Validacion con la metodlogía bootstrap. - Remuestreo entre las 20 diapos, cada diapo tiene 2 clasificaciones - Para cada muestra tengo una estimación de \\(\\hat\\kappa_{b_{1}}\\). - Haste tener \\(\\hat\\kappa_{b_{1000}}\\), tendré mil valores de mis estimaciones\n\n# 0 = sin lesion, 1 = daño leve, 2 = daño severo\nlibrary(psych)\nrad1 &lt;- c(2,1,0,0,2,0,0,0,2,2,1,1,2,0,1,1,2,2,1,0)\nrad2 &lt;- c(1,1,0,0,2,0,0,0,2,2,1,2,1,1,1,0,2,1,2,0)\n\n\ncokapp &lt;- cohen.kappa(x&lt;-cbind(rad1,rad2))\n#str(cokapp)\ncokapp$kappa\n\n[1] 0.4756554\n\nCoefi &lt;- cokapp$kappa\n\nBootstrap para Kappa\n\n#estructura bootstrap\nN_boot &lt;- 2000\nnn &lt;- length(rad1)\nB1 &lt;- numeric(nn)\nB2 &lt;- numeric(nn)\nk_boo &lt;- N_boot\n\n\n#Remuestreo en las placas de radiografías\nset.seed(108)\ntmp1 &lt;- sample(1:nn, nn*N_boot, replace=TRUE)\n\n\n# Asignamos a cada valor tmp1 la opinion de los radiologos\n# B1 para el radiólogo 1 y B2 para el radiólogo 2\n# Calculamos kappa en cada muestra bootstrap\n\nfor(j in 1:N_boot){\n  jj &lt;- j-1\n  for( i in 1:nn){\n    B1[i] &lt;- rad1[tmp1[nn*jj+i]]\n    B2[i] &lt;- rad2[tmp1[nn*jj+i]] }\n  y &lt;- cbind(B1,B2)\n  ckb &lt;- cohen.kappa(y)\n  k_boo[j] &lt;- ckb$kappa\n}\n\ndiff &lt;- k_boo - cokapp$kappa\ncuantiles &lt;- quantile(diff, c(.05, .95))\n(IC_kappa &lt;- cokapp$kappa + c(cuantiles[1], cuantiles[2]))\n\n       5%       95% \n0.1821561 0.7014925 \n\n\nVamos a analizar las diferencias entre la estimacion de la muestra inicial m0 contra cada una de estas estimaciones:\n\nConcordancia desde insignificante hasta sustancial.\nMuy muy muy válida la estimación no es.\n\nNo es porque la hayamos hecho mal, sino porque no podemos defender a muerte nuesta estimacion puntual.\nLo que sí podemos decir es que algún criterio común tienen."
  },
  {
    "objectID": "tema_03/tema_03_4_remuestreo_inferencia_roc.html#remuestreo-bootstrap-naïf",
    "href": "tema_03/tema_03_4_remuestreo_inferencia_roc.html#remuestreo-bootstrap-naïf",
    "title": "4. Técnicas de remuestreo aplicadas a la inferencia de curvas ROC ✗",
    "section": "Remuestreo bootstrap Naïf",
    "text": "Remuestreo bootstrap Naïf\nGenera una muestra con reemplazamiento de \\(n\\) individuos con información \\((Y_i, D_i)\\) de mi muestra.\nPor lo general conduce a malas aproximaciones de los cuantiles de la distribución empírica.\nAl elegir aleatoriamente con reemplazamiento pierdo la proporción de enfermos y sanos.\nPara generar la curva ROC y demás estimaciones perdemos la characteristic intrínseca del biomarcador de que no haya valores repetidos.\n\nRemuestreo bootstrap con aproximación de Monte-Carlo\nTras obtener un remuestreo naïf se altera el resultado de la prueba. Se perturba a través de la simulación de un valor aditivo \\(a\\) con distribucion \\(N(0, \\frac{1}{\\sqrt[5]{n}})\\)\nConseguimos alterar levemente los resultados del remuestreo naïf para romper esos “empates”.\nReproduce las ventajas teóricas del procedimiento suavizado con núcleo Gaussiano.\n\\[\n\\begin{array}{|c|c|}\n\\hline\n\\textbf{Y} & \\textbf{D} \\\\ \\hline\n\\hline\nY_1 + a_1 & D_1 \\\\ \\hline\nY_2 + a_2 & D_2 \\\\ \\hline\n\\vdots & \\vdots \\\\ \\hline\nY_n + a_n & D_n \\\\ \\hline\n\\end{array}\n\\]\n\n\nBootstrap suavizado con núcleo K\ntrabajar priemero con las funciones de distribucion empricas y simular con ellas\n\ntener muestra inicial\n1.1. aplicarmétodos asintóticos y pensar que es caca\n\nestimar curva roc y AUC\n2.1 esos valoers originales serán ROC_0 y AUC_0\n\nremuestreo a partir de la muestra inicial con la pertubación añadida del mismo tamaño muestral\n\nsaco la curva ROC y AUC\n\nRepito el punto 3 y 4.\n\nB muestras\nsi el “Calculamos di= AUC*i – AUC^, i=1,…,B” lo hiciéramos sin hacer la resta, es decir, apartar solo los extremos de las muestras en vez de los extremos de las diferencias, podría pasar que mi estimacion inicial no estuviera en el intervalo.\nel hacer la resta y tal se hace para asegurar que mi estimacion está contenida en el intervalo. busco la estimacion del AUC de las muestas bootstrap\nNOTA: en la diapo51 los \\(\\hat{F}_D^*\\) y \\(\\hat{F}_{\\over{D}}^*\\) son estimados.\nEjemplo.\nlos pacientes sin anemia tiene unos contenidos de hierro bajo inferiores (relacion inversa a la que estamos acostumrbados): tengo que identificar los ceros y los unos al revés.\nseleccionamos remuestreos, alterarlo con la dist normal, calcular diferencias, etc.\nes es curva roc con muestra inicial. 0.70 puede ser una pruebaadmisible si tuvieramos un IC que dice que no estamos con algo muy disparatado.\ncalculamos el IC. para una muestr que no era muy grande y aplicamos boots pq no etabamos seguros que nos diera buenos resultados aplicando los metodos asintoticos nos un IC estrechito. buscamos punto de corte con el metodo habitial.\n¿el índice yeuden (pregunto yo) es sobre la muestra original o sobre las muestas simuladas?\nyouden lo haces sobre tu original, la simu bootrap lo utilizas para IC para tu curva roc.\n\nbandas para la roc\ndiapo55 compara las curvas bootstrap de las muestras simuladas y la original. busca la máxima distancia entre las curvas simuladas y la original.\ntomar una dist mayor entre las muestra roc original y las roc simualadas por boostap"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Camargo-Ramos, C. M., and otros autores. 2012. “Evaluación de\nFactores Asociados Al Embarazo Adolescente.” Revista\nColombiana de Obstetricia y Ginecología 61 (3): 256–62. http://www.scielo.org.co/pdf/rcog/v61n3/v61n3a09.pdf.\n\n\nFisterra. 2024. “La Fiabilidad de Las Mediciones Clínicas:\nAnálisis de La Concordancia Para Variables Numéricas.” 2024. https://www.fisterra.com/formacion/metodologia-investigacion/la-fiabilidad-mediciones-clinicas-analisis-concordancia-para-variables-numericas/."
  }
]