[
  {
    "objectID": "index.html#evaluación",
    "href": "index.html#evaluación",
    "title": "Introducción",
    "section": "Evaluación",
    "text": "Evaluación\nTema 1: un examen (entre finales de octubre y noviembre)\nTema 2: un examen (el último día de clase)\nNota final: promedio de exámenes. (pilar de la nota final)\nNota final + varias entregas hasta un total de un punto (en total serán seis entregas, no son obligatorias)."
  },
  {
    "objectID": "index.html#temario",
    "href": "index.html#temario",
    "title": "Introducción",
    "section": "Temario",
    "text": "Temario\nDos primeros temas: pruebas diagnósticas. (probabildad) Tercer tema: tema transversal dirante el cuatrimestre, relacionado con la creación de muestras. (simulación)"
  },
  {
    "objectID": "DUDAS.html#ic-del-parámetro-por-el-método-de-los-momentos.",
    "href": "DUDAS.html#ic-del-parámetro-por-el-método-de-los-momentos.",
    "title": "Dudas",
    "section": "2.1 IC del parámetro por el método de los momentos.",
    "text": "2.1 IC del parámetro por el método de los momentos.\nEstamos haciendo IC por el método de los momentos. Entonces, cuando el parámetro a estimar es la media y teniendo en cuenta que la media tiene distrubución normal, ¿podemos hacer el IC por el método “clásico”?\n\nRESPUESTA.\n\nVale, nop.\nDos cosas:\n- En el método de los percentiles (el de clase) parece que es “innecesario” hacer las diferencias porque cuando buscamos los percentiles de las diferencias son los percentiles de una transformación lineal que luego deshacemos. Esto solo lo es para la media. Para la mediana, varianza, etc. no existe esa similitud.\n- No podemos hacer el método “clásico” tal cual porque al incluir el sesgo para calcular la amplitud del intervalo, este sesgo debe ser simulado por bootstrap con cada una de las muestras simuladas."
  },
  {
    "objectID": "DUDAS.html#qué-cojones-significa-validación-de-la-estimación",
    "href": "DUDAS.html#qué-cojones-significa-validación-de-la-estimación",
    "title": "Dudas",
    "section": "2.2 ¿Qué cojones significa validación de la estimación?",
    "text": "2.2 ¿Qué cojones significa validación de la estimación?\nRESPUESTA.\nSignifica cómo de válida es mi estimación para representar a mi población.\nSi mi media es 3.5 pero la muestra va de 3 a 9 (suponiendo que 9 no es outlier) no es representativo.\nTambién ligado a la amplitud de mi IC (en relación con el rango de los datos muestrales)."
  },
  {
    "objectID": "DUDAS.html#la-diferencia-entre-hattheta-y-hattheta-se-podría-llamar-sesgo",
    "href": "DUDAS.html#la-diferencia-entre-hattheta-y-hattheta-se-podría-llamar-sesgo",
    "title": "Dudas",
    "section": "2.3 La diferencia entre \\(\\hat\\theta\\) y \\(\\hat\\theta*\\), ¿se podría llamar sesgo?",
    "text": "2.3 La diferencia entre \\(\\hat\\theta\\) y \\(\\hat\\theta*\\), ¿se podría llamar sesgo?\nSeguro que no."
  },
  {
    "objectID": "DUDAS.html#concordancia-vs-chi2",
    "href": "DUDAS.html#concordancia-vs-chi2",
    "title": "Dudas",
    "section": "2.4 Concordancia vs \\(\\chi^2\\)",
    "text": "2.4 Concordancia vs \\(\\chi^2\\)"
  },
  {
    "objectID": "DUDAS.html#existe-un-intervalo-o-p-valor-asociado-a-la-medida-de-la-concordancia",
    "href": "DUDAS.html#existe-un-intervalo-o-p-valor-asociado-a-la-medida-de-la-concordancia",
    "title": "Dudas",
    "section": "2.5 ¿Existe un intervalo o p-valor asociado a la medida de la concordancia?",
    "text": "2.5 ¿Existe un intervalo o p-valor asociado a la medida de la concordancia?"
  },
  {
    "objectID": "DUDAS.html#concordancia-dos-observadores-cuando-tenemos-solo-un-observador",
    "href": "DUDAS.html#concordancia-dos-observadores-cuando-tenemos-solo-un-observador",
    "title": "Dudas",
    "section": "2.6 Concordancia dos observadores cuando tenemos solo un observador",
    "text": "2.6 Concordancia dos observadores cuando tenemos solo un observador\nLos métodos para medir la concordancia entre dos observadores/pruebas aplicados a dos mediciones de un mismo agente, ¿sirve como fiabiidad? ¿O tenemos otros métodos para medir la fiabilidad?"
  },
  {
    "objectID": "tema_00/tema_00_introduccion.html#propósito",
    "href": "tema_00/tema_00_introduccion.html#propósito",
    "title": "Introducción",
    "section": "Propósito",
    "text": "Propósito\nClasificación de un individuo dentro de un grupo de categorías.\nPrueba diagnóstica: cualquier prueba que me hacen. Una prueba diagnostica también puede ser un examen, ya que evaluo un conocimiento y creo una clasificacion.\nVer la concordancia entre varias pruebas o metodologías.\nUn examen, independientemente de qué profesor lo corrija, debería tener práctiamente la misma nota.\nTodo contraste estadístico podemos usarlos para varias acciones. “Un test puede ser usado para medir algo y también todo lo opuesto”.\nConcordancia = acuerdo.\nNo concordancia = actuar de manera idependiente = independientes.\nSi tengo varias metodologías para un mismo propósito (ej: enseñar a bebés a nadar) tengo que ver qué metodología funciona mejor.\nSi para una prueba tengo dos procedimientos y concuerdan entre ellos, puedo usar uno o otro. Si no hay concordancia mezclar los procedimientos me lleva a error.\nMuestra hetérea sin depender de datos muestrales.\nUn modelo que yo le dé cualquier muestra o información, y no un modelo que sirva solo para lo que he observado.\nLos registros simulados tiene que tener las mismas características que los datos muestrales.\nCómo consigo datos muestrales:\n- Datos poblacionales.\n- Simulacion, datos de individuos ficticios.\nProblema: datos faltantes de una de las variables de la muestra. Vamos a hablar más de los problemas que vamos a encontrar que cómo solucionarlos.\nDebemos ser conscientes de que la muestra actual es poco válida para experimentos futuros. Y posiblemente sea necesario ajustes del modelo en el futuro. \\(\\Rightarrow\\) Al igual que calibramos modelos, calibramos la muestra simulada y el modelo teórico.\nEn la vida real es al revés. Te dan unos datos y tienes que crear un modelo, es decir, crear algo que los represente."
  },
  {
    "objectID": "tema_00/tema_00_introduccion.html#planteamiento-estadístico",
    "href": "tema_00/tema_00_introduccion.html#planteamiento-estadístico",
    "title": "Introducción",
    "section": "Planteamiento ‘estadístico’",
    "text": "Planteamiento ‘estadístico’\n\nUn modelo teórico que podamos aplicar a cualquier conjunto de datos.\nX es una variable aleatoria y es la base del modelo teórico\nLa idea es estudiar X, conocer su comportamiento, tomar decisiones sobre X y aplicarlas sobre datos reales.\n¿La variable X ha sido estudiada antes?\n– Sí. Puedo utilizar esos resultados y aplicarlos a mi problema.\n– No. Debo crear el modelo y estudiarlo teóricamente.\n\n\nModelo teórico\nRepresentación matemática abstracta que describe el comportamiento de un fenómeno o conjunto de datos. Permite realizar inferencias, predicciones y análisis de fenómenos aleatorios o deterministas, y se basan en suposiciones sobre la estructura y distribución subyacente de los datos.\n\nComponentes\n\nVariables (características)\nParámetros. Valores desconocidos que determinan la forma de la distribución de datos.\nDistribución de probabilidad.\nSuposiciones sobre el comportamiento de los datos. Por ejemplo, en un modelo de regresión lineal simple se asume que la relación entre las variables es lineal, que los errores tienen una distribución normal con media cero y varianza constante y que son independientes entre sí.\n\n\n\nUso\nEntender la estructura subyacente de los datos, hacer predicciones sobre nuevas observaciones y probar hipótesis.\nEl modelo teórico tiene su propia unidad de medida: la probabilidad.\nIt’s not the same probabilidades que porcentajes, es impreciso. Si p(“enfermo”) = 0.10 no podemos decir que el 10 % de la pobaclión esté enferma, aunque usemos ese lenguaje. Lo correcto es decir que la probabilidad de que una persona de la población esté enferma es 10 %.\nEl IC no es para el parámetro. El IC es para el estimador del parámetro. El IC contiene el % de las muestras del resto del mundo.\nDebemos interpretar las estimaciones puntuales y la distribución acumulada FDA/CFD."
  },
  {
    "objectID": "tema_00/tema_00_introduccion.html#iteraciones-de-todo-problema-estadístico",
    "href": "tema_00/tema_00_introduccion.html#iteraciones-de-todo-problema-estadístico",
    "title": "Introducción",
    "section": "Iteraciones de todo problema estadístico",
    "text": "Iteraciones de todo problema estadístico\nCuestiones asociadas:\n\n¿A qué alumnos? A todos los alumnos.\n\nTengo una oferta si hago un pedido grande.\n\n¿El modelo es único? No, hay tres tallas: P, M y G.\n\nNecesito: Medir la circunferencia del dedo de los alumnos. ¿A todos? ¿qué dedo?\n\n¿Quién aporta la información de 4? Los alumnos del curso actual. ¿Cómo se realiza la medida?\n\n\n\n\nHay varios procedimientos ¿Por ejemplo?\n\n\n\nLos resultados de cada procedimiento\n\n\n\n\nConcuerdan (puedo utilizar cualquiera o mezclarlos) (*1)\n\nHay alguno “mejor” (*2)\n\n\n\nCon (todas/parte de) las medidas de los alumnos establecemos las tallas P, M y G\n\nDecisión sobre la cantidad a pedir, en total y de cada talla\nPoner en práctica → REVISAR: Si hay errores volver a 4"
  },
  {
    "objectID": "tema_00/tema_00_introduccion.html#ejemplo",
    "href": "tema_00/tema_00_introduccion.html#ejemplo",
    "title": "Introducción",
    "section": "Ejemplo",
    "text": "Ejemplo\nEl modelo teórico asociado a este tipo de epidemias indica que:\n– La probabilidad de que en una familia la madre tenga gripe es 0.1\n– En el 12% de las familias el padre tiene gripe\n– Ambos progenitores tienen gripe en el 2% de las familias (= con probabilidad 0.02)\n\n\n\n\n\n\nImportante\n\n\n\nSi sé controlar el comportamiento de una v.a. y sé simular cien familias, ¿qué esperaría si tengo una simulación buena? Que en estas cien simulaciones haya un comportamiento parecido a las bases del modelo teórico.\nCon el modelo teorico tendré dudas pero con datos simulados tangibles ya no tengo probabilidades, tengo un hecho.\n\n\nPara simular una familia simularía pares de datos (padres). Es decir, ´trabajamos con la distribución conjunta. Simularía el 00, el 01, el 10 y el 11. Esto nos permite usar un único número aleatorio por familia, ya que con una única simulación puedo asignar el estado del par de datos (no necesito simular la madre y luego el padre).\nFijo el estado más probables para todas las familias y si sale la condición de los menos frecuentes, lo cambio.\n0.0 - 0.2 pongo 11 0.2 - 0.3 pongo 10 0.3 - 0.5 pongo 01 0.5 - 1.0 pongo 00\nSi quiero algo MÁS ROBUSTO necesito más muestra.\n\n\n\n\n\n\nNota\n\n\n\nEn un modelo teórico tengo un suceso con una probabilidad tal. Con los datos simulados tengo un porcentaje de cada suceso."
  },
  {
    "objectID": "tema_01/tema_01.html#medidas-de-precisión-en-pruebas-diagnósticas.-índices-de-concordancia",
    "href": "tema_01/tema_01.html#medidas-de-precisión-en-pruebas-diagnósticas.-índices-de-concordancia",
    "title": "TEMA 1",
    "section": "Medidas de precisión en pruebas diagnósticas. Índices de concordancia",
    "text": "Medidas de precisión en pruebas diagnósticas. Índices de concordancia\nSimulación numérica para desarrollar pruebas diagnósticas.\nAnálisis de concordancia: se analiza de manera distinta dependiendo del objetivo (y tipo de variables)\nSiempre es lo mismo, árbitro!\nModelo teótico \\(\\Rightarrow\\) Realidad \\(\\Rightarrow\\) Estimamos\nNo es posible dar un valor de concordancia único sino un grado de amplitud. (el nuestro es este grado pero podíamos haber obtenido en un x % de los casos un valor en este intervalo)"
  },
  {
    "objectID": "tema_01/tema_01_1_introduccion.html#definiciones-basadas-o-pensadas-sobre-el-modelo-teórico",
    "href": "tema_01/tema_01_1_introduccion.html#definiciones-basadas-o-pensadas-sobre-el-modelo-teórico",
    "title": "1. Introducción ✓",
    "section": "Definiciones basadas o pensadas sobre el modelo teórico",
    "text": "Definiciones basadas o pensadas sobre el modelo teórico\nPrevalencia\nProbabilidad de que se observe algo. (porque estamos en el modelo teórico)\nPueba diagnóstica\nRápidamente pensamos que es algo clínico pero no siempre es así. Puede ser un examen donde yo asigne un nivel de conocimiento. La prueba diagnóstica puede fallar porque puede dar una nota menor a la real o dar un mayor conociemiento que el verdadero.\nLas pruebas diagnósticas clínicas o experimentales sirven para localizar y situar a un individuo dentro de una clasificacion precisa (una medida según un baremo) o grupo que le corresponde (aprobado/enfermo, sano/enfermo, brazo roto/brazo no roto).\nDependiendo del tipo de variable al que esté asociada la p.d. nos indica qué metodología vamos a usar.\nDebo tener claro para qué fin realizo la p.d..\nPara ver si una prueba diagnóstica cumple su objetivo nos apoyamos en estimaciones basadas en el modelo teórico. Las estimaciones están basadas en los valores que nos den los individuos. Para tener buenas estimaciones necesito buenas medidas. Un aspecto fundamental en estudios de investigación es garantizar la calidad de los procedimientos de medida. La calidad de una medida depende tanto de su fiabilidad como de su validez.\n\nLa fiabilidad indica hasta qué punto se obtienen los mismos valores al efectuar la medición en más de una ocasión, bajo condiciones similares.\n\nLa validez expresa el grado en el que realmente se mide el fenómeno de interés.\n\nQue una medida sea muy precisa no implica que sea necesariamente válida. Por ejemplo, si se realizan dos mediciones consecutivas a un paciente con una herramienta mal calibrada los valores obtenidos seguramente serán parecidos aunque inexactos. (Fisterra 2024)"
  },
  {
    "objectID": "tema_01/tema_01_1_introduccion.html#clasificación-de-estudios-para-la-evaluación-de-la-calidad-de-los-procedimientos-de-medidas",
    "href": "tema_01/tema_01_1_introduccion.html#clasificación-de-estudios-para-la-evaluación-de-la-calidad-de-los-procedimientos-de-medidas",
    "title": "1. Introducción ✓",
    "section": "Clasificación de estudios para la evaluación de la calidad de los procedimientos de medidas",
    "text": "Clasificación de estudios para la evaluación de la calidad de los procedimientos de medidas\n\nFiabilidad: comparación/variación consigo mismo. (concordancia intraobservador)\n\nConcordancia: comparación/variación con otro observador (interobservador) o entre métodos.\n\nCalibración: comparación/variación con un método estándar.\n\n\n\n\n\n\n\nFiabilidad, repetibilidad.\nMedidas fiables/repetibles/reproducibles.\nEstabilidad \\(\\Rightarrow\\) Individuos con caracteristicas similares darán lugar a medidas similares.\nEstudios de fiabilidad. Intentan evaluar como concuerdan las medidas obtenidas por un único método o instrumento. Se evalúa el error de medida del método mediante el estudio de la concordancia intramétodo, si las medidas concuerdan se puede decir que el método es repetible.\n\n\nConcordancia.\nSi la damos la vuelta podríamos pensar en independencia. Personas que funcionan de forma independiente. (no lo termino de pillar)\nEstudios de concordancia: se desea evaluar como concuerdan las medidas realizadas con el método cuya calidad se desea valorar con los obtenidos por otro método. Valoramos la concordancia entre métodos de medida con el objetivo de determinar si ambos son intercambiables.\nLas técnicas utilizadas para estudiar la concordancia varían según la naturaleza de las variables, dependiendo de si las medidas corresponden a una escala de medida cualitativa o cuantitativa.\nLa concordancia adquiere importancia cuando se desea conocer si con un método o instrumento nuevo, diferente al habitual, se obtienen resultados equivalentes de tal manera que eventualmente uno y otro puedan ser remplazados o intercambiados ya sea porque uno de ellos es más sencillo, menos costoso y por lo tanto más costo-efectivo, o porque uno de ellos resulta más seguro para el paciente, entre otras múltiples razones. En términos generales, la concordancia es el grado en que dos o más observadores, métodos, técnicas u observaciones están de acuerdo sobre el mismo fenómeno observado. La concordancia no evalúa la validez o la certeza sobre una u otra observación con relación a un estándar de referencia dado, sino cuán acordes están entre sí observaciones sobre el mismo fenómeno. (Camargo-Ramos y autores 2012)\n\n\nCalibración.\nNo solo queremos que no haya errores de medida. Cuando calibramos algo lo calibramos con un instrumento. Calibrar es comparar las medidas que tenemos con un método con otras medidas que están aprobadas por todos (gold standar).\nEstudios de calibración: pueden verse como caso particular de los estudios de concordancia entre métodos, cuando se compara un procedimiento de medida con los valores reales o de referencia (gold standard).\n\nEn el mundo clínico existe mucha confusión debido a que heredamos mucha terminología inglesa ya que a veces no tiene una equivalencia.\nLa medida de un individio vendrá dada por características que podemos medir y por algo intrínseco del propio individuo. ¿Qué factores influyen para que haya resultados diferentes entre individuos con las mismas características? Esos factores son el error. Queremos medir esa parte intrínseca al individuo y que no podemos incluir en el modelo.\nEl error lo queremos controlar midiéndolo con diferentes metodologías. ¿El error es diferente dependiendo de la metodología?\n\\[\n\\displaylines{\n& \\text {Sea X el resultado de la prueba diagnóstica} \\\\\n&\\begin{array}{cccc}\n\\hline  \\text { Tipo de la variable X } & \\text { Objetivo de la prueba } & \\text { Índice o argumento de concordancia } \\\\\n\\hline\n\\text {Cualitativo} & \\text {La prueba clasfica} & \\text {Coeficiente kappa} \\\\\n\\text {Cuantitativo} & \\text {Tiene una unidad de medida} & \\text {Concordancia intraclase} \\\\\n&  & \\text {Método gráfico de Bland y Altman} \\\\\n\\hline\n\\end{array}\n}\n\\]\n\n\n\n\nCamargo-Ramos, C. M., y otros autores. 2012. «Evaluación de factores asociados al embarazo adolescente». Revista Colombiana de Obstetricia y Ginecología 61 (3): 256-62. http://www.scielo.org.co/pdf/rcog/v61n3/v61n3a09.pdf.\n\n\nFisterra. 2024. «La fiabilidad de las mediciones clínicas: análisis de la concordancia para variables numéricas». 2024. https://www.fisterra.com/formacion/metodologia-investigacion/la-fiabilidad-mediciones-clinicas-analisis-concordancia-para-variables-numericas/."
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#clasificaciones-binomiales",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#clasificaciones-binomiales",
    "title": "2. Análisis de concordancia en variables categóricas ✗",
    "section": "Clasificaciones binomiales",
    "text": "Clasificaciones binomiales\nCasos donde la prueba diagnóstica solo presenta dos resultados distintos.\nAnalizamos si dos agentes que clasifican a los individuos según el resultado de la p.d. concuerdan en sus opiniones. También me sirve la discordancia completa. (sé que un agente siempre clasifica lo contrario al otro agente)\nComo la p.d. depende del agente que me da la medida, tengo una variable aleatoria por cada agente.\nX = “clasificación dada por el observador A.”\nY = “clasificación dada por el observador B.”\nDado un individuo va a ser analizado por el agente A y el agente B y cada uno de ellos va a dar su opinión.\n\\[\n\\displaylines{\n& \\text {Medidas para cada inidividuo} \\\\\n&\\begin{array}{ccc}\n\\hline\n\\text{individuo k} & X_k & Y_k\n\\end{array}\n}\n\\]\nResumen de la información del modelo teórico.\nSuponemos distribución de probabilidad conjunta.\n\\[\n\\begin{array}{c|c c c c}\n    & & \\textbf{Y} & & \\\\\n    & \\textbf{Categoría} & \\textbf{1} & \\textbf{2} & \\textbf{Total} \\\\\n\\hline\n\\textbf{X} & \\textbf{1} & \\pi_{11} & \\pi_{12} & \\pi_{1\\cdot} \\\\\n           & \\textbf{2} & \\pi_{21} & \\pi_{22} & \\pi_{2\\cdot} \\\\\n\\hline\n& \\textbf{Total} & \\pi_{\\cdot1} & \\pi_{\\cdot2} & 1 \\\\\n\\end{array}\n\\]\n\\[\n\\begin{array}{}\n&\\pi_{.j} = (X,Y) \\epsilon {(1,1),(1,2),(2,1),(2,2)} \\\\\n\\pi_{ij} = \\text{p({X=i, Y=j})} & \\pi_{i.} = \\text{p({Y=j})} & \\pi_{.j} =  \\text{p({X=i})} \\\\\n\\end{array}\n\\]"
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#acuerdo-observado-o-índice-de-concordancia-global",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#acuerdo-observado-o-índice-de-concordancia-global",
    "title": "2. Análisis de concordancia en variables categóricas ✗",
    "section": "Acuerdo observado o Índice de concordancia global",
    "text": "Acuerdo observado o Índice de concordancia global\nLa aproximación a la concordancia más intuitiva. Expresa el porcentaje de coincidencia en la clasificación de ambos agentes.\nEl problema que plantea este índice básico es que una parte de ese acuerdo puede deberse exclusivamente al azar.\nMedir la concordancia es saber la probabilidad con la que los dos dan la misma clasificación.\n\nMedida de concordancia global: \\(\\Pi_0 = \\pi_{11} + \\pi_{22} = \\sum_{i=j} \\pi_{ij}\\)\n\nEjemplo 1.0.\nSe recogen las respuestas a las entrevistas 1 y 2 de manera independiente.\nEntrevista 1: ¿Consume usted suplementos vitamínicos?\nEntrevista 2: Responda si consume vitaminas sin contar sus aportes alimentarios\n\n\nCaso 1. Concordancia perfecta. {X=Y} o {X\\(\\neq\\)Y}\np({X=Y})=1\n\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.6 & 0 & 0.6 \\\\\n                      & \\textbf{Sí} & 0 & 0.4 & 0.4 \\\\\n\\hline\n& \\textbf{Total} & 0.6 & 0.4 & 1 \\\\\n\\end{array}\n\\]\n\nLa probabilidad de que un individuo conteste lo mismo en ambos cuestionarios es 1.\n\n\nCaso 2. Concordancia.\n\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{2} & \\textbf{Sí} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.4 & 0.2 & 0.6 \\\\\n                      & \\textbf{Sí} & 0.2 & 0.2 & 0.4 \\\\\n\\hline\n& \\textbf{Total} & 0.6 & 0.4 & 1 \\\\\n\\end{array}\n\\]\n\nLa probabilidad de dar la misma respuesta es 0.8.\n\n\nCaso 3. Sin concordancia.\n\n\nLas dos entrevistas concuerdan el 40 % de las ocasiones. Puede no haber asociación real entre las dos entrevistas porque toda la concordancia puede atribuirse al azar.\n¿Cuál sería la distribución de las respuestas si los individuos responden tirando una moneda? (cara: no, cruz: sí)\np(X=“no”, Y=“sí”) = p(“sale cara primera moneda”, “sale cara segunda moneda”) = 0.5 * 0.5 = 0.25\np(X=“sí”, Y=“no”) = p(X=“no”, Y=“no”) = p(X=“sí”, Y=“sí”) = 0.25\n\\(\\pi_{0}\\) = 0.25 + 0.25 = 0.5\\(\\pi_{i} = \\pi_{1.} * \\pi_{.1} + \\pi_{2.} * \\pi_{.2}\\) = 0.5 * 0.5 + 0.5 * 0.5 (concordancia debida al azar)\n\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.25 & 0.25 & 0.5 \\\\\n                      & \\textbf{Sí} & 0.25 & 0.25 & 0.5 \\\\\n\\hline\n& \\textbf{Total} & 0.5 & 0.5 & 1 \\\\\n\\end{array}\n\\]\n\nEsta sería la máxima no concordancia, ya que si la diagonal principal acumula menos de la mitad de los casos podríamos medir una concordancia inversa.\n\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.3 & 0.3 & 0.6 \\\\\n                      & \\textbf{Sí} & 0.3 & 0.1 & 0.4 \\\\\n\\hline\n& \\textbf{Total} & 0.6 & 0.4 & 1 \\\\\n\\end{array}\n\\]\nEl problema de \\(\\Pi_{0}\\) es que incluye concordancia que puede atribuirse al azar. Y concordar por azar no es concordar.\nSi los dos observadores clasificasen de forma independiente y, por tanto, totalmente al azar en las dos categorías, la probabilidad de la concordancia (p(X=Y)) sería:\n\nMedida de concordancia debida al azar: \\(\\Pi_c = \\pi_{1.}\\pi_{.1} + \\pi_{2.}\\pi_{.2} = \\sum_{k=1,2} \\pi_{k.}\\pi_{.k}\\)\n\nA la concordancia global debo restarle la concordancia del azar."
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#índice-kappa-de-cohen",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#índice-kappa-de-cohen",
    "title": "2. Análisis de concordancia en variables categóricas ✗",
    "section": "Índice kappa (de Cohen)",
    "text": "Índice kappa (de Cohen)\nEste índice es una medida basada en resultados teóricos.\n\\(k = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c}\\) tq \\(\\Pi_0 = \\pi_{11} + \\pi_{22}\\) y \\(\\Pi_c = \\pi_{1.}\\pi_{.1} + \\pi_{2.}\\pi_{.2}\\)\nA la concordancia le quita la concordancia debida al azar y lo compara con la concordancia perfecto sin contar la concordancia al azar. Es decir, a la concordancia y a la concordancia perfecta les substrae la concordancia debida al azar y compara la concordancia sin el azar obtenida contra la máxima concordancia sin el azar posible.\nLa interpretabilidad o Podemos tener resultados negativos: el intervalor [-1,0) es sin corcondancia.\nEjemplo 1.0.\n\nCaso 1. Concordancia perfecta.\n\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.6 & 0 & 0.6 \\\\\n                      & \\textbf{Sí} & 0 & 0.4 & 0.4 \\\\\n\\hline\n& \\textbf{Total} & 0.6 & 0.4 & 1 \\\\\n\\end{array}\n\\]\n\nCuando tengo una concordancia perfecta, la concordancia debido al azar es 0.5.\n\\(\\Pi_c = \\pi_{1.}\\pi_{.1} + \\pi_{2.}\\pi_{.2} = 0.36 + 0.24 = 0.5\\)\nY POR EEEEEEEEEEEESO, usamos el índice kappa del señor Cohen.\n\n\n\nCaso 3. Sin corcondancia.\n\nCuando la concordancia o acuerdo observado coincide con la esperada por el azar el índice kappa toma el valor 0.\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.25 & 0.25 & 0.5 \\\\\n                      & \\textbf{Sí} & 0.25 & 0.25 & 0.5 \\\\\n\\hline\n& \\textbf{Total} & 0.5 & 0.5 & 1 \\\\\n\\end{array}\n\\] \\(\\Pi_0 = \\pi_{11} + \\pi_{22} = 0.5\\) y \\(\\Pi_c = \\pi_{1.}\\pi_{.1} + \\pi_{2.}\\pi_{.2} = 0.5*0.5 + 0.5*0.5 = 0.5\\)\n\\(k = \\frac{0.5-0.5}{1-0.5} = 0\\)\n\\[\n\\begin{array}{c|c c c c}\n& & \\textbf{Entervista 1} \\\\\n& & \\textbf{No} & \\textbf{Sí} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Entrevista 2} & \\textbf{No} & 0.3 & 0.3 & 0.6 \\\\\n                      & \\textbf{Sí} & 0.3 & 0.1 & 0.4 \\\\\n\\hline\n& \\textbf{Total} & 0.6 & 0.4 & 1 \\\\\n\\end{array}\n\\]\n\\(\\Pi_0 = \\pi_{11} + \\pi_{22} = 0.4\\) y \\(\\Pi_c = \\pi_{1.}\\pi_{.1} + \\pi_{2.}\\pi_{.2} = 0.6*0.6 + 0.4*0.4 = 0.52\\)\n\\(k = \\frac{0.4-0.52}{1-0.52} = -0.25\\)\n\nInterpretación del índice kappa (si \\(k\\)&gt;0)\nLos datos observados son un caso particular.\nLa probabilidad de acuerdo observado es una media ponderada del máximo acuerdo (1) y del acuerdo debido al azar (\\(Pi_{c\\)), siendo \\(k\\) el peso del máximo acuerdo.\n\n\\[\n\\displaylines{\nk = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c} \\\\ \\\\\nk * (1-\\Pi_{i}) = (\\Pi_{0} - \\Pi_{c}) \\\\ \\\\\n\\Pi_{0} = k + (1-k)*\\Pi_{c}\n}\n\\]\nEl acuerdo observado es una combinación de la concordania perfecta y concordania debida al azar. El ínidce kappa da más peso a uno o al otro.\n\nEjemplo 1.1.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.04 & 0.06 & 0.10 \\\\\n                      & \\textbf{No pulmonía} & 0.10 & 0.80 & 0.90 \\\\\n\\hline\n& \\textbf{Total} & 0.14 & 0.86 & 1 \\\\\n\\end{array}\n\\]\n\ndata &lt;- matrix(c(0.04, 0.06, 0.10, 0.80), nrow = 2, byrow = TRUE,\n               dimnames = list(c(\"Pulmonía B\", \"No pulmonía B\"),\n                               c(\"Pulmonía A\", \"No pulmonía A\")))\ndata\n\n              Pulmonía A No pulmonía A\nPulmonía B          0.04          0.06\nNo pulmonía B       0.10          0.80\n\n\n\ndf &lt;- data.frame(col1 = c(0.04, 0.10),\n                 col2 = c(0.06, 0.80)\n)\ncolnames(df) &lt;- c(\"Pulmonía A\", \"No pulmonía A\")\nrownames(df) &lt;- c(\"Pulmonía B\", \"No pulmonía B\")\ndf\n\n              Pulmonía A No pulmonía A\nPulmonía B          0.04          0.06\nNo pulmonía B       0.10          0.80\n\n\n\\(\\Pi_{0} = \\text{\"concordancia absoluta\"} = 0.04 + 0.80 = 0.84\\)\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n\n[1] 0.84\n\n\n\n(pi_0 &lt;- df[1,1] + df[2,2])\n\n[1] 0.84\n\n\n\n(pi_0 &lt;- df[\"Pulmonía B\",\"Pulmonía A\"] + df[\"No pulmonía B\",\"No pulmonía A\"])\n\n[1] 0.84\n\n\n\\(\\Pi_{1} = \\text{\"concordancia debida al azar\"} = 0.10*0.14 + 0.86*0.90 = 0.788\\)\n\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n\n[1] 0.788\n\n\n\n(pi_1 &lt;- sum(df[1]) * sum(df[1,])  + sum(df[2]) * sum(df[2,]))\n\n[1] 0.788\n\n\n\n(pi_1 &lt;- sum(data[1,])*sum(data[,1]) + sum(data[2,])*sum(data[,2]))\n\n[1] 0.788\n\n\nInterpretación.]{underline}\nLa concordancia observada en los informes (84%) está compuesta por un 24,5% de la concordancia máxima y un 78,8% de la esperada al azar.\nClasificaciones multinomiales\n\\[\n\\begin{array}{c|c c c c}\n    & & \\textbf{Y} & & \\\\\n    & \\textbf{Categoría} & \\textbf{1} & \\textbf{...} & \\textbf{t} & \\textbf{Total} \\\\\n\\hline\n\\textbf{X} & \\textbf{1} & \\pi_{11} & ... & \\pi_{1t} & \\pi_{1\\cdot} \\\\\n           & \\textbf{2} & \\pi_{21} & ... & \\pi_{2t} & \\pi_{2\\cdot} \\\\\n           & \\textbf{...} & ... & ... & ... & ... \\\\\n           & \\textbf{t} & \\pi_{2t} & ... & \\pi_{tt} & \\pi_{t\\cdot} \\\\\n\\hline\n& \\textbf{Total} & \\pi_{\\cdot1} & ... & \\pi_{\\cdot t} & 1 \\\\\n\\end{array}\n\\]\n\nMedida de concordancia global: \\(\\Pi_0 = \\sum_{i=1}^{t} \\pi_{ii}\\)\n\n\nMedida de concordancia debida al azar: \\(\\Pi_c = \\sum_{i=1}^{t} \\pi_{i.}\\pi_{.i}\\)\n\n\nÍndice kappa: \\(k = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c}\\)\n\nHay veces que nos interesa saber si la concordancia es mayor en unas categorías o en otras. Por ejemplo, en la categoría grave hay muchas concordancia pero en las categorías leve y media no. En este caso se suele estandarizar para tener dos categorías y medir concordancia para cada categoría.\nEjemplo 1.2.\n\ndf &lt;- data.frame(rbind(c(0.1125, 0.1, 0.0375),\n                       c(0.1125, 0.3625, 0.0625),\n                       c(0, 0.0375, 0.175))\n)\ncolnames(df) &lt;- c(\"Leve\", \"Moderada\", \"Grave\")\nrownames(df) &lt;- c(\"Leve\", \"Moderada\", \"Grave\")\ndf\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\n\n\n\ndf_extended &lt;- rbind(df, \"Total\" = colSums(df))\ndf_extended\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\nTotal    0.2250   0.5000 0.2750\n\n\n\ndf_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\ndf_extended\n\n           Leve Moderada  Grave  Total\nLeve     0.1125   0.1000 0.0375 0.2500\nModerada 0.1125   0.3625 0.0625 0.5375\nGrave    0.0000   0.0375 0.1750 0.2125\nTotal    0.2250   0.5000 0.2750 1.0000\n\n\nMedida de concordancia global\n\n(pi_0 &lt;- sum(diag(as.matrix(df))))\n\n[1] 0.65\n\n\nMedida de concordancia debida al azar\n\npi_1 &lt;- 0\nt &lt;- dim(df_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_extended[i,t]*df_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\n[1] 0.3834375\n\n\nÍndice kappa\n\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] 0.4323365\n\n\nEl índice kappa no mide la distancia entre las discordancias. No es lo mismo decir (X=7, Y=8) que (X=7, Y=2).\nComparando cada categoría con el resto\n\ndf\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\n\n\n\nCódigocat &lt;- \"Leve\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Leve\"\n        Leve  Otras Total\nLeve  0.1125 0.1375  0.25\nOtras 0.1125 0.6375  0.75\nTotal 0.2250 0.7750  1.00\n[1] \"Medida de concordancia global:\"\n[1] 0.75\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.6375\n[1] \"Índice kappa:\"\n[1] 0.3103448\n\n\n\nCódigocat &lt;- \"Moderada\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Moderada\"\n         Moderada Otras  Total\nModerada   0.3625 0.175 0.5375\nOtras      0.1375 0.325 0.4625\nTotal      0.5000 0.500 1.0000\n[1] \"Medida de concordancia global:\"\n[1] 0.6875\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.5\n[1] \"Índice kappa:\"\n[1] 0.375\n\n\n\nCódigocat &lt;- \"Grave\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Grave\"\n      Grave  Otras  Total\nGrave 0.175 0.0375 0.2125\nOtras 0.100 0.6875 0.7875\nTotal 0.275 0.7250 1.0000\n[1] \"Medida de concordancia global:\"\n[1] 0.8625\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.629375\n[1] \"Índice kappa:\"\n[1] 0.6290051\n\n\nHasta ahora hemos hablado de probabilidades ya que estamos con el modelo teórico. Cuando trabajamoes con datos de una muestra hablaremos de porcentajes. \\(\\Rightarrow\\) “Este % es consecuencia de un acuerdo de tal y un azar de no sé cuántos”\nEl índice kappa tiene una distribución teórica y lo estimo según una muestra. Con una poca información (la muestra) doy un intervalo. No doy intervalos de confianza para valores desconocidos sino para las estimaciones.\nInconvenientes del índice kappa\nEjemplo 1.1.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.04 & 0.06 & 0.10 \\\\\n                      & \\textbf{No pulmonía} & 0.10 & 0.80 & 0.90 \\\\\n\\hline\n& \\textbf{Total} & 0.14 & 0.86 & 1 \\\\\n\\end{array}\n\\]\nLa prevalencia de pulmonía es baja: 014 para el radiólogo A y 0.1 para el rediólogo B.\n**Ambas marginales desequilibradas*: la prevalencia observada dista mucho de 0.5 (a favor de los diagnósticos negativos)\nEl bajo valor del índice kappa se explica porque nos encontramos en el peor de los escenarios: baja prevalencia y marginales desequilibradas.\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.788\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.245283\n\nEl valor del índice kappa depende de la prevalencia de la característica observada.\nEjemplo 1.1.a.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.30 & 0.06 & 0.36 \\\\\n                      & \\textbf{No pulmonía} & 0.10 & 0.54 & 0.64 \\\\\n\\hline\n& \\textbf{Total} & 0.40 & 0.60 & 1 \\\\\n\\end{array}\n\\]\nAumentamos la prevalencia: 0.40 para el radiólogo A y 0.36 para el rediólogo B.\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.528\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.6610169\n\n\nCuanto más cercana a 0,5 sea la prevalencia (más equilibradas estén las marginales) mayor es el índice kappa, para igual probabilidad de acuerdos observados.\n\n\\[\\Downarrow\\]\n\nPrevalencias muy altas o muy bajas penalizan el índice kappa.\n\nEl valor del índice kappa depende de la simetría y homogeneidad de las marginales.\nEn el primer caso el comportamiento de los agentes es homogéneo: pues ambos emiten informes positivos con mayor frecuencia.\nEn el segundo caso el comportamiento de los agentes es heterogéneo y asimétrico.\nEn ambos casos los diagnósticos del agente A distan 0.2 de 0.5.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.45 & 0.15 & 0.60 \\\\\n                      & \\textbf{No pulmonía} & 0.25 & 0.15 & 0.40 \\\\\n\\hline\n& \\textbf{Total} & 0.70 & 0.30 & 1 \\\\\n&  & 0.50+0.20 & 0.50-0.20 &  \\\\\n\\end{array}\n\\]\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.54\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.6521739\n\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.25 & 0.35 & 0.60 \\\\\n                      & \\textbf{No pulmonía} & 0.05 & 0.35 & 0.40 \\\\\n\\hline\n& \\textbf{Total} & 0.30 & 0.70 & 1 \\\\\n&  & 0.50-0.20 & 0.50+0.20 &  \\\\\n\\end{array}\n\\]\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.46\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.7037037\n\nEl valor del índice kappa depende del equilibrio de las marginales.\nCuanto mayor sea la diferencia de la prevalencia observada de cada agente respecto de 0.5 mayor es el índice kappa. (incluso para un mismo valor de acuerdos observados).\nÍndice kappa ponderado.\nSólo tiene sentido para variables ordinales.\nEstá diseñado para recoger la idea de que algunas discordancias son más severas que otras y, por tanto, asigna pesos que representan la importancia entre los desacuerdos. El máximo peso se da a la concordancia perfecta y pesos proporcionalmente menores según la importancia del desacuerdo.\nNo tiene la misma importancia un desacuerdo en la clasificación entre las categorías leve y moderada que entre leve y grave, obviamente la última representa un mayor desacuerdo que la primera.\n\nMedida de concordancia global: \\(\\Pi_0 = \\sum_{i=1}^{t} \\sum_{j=1}^{t} w_{ij} \\pi_{ij}\\)\n\n\nMedida de concordancia debida al azar: \\(\\Pi_c = \\sum_{i=1}^{t} \\sum_{j=1}^{t} w_{ij} \\pi_{i.}\\pi_{.j}\\)\n\n\nÍndice kappa: \\(k_{w} = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c}\\)\n\nLos pesos satisfacen:\n\\[\nw_{ij} = \\begin{cases}\nw_{ii} = 1 \\\\\n0 \\le w_{ij} \\le 1 \\\\\nw_{ij} = w_{ji}\n\\end{cases}\n\\]\n\\[\nw_{ij} = \\begin{cases}\n1, \\hspace{1em} i = j \\\\\n0, \\hspace{1em} i \\neq j \\\\\n\\end{cases} \\hspace{1em} \\text{sii} \\hspace{1em} k_{w} = k \\\\\n\\] \\[\n\\text{si t = 2} \\Rightarrow k_{w} = k\n\\]\nPonderaciones.\n\nCicchetti-Allison (1971) \\(w_{ij} = 1 - \\frac{|i-j|}{t-1}\\)\nFleiss-Cohen (1973) \\(w_{ij} = 1 - \\frac{(i-j)^2}{(t-1)^2}\\)\nTratamiento muestral de los índices kappa\nSea: - n el número de individuos en la muestra.\n\n\n\\(p_ij\\) la proporción de individuos asignados a la categoría i por el observador X y a la categoría j por el observador Y, donde i, j =1,2,…,t.\n\n\\(p_{i.}\\) la proporción marginal de que un sujeto sea asignado a la clase i por el observador X (análogo para \\(p_{.j}\\), la clase j y el observador Y), donde i, j =1,2,…,t.\n\n\n\\(p_{i.}\\) = \\(p_{i1}\\) + … + \\(p_{it}\\)\n\n\n\\(p_{.j}\\) = \\(p_{1j}\\) + … + \\(p_{tj}\\)\n\n\n\n\nTenemos:\n\\[E(p_{ij}) = \\pi_{ij}, \\quad E(p_{i.}) = \\pi_{i.}, \\quad E(p_{.j}) = \\pi_{.j}\\]\nEl estimador del índice de kappa ponderado es:\n\\[\n\\kappa_w = \\frac{P_0 - P_c}{1 - P_c}, \\quad P_0 = \\sum_{i=1}^{t}\\sum_{j=1}^{t} w_{ij}p_{ij}, \\quad P_c = \\sum_{i=1}^{t}\\sum_{j=1}^{t} w_{ij}p_{i.}p_{.j}\n\\]\nEs un estimador con una distribución asintótica normal.\n\\[ \\kappa_w ~ N(\\kappa_w, \\sigma_{\\kappa_w})\\]\nTengo una distribucion normal centrada en k y al elegir un dato al azar (o sea, de la muestra aleatoria) puede tener un valor muy cercano o muy lejano de la media.\nLa varianza también es una estimación hecha con lo que yo he visto. Es otra estimación más. Esto hace que vayamos arrastrando aproximaciones AKA posibles errores.\n\\[\n\\displaylines{\n\\sigma^2_{\\kappa_w} = \\frac{\\sum_i \\sum_j \\pi_{ij} \\left[ w_{ij} - (\\overline{w}_{i \\cdot} + \\overline{w}_{\\cdot j})(1 - \\kappa_w) \\right]^2 - \\left[ \\kappa_w - \\Pi_c (1 - \\kappa_w) \\right]^2}{n (1 - \\Pi_c)^2} \\\\\n\\overline{w}_{i \\cdot} = \\sum_{j=1}^t \\pi_{\\cdot j} w_{ij} \\\\\n\\overline{w}_{\\cdot j} = \\sum_{i=1}^t \\pi_{i \\cdot} w_{ij}\n}\n\\]\nEl estimador de esta varianza su contrapartida muestral, sustituyendo probabilidades por proporciones y el índice kappa por su estimador.\n\\[\n\\displaylines{\n\\hat{\\sigma}^2_{\\kappa_w} = s^2_{\\kappa_w} = \\frac{\\sum_i \\sum_j p_{ij} \\left[ w_{ij} - (\\hat{w}_{i \\cdot} + \\hat{w}_{\\cdot j})(1 - \\hat{\\kappa}_w) \\right]^2 - \\left[ \\hat{\\kappa}_w - P_c (1 - \\hat{\\kappa}_w) \\right]^2}{n (1 - P_c)^2} \\\\\n\\hat{w}_{i \\cdot} = \\sum_{j=1}^t p_{\\cdot j} w_{ij} \\\\\n\\hat{w}_{\\cdot j} = \\sum_{i=1}^t p_{i \\cdot} w_{ij}\n}\n\\] Límites de confianza para el índice kappa con nivel de confianza \\(\\alpha\\):\n\\[ \\hat{\\kappa}_w +- z_\\alpha s_{\\hat{\\kappa}_w}\\]\nEscala de valoración del índice Kappa.\n\\[\n\\begin{array}{c|c}\n\\textbf{Kappa} & \\textbf{Grado de concordancia} \\\\\n\\hline\n&lt; 0.00 & \\text{Sin concordancia} \\\\\n0.00 - 0.20  & \\text{Insignificante} \\\\\n0.20 - 0.40  & \\text{Discreta} \\\\\n0.40 - 0.60  & \\text{Moderada} \\\\\n0.60 - 0.80  & \\text{Sustancial} \\\\\n0.80 - 1  & \\text{Casi perfecta} \\\\\n\\end{array}\n\\]\nEjemplo 1.3.\nDos radiólogos independientes informan de la presencia o ausencia de neumonía en 100 radiografías, siendo los resultados los siguientes.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Neumonía} & \\textbf{No neumonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Neumonía} & 4 & 6 & 10 \\\\\n                      & \\textbf{No neumonía} & 10 & 80 & 90 \\\\\n\\hline\n& \\textbf{Total} & 14 & 86 & 100 \\\\\n\\end{array}\n\\]\n\\(P_0 = \\frac{4+80}{100} = 0.84 \\quad \\quad P_c = \\frac{10*14 + 90*86}{100^2} = 0.788\\)\\(\\hat{\\kappa_w} = \\frac{0.84 - 0.788}{1 - 0.788} = 0.245\\)\\(s^2_{\\kappa_w} = 0.018 \\quad \\quad z_{0.05}s_{\\kappa_w} = 1.65 \\sqrt{0.018} = 0.22\\)\nSe podría hacer también un aproximación asintótica pero conlleva arrastrar una simulacion más.\nLa estimación puntual la tengo. Como no tengo una muestra más grande puedo remuestrear.\nEjemplo 1.4.\nDos psiquiatras evaluaron a 129 pacientes que habían sido diagnosticados previamente como clínicamente deprimidos. Las categorías de clasificación fueron: 0 para no deprimido, 1 para moderadamente deprimido y 2 para clínicamente deprimido. La tabla siguiente muestra los resultados de la clasificación realizada por los dos psiquiatras.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Psiquiatra 1} \\\\\n& & \\textbf{0} & \\textbf{1} & \\textbf{2} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Psiquiatra 2} & \\textbf{0} & 11 & 2 & 19 & 32 \\\\\n                      & \\textbf{1} & 1 & 3 & 3 & 7 \\\\\n                      & \\textbf{2} & 0 & 8 & 82 & 90 \\\\\n\\hline\n& \\textbf{Total} & 12 & 13 & 104 & 129 \\\\\n\\end{array}\n\\]\nDatos.\n\nCódigolibrary(psych)\n(A &lt;- matrix(c(11,2,19,1,3,3,0,8,82),ncol=3,byrow=TRUE))\n\n     [,1] [,2] [,3]\n[1,]   11    2   19\n[2,]    1    3    3\n[3,]    0    8   82\n\n\nPesos Ciccetti y Allison.\n\nCódigo(w_CA&lt;- matrix(c( 1,0.5,0, 0.5,1,0.5,0,0.5,1),ncol=3))\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.5  0.0\n[2,]  0.5  1.0  0.5\n[3,]  0.0  0.5  1.0\n\n\nEstimaciones de kappa.\n\nCódigokappa1 &lt;- cohen.kappa(A, w=w_CA, n.obs=sum(A), alpha=0.1)\nstr(kappa1)\n## List of 11\n##  $ kappa         : num 0.375\n##  $ weighted.kappa: num 0.402\n##  $ n.obs         : num 129\n##  $ agree         : num [1:3, 1:3] 0.08527 0.00775 0 0.0155 0.02326 ...\n##  $ weight        : num [1:3, 1:3] 1 0.5 0 0.5 1 0.5 0 0.5 1\n##  $ var.kappa     : num 0.00622\n##  $ var.weighted  : num 0.00688\n##  $ confid        : num [1:2, 1:3] 0.245 0.265 0.375 0.402 0.504 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:2] \"unweighted kappa\" \"weighted kappa\"\n##   .. ..$ : chr [1:3] \"lower\" \"estimate\" \"upper\"\n##  $ plevel        : num 0.1\n##  $ bad           : logi FALSE\n##  $ Call          : language cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels,      w.exp = w.exp)\n##  - attr(*, \"class\")= chr [1:2] \"psych\" \"kappa\"\n\n\n\nkappa1$kappa\n## [1] 0.3745225\nkappa1$weighted.kappa\n## [1] 0.4018192\nkappa1$var.kappa\n## [1] 0.006221038\nkappa1$var.weighted\n## [1] 0.006884677\nkappa1$confid\n##                      lower  estimate     upper\n## unweighted kappa 0.2447870 0.3745225 0.5042579\n## weighted kappa   0.2653391 0.4018192 0.5382992\n\nPesos de Fleiss y Cohen.\n\n(w_FC&lt;- matrix(c( 1,0.75,0,0.75,1,0.75,0,0.75,1),ncol=3))\n\n     [,1] [,2] [,3]\n[1,] 1.00 0.75 0.00\n[2,] 0.75 1.00 0.75\n[3,] 0.00 0.75 1.00\n\n\nEstimaciones de kappa.\n\nCódigokappa2 &lt;- cohen.kappa(A, w=w_FC, n.obs=sum(A),alpha=0.1)\nstr(kappa2)\n## List of 11\n##  $ kappa         : num 0.375\n##  $ weighted.kappa: num 0.42\n##  $ n.obs         : num 129\n##  $ agree         : num [1:3, 1:3] 0.08527 0.00775 0 0.0155 0.02326 ...\n##  $ weight        : num [1:3, 1:3] 1 0.75 0 0.75 1 0.75 0 0.75 1\n##  $ var.kappa     : num 0.00622\n##  $ var.weighted  : num 0.00796\n##  $ confid        : num [1:2, 1:3] 0.245 0.274 0.375 0.42 0.504 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:2] \"unweighted kappa\" \"weighted kappa\"\n##   .. ..$ : chr [1:3] \"lower\" \"estimate\" \"upper\"\n##  $ plevel        : num 0.1\n##  $ bad           : logi FALSE\n##  $ Call          : language cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels,      w.exp = w.exp)\n##  - attr(*, \"class\")= chr [1:2] \"psych\" \"kappa\"\n\n\n\nkappa2$weighted.kappa\n## [1] 0.4203694\nkappa2$var.weighted\n## [1] 0.007955659\nkappa2$confid\n##                      lower  estimate     upper\n## unweighted kappa 0.2447870 0.3745225 0.5042579\n## weighted kappa   0.2736575 0.4203694 0.5670813\n\n\\[\n\\begin{array}{c|c}\n\\textbf{Estadístico} & \\textbf{Estimación} & \\textbf{Varianza estimada} & LI_{90} \\\\\n\\hline\n\\text{Kappa sin ponderar} & 0.3745225 & 0.006221038 & 0.2447870 \\\\\n\\text{Kappa ponderado (Ciccetti-Allison)} & 0.4018192 & 0.006884677 & 0.2653391 \\\\\n\\text{Kappa ponderado (Fleiss-Cohen)} & 04203694 & 0007955659 & 0.2736575 \\\\\n\\end{array}\n\\]\nDepués de remuestrar tengo las 23 radiografías con la clasificación de los dos radiólogos cuando remuestre.\nObtengo muestras bootstrap con 23 radiografias pero la clasificación es la misma para cada radiografía.\nCalculo el kappa \\(\\Rightarrow\\) calculo las diferencias \\(\\Rightarrow\\) Despercio las más alejadas \\(\\Rightarrow\\) Me quedo con el 95 % central.\nSi hay más de 2 observadores \\(\\Rightarrow\\) Índice de Kappa para múltiples observadores (Fleiss JL Statistical Methods for Rates and Proportions, 2003)\n\n\n\n      \n         1. Introducción ✓\n                \n  \n  \n      \n        2.1. Continuación ✗"
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#interpretación-del-índice-kappa-si-k0",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#interpretación-del-índice-kappa-si-k0",
    "title": "2. Análisis de concordancia en variables categóricas ✗",
    "section": "Interpretación del índice kappa (si \\(k\\)>0)",
    "text": "Interpretación del índice kappa (si \\(k\\)&gt;0)\nLos datos observados son un caso particular.\nLa probabilidad de acuerdo observado es una media ponderada del máximo acuerdo (1) y del acuerdo debido al azar (\\(Pi_{c\\)), siendo \\(k\\) el peso del máximo acuerdo.\n\n\\[\n\\displaylines{\nk = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c} \\\\ \\\\\nk * (1-\\Pi_{i}) = (\\Pi_{0} - \\Pi_{c}) \\\\ \\\\\n\\Pi_{0} = k + (1-k)*\\Pi_{c}\n}\n\\]\nEl acuerdo observado es una combinación de la concordania perfecta y concordania debida al azar. El ínidce kappa da más peso a uno o al otro.\n\nEjemplo 1.1.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.04 & 0.06 & 0.10 \\\\\n                      & \\textbf{No pulmonía} & 0.10 & 0.80 & 0.90 \\\\\n\\hline\n& \\textbf{Total} & 0.14 & 0.86 & 1 \\\\\n\\end{array}\n\\]\n\ndata &lt;- matrix(c(0.04, 0.06, 0.10, 0.80), nrow = 2, byrow = TRUE,\n               dimnames = list(c(\"Pulmonía B\", \"No pulmonía B\"),\n                               c(\"Pulmonía A\", \"No pulmonía A\")))\ndata\n\n              Pulmonía A No pulmonía A\nPulmonía B          0.04          0.06\nNo pulmonía B       0.10          0.80\n\n\n\ndf &lt;- data.frame(col1 = c(0.04, 0.10),\n                 col2 = c(0.06, 0.80)\n)\ncolnames(df) &lt;- c(\"Pulmonía A\", \"No pulmonía A\")\nrownames(df) &lt;- c(\"Pulmonía B\", \"No pulmonía B\")\ndf\n\n              Pulmonía A No pulmonía A\nPulmonía B          0.04          0.06\nNo pulmonía B       0.10          0.80\n\n\n\\(\\Pi_{0} = \\text{\"concordancia absoluta\"} = 0.04 + 0.80 = 0.84\\)\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n\n[1] 0.84\n\n\n\n(pi_0 &lt;- df[1,1] + df[2,2])\n\n[1] 0.84\n\n\n\n(pi_0 &lt;- df[\"Pulmonía B\",\"Pulmonía A\"] + df[\"No pulmonía B\",\"No pulmonía A\"])\n\n[1] 0.84\n\n\n\\(\\Pi_{1} = \\text{\"concordancia debida al azar\"} = 0.10*0.14 + 0.86*0.90 = 0.788\\)\n\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n\n[1] 0.788\n\n\n\n(pi_1 &lt;- sum(df[1]) * sum(df[1,])  + sum(df[2]) * sum(df[2,]))\n\n[1] 0.788\n\n\n\n(pi_1 &lt;- sum(data[1,])*sum(data[,1]) + sum(data[2,])*sum(data[,2]))\n\n[1] 0.788\n\n\nInterpretación.]{underline}\nLa concordancia observada en los informes (84%) está compuesta por un 24,5% de la concordancia máxima y un 78,8% de la esperada al azar."
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#clasificaciones-multinomiales",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#clasificaciones-multinomiales",
    "title": "2. Análisis de concordancia en variables categóricas ✗",
    "section": "Clasificaciones multinomiales",
    "text": "Clasificaciones multinomiales\n\\[\n\\begin{array}{c|c c c c}\n    & & \\textbf{Y} & & \\\\\n    & \\textbf{Categoría} & \\textbf{1} & \\textbf{...} & \\textbf{t} & \\textbf{Total} \\\\\n\\hline\n\\textbf{X} & \\textbf{1} & \\pi_{11} & ... & \\pi_{1t} & \\pi_{1\\cdot} \\\\\n           & \\textbf{2} & \\pi_{21} & ... & \\pi_{2t} & \\pi_{2\\cdot} \\\\\n           & \\textbf{...} & ... & ... & ... & ... \\\\\n           & \\textbf{t} & \\pi_{2t} & ... & \\pi_{tt} & \\pi_{t\\cdot} \\\\\n\\hline\n& \\textbf{Total} & \\pi_{\\cdot1} & ... & \\pi_{\\cdot t} & 1 \\\\\n\\end{array}\n\\]\n\nMedida de concordancia global: \\(\\Pi_0 = \\sum_{i=1}^{t} \\pi_{ii}\\)\n\n\nMedida de concordancia debida al azar: \\(\\Pi_c = \\sum_{i=1}^{t} \\pi_{i.}\\pi_{.i}\\)\n\n\nÍndice kappa: \\(k = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c}\\)\n\nHay veces que nos interesa saber si la concordancia es mayor en unas categorías o en otras. Por ejemplo, en la categoría grave hay muchas concordancia pero en las categorías leve y media no. En este caso se suele estandarizar para tener dos categorías y medir concordancia para cada categoría.\nEjemplo 1.2.\n\ndf &lt;- data.frame(rbind(c(0.1125, 0.1, 0.0375),\n                       c(0.1125, 0.3625, 0.0625),\n                       c(0, 0.0375, 0.175))\n)\ncolnames(df) &lt;- c(\"Leve\", \"Moderada\", \"Grave\")\nrownames(df) &lt;- c(\"Leve\", \"Moderada\", \"Grave\")\ndf\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\n\n\n\ndf_extended &lt;- rbind(df, \"Total\" = colSums(df))\ndf_extended\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\nTotal    0.2250   0.5000 0.2750\n\n\n\ndf_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\ndf_extended\n\n           Leve Moderada  Grave  Total\nLeve     0.1125   0.1000 0.0375 0.2500\nModerada 0.1125   0.3625 0.0625 0.5375\nGrave    0.0000   0.0375 0.1750 0.2125\nTotal    0.2250   0.5000 0.2750 1.0000\n\n\nMedida de concordancia global\n\n(pi_0 &lt;- sum(diag(as.matrix(df))))\n\n[1] 0.65\n\n\nMedida de concordancia debida al azar\n\npi_1 &lt;- 0\nt &lt;- dim(df_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_extended[i,t]*df_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\n[1] 0.3834375\n\n\nÍndice kappa\n\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] 0.4323365\n\n\nEl índice kappa no mide la distancia entre las discordancias. No es lo mismo decir (X=7, Y=8) que (X=7, Y=2).\nComparando cada categoría con el resto\n\ndf\n\n           Leve Moderada  Grave\nLeve     0.1125   0.1000 0.0375\nModerada 0.1125   0.3625 0.0625\nGrave    0.0000   0.0375 0.1750\n\n\n\nCódigocat &lt;- \"Leve\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Leve\"\n        Leve  Otras Total\nLeve  0.1125 0.1375  0.25\nOtras 0.1125 0.6375  0.75\nTotal 0.2250 0.7750  1.00\n[1] \"Medida de concordancia global:\"\n[1] 0.75\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.6375\n[1] \"Índice kappa:\"\n[1] 0.3103448\n\n\n\nCódigocat &lt;- \"Moderada\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Moderada\"\n         Moderada Otras  Total\nModerada   0.3625 0.175 0.5375\nOtras      0.1375 0.325 0.4625\nTotal      0.5000 0.500 1.0000\n[1] \"Medida de concordancia global:\"\n[1] 0.6875\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.5\n[1] \"Índice kappa:\"\n[1] 0.375\n\n\n\nCódigocat &lt;- \"Grave\"\nprint(paste(\"Categoría de referencia:\", cat))\n\ndf_new &lt;- df |&gt; mutate(\"Otras\" = df[,names(df)[!names(df) %in% cat]] |&gt; rowSums()) |&gt; select(-c( names(df)[!names(df) %in% cat]))\n\ndf_new[\"Otras\",] &lt;- df_new %&gt;% filter(!row.names(df_new) %in% c(cat)) |&gt; colSums()\ndf_new &lt;- df_new |&gt; filter(rownames(df_new) %in% c(cat,\"Otras\"))\n\ndf_extended &lt;- rbind(df_new, \"Total\" = colSums(df_new))\ndf_new_extended &lt;- df_extended |&gt; mutate(\"Total\" = rowSums(df_extended)) # arreglar lo de las librerías\n(df_new_extended)\n\nprint(\"Medida de concordancia global:\")\n(pi_0 &lt;- sum(diag(as.matrix(df_new))))\n\nprint(\"Medida de concordancia debida al azar:\")\npi_1 &lt;- 0\nt &lt;- dim(df_new_extended)[1]\nfor (i in seq(1:(t-1))){\n  pi_1 &lt;- pi_1 + df_new_extended[i,t]*df_new_extended[t,i]\n}\n(pi_1 &lt;- pi_1 |&gt; unname())\n\nprint(\"Índice kappa:\")\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n\n[1] \"Categoría de referencia: Grave\"\n      Grave  Otras  Total\nGrave 0.175 0.0375 0.2125\nOtras 0.100 0.6875 0.7875\nTotal 0.275 0.7250 1.0000\n[1] \"Medida de concordancia global:\"\n[1] 0.8625\n[1] \"Medida de concordancia debida al azar:\"\n[1] 0.629375\n[1] \"Índice kappa:\"\n[1] 0.6290051\n\n\nHasta ahora hemos hablado de probabilidades ya que estamos con el modelo teórico. Cuando trabajamoes con datos de una muestra hablaremos de porcentajes. \\(\\Rightarrow\\) “Este % es consecuencia de un acuerdo de tal y un azar de no sé cuántos”\nEl índice kappa tiene una distribución teórica y lo estimo según una muestra. Con una poca información (la muestra) doy un intervalo. No doy intervalos de confianza para valores desconocidos sino para las estimaciones."
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#inconvenientes-del-índice-kappa",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#inconvenientes-del-índice-kappa",
    "title": "2. Análisis de concordancia en variables categóricas ✗",
    "section": "Inconvenientes del índice kappa",
    "text": "Inconvenientes del índice kappa\nEjemplo 1.1.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.04 & 0.06 & 0.10 \\\\\n                      & \\textbf{No pulmonía} & 0.10 & 0.80 & 0.90 \\\\\n\\hline\n& \\textbf{Total} & 0.14 & 0.86 & 1 \\\\\n\\end{array}\n\\]\nLa prevalencia de pulmonía es baja: 014 para el radiólogo A y 0.1 para el rediólogo B.\n**Ambas marginales desequilibradas*: la prevalencia observada dista mucho de 0.5 (a favor de los diagnósticos negativos)\nEl bajo valor del índice kappa se explica porque nos encontramos en el peor de los escenarios: baja prevalencia y marginales desequilibradas.\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.788\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.245283\n\nEl valor del índice kappa depende de la prevalencia de la característica observada.\nEjemplo 1.1.a.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.30 & 0.06 & 0.36 \\\\\n                      & \\textbf{No pulmonía} & 0.10 & 0.54 & 0.64 \\\\\n\\hline\n& \\textbf{Total} & 0.40 & 0.60 & 1 \\\\\n\\end{array}\n\\]\nAumentamos la prevalencia: 0.40 para el radiólogo A y 0.36 para el rediólogo B.\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.528\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.6610169\n\n\nCuanto más cercana a 0,5 sea la prevalencia (más equilibradas estén las marginales) mayor es el índice kappa, para igual probabilidad de acuerdos observados.\n\n\\[\\Downarrow\\]\n\nPrevalencias muy altas o muy bajas penalizan el índice kappa.\n\nEl valor del índice kappa depende de la simetría y homogeneidad de las marginales.\nEn el primer caso el comportamiento de los agentes es homogéneo: pues ambos emiten informes positivos con mayor frecuencia.\nEn el segundo caso el comportamiento de los agentes es heterogéneo y asimétrico.\nEn ambos casos los diagnósticos del agente A distan 0.2 de 0.5.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.45 & 0.15 & 0.60 \\\\\n                      & \\textbf{No pulmonía} & 0.25 & 0.15 & 0.40 \\\\\n\\hline\n& \\textbf{Total} & 0.70 & 0.30 & 1 \\\\\n&  & 0.50+0.20 & 0.50-0.20 &  \\\\\n\\end{array}\n\\]\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.54\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.6521739\n\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Pulmonía} & \\textbf{No pulmonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Pulmonía} & 0.25 & 0.35 & 0.60 \\\\\n                      & \\textbf{No pulmonía} & 0.05 & 0.35 & 0.40 \\\\\n\\hline\n& \\textbf{Total} & 0.30 & 0.70 & 1 \\\\\n&  & 0.50-0.20 & 0.50+0.20 &  \\\\\n\\end{array}\n\\]\n\n(pi_0 &lt;- data[1,1] + data[2,2])\n## [1] 0.84\n(pi_1 &lt;- sum(df[\"Pulmonía A\"]) * sum(df[\"Pulmonía B\",])  + sum(df[\"No pulmonía A\"]) * sum(df[\"No pulmonía B\",]))\n## [1] 0.46\n(k &lt;- ((pi_0 - pi_1) / (1 - pi_1)) |&gt; unname())\n## [1] 0.7037037\n\nEl valor del índice kappa depende del equilibrio de las marginales.\nCuanto mayor sea la diferencia de la prevalencia observada de cada agente respecto de 0.5 mayor es el índice kappa. (incluso para un mismo valor de acuerdos observados)."
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#índice-kappa-ponderado.",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#índice-kappa-ponderado.",
    "title": "2. Análisis de concordancia en variables categóricas ✗",
    "section": "Índice kappa ponderado.",
    "text": "Índice kappa ponderado.\nSólo tiene sentido para variables ordinales.\nEstá diseñado para recoger la idea de que algunas discordancias son más severas que otras y, por tanto, asigna pesos que representan la importancia entre los desacuerdos. El máximo peso se da a la concordancia perfecta y pesos proporcionalmente menores según la importancia del desacuerdo.\nNo tiene la misma importancia un desacuerdo en la clasificación entre las categorías leve y moderada que entre leve y grave, obviamente la última representa un mayor desacuerdo que la primera.\n\nMedida de concordancia global: \\(\\Pi_0 = \\sum_{i=1}^{t} \\sum_{j=1}^{t} w_{ij} \\pi_{ij}\\)\n\n\nMedida de concordancia debida al azar: \\(\\Pi_c = \\sum_{i=1}^{t} \\sum_{j=1}^{t} w_{ij} \\pi_{i.}\\pi_{.j}\\)\n\n\nÍndice kappa: \\(k_{w} = \\frac{\\Pi_0-\\Pi_c}{1-\\Pi_c}\\)\n\nLos pesos satisfacen:\n\\[\nw_{ij} = \\begin{cases}\nw_{ii} = 1 \\\\\n0 \\le w_{ij} \\le 1 \\\\\nw_{ij} = w_{ji}\n\\end{cases}\n\\]\n\\[\nw_{ij} = \\begin{cases}\n1, \\hspace{1em} i = j \\\\\n0, \\hspace{1em} i \\neq j \\\\\n\\end{cases} \\hspace{1em} \\text{sii} \\hspace{1em} k_{w} = k \\\\\n\\] \\[\n\\text{si t = 2} \\Rightarrow k_{w} = k\n\\]\nPonderaciones.\n\nCicchetti-Allison (1971) \\(w_{ij} = 1 - \\frac{|i-j|}{t-1}\\)\nFleiss-Cohen (1973) \\(w_{ij} = 1 - \\frac{(i-j)^2}{(t-1)^2}\\)"
  },
  {
    "objectID": "tema_01/tema_01_2_concordancia_categoricas.html#tratamiento-muestral-de-los-índices-kappa",
    "href": "tema_01/tema_01_2_concordancia_categoricas.html#tratamiento-muestral-de-los-índices-kappa",
    "title": "2. Análisis de concordancia en variables categóricas ✗",
    "section": "Tratamiento muestral de los índices kappa",
    "text": "Tratamiento muestral de los índices kappa\nSea: - n el número de individuos en la muestra.\n\n\n\\(p_ij\\) la proporción de individuos asignados a la categoría i por el observador X y a la categoría j por el observador Y, donde i, j =1,2,…,t.\n\n\\(p_{i.}\\) la proporción marginal de que un sujeto sea asignado a la clase i por el observador X (análogo para \\(p_{.j}\\), la clase j y el observador Y), donde i, j =1,2,…,t.\n\n\n\\(p_{i.}\\) = \\(p_{i1}\\) + … + \\(p_{it}\\)\n\n\n\\(p_{.j}\\) = \\(p_{1j}\\) + … + \\(p_{tj}\\)\n\n\n\n\nTenemos:\n\\[E(p_{ij}) = \\pi_{ij}, \\quad E(p_{i.}) = \\pi_{i.}, \\quad E(p_{.j}) = \\pi_{.j}\\]\nEl estimador del índice de kappa ponderado es:\n\\[\n\\kappa_w = \\frac{P_0 - P_c}{1 - P_c}, \\quad P_0 = \\sum_{i=1}^{t}\\sum_{j=1}^{t} w_{ij}p_{ij}, \\quad P_c = \\sum_{i=1}^{t}\\sum_{j=1}^{t} w_{ij}p_{i.}p_{.j}\n\\]\nEs un estimador con una distribución asintótica normal.\n\\[ \\kappa_w ~ N(\\kappa_w, \\sigma_{\\kappa_w})\\]\nTengo una distribucion normal centrada en k y al elegir un dato al azar (o sea, de la muestra aleatoria) puede tener un valor muy cercano o muy lejano de la media.\nLa varianza también es una estimación hecha con lo que yo he visto. Es otra estimación más. Esto hace que vayamos arrastrando aproximaciones AKA posibles errores.\n\\[\n\\displaylines{\n\\sigma^2_{\\kappa_w} = \\frac{\\sum_i \\sum_j \\pi_{ij} \\left[ w_{ij} - (\\overline{w}_{i \\cdot} + \\overline{w}_{\\cdot j})(1 - \\kappa_w) \\right]^2 - \\left[ \\kappa_w - \\Pi_c (1 - \\kappa_w) \\right]^2}{n (1 - \\Pi_c)^2} \\\\\n\\overline{w}_{i \\cdot} = \\sum_{j=1}^t \\pi_{\\cdot j} w_{ij} \\\\\n\\overline{w}_{\\cdot j} = \\sum_{i=1}^t \\pi_{i \\cdot} w_{ij}\n}\n\\]\nEl estimador de esta varianza su contrapartida muestral, sustituyendo probabilidades por proporciones y el índice kappa por su estimador.\n\\[\n\\displaylines{\n\\hat{\\sigma}^2_{\\kappa_w} = s^2_{\\kappa_w} = \\frac{\\sum_i \\sum_j p_{ij} \\left[ w_{ij} - (\\hat{w}_{i \\cdot} + \\hat{w}_{\\cdot j})(1 - \\hat{\\kappa}_w) \\right]^2 - \\left[ \\hat{\\kappa}_w - P_c (1 - \\hat{\\kappa}_w) \\right]^2}{n (1 - P_c)^2} \\\\\n\\hat{w}_{i \\cdot} = \\sum_{j=1}^t p_{\\cdot j} w_{ij} \\\\\n\\hat{w}_{\\cdot j} = \\sum_{i=1}^t p_{i \\cdot} w_{ij}\n}\n\\] Límites de confianza para el índice kappa con nivel de confianza \\(\\alpha\\):\n\\[ \\hat{\\kappa}_w +- z_\\alpha s_{\\hat{\\kappa}_w}\\]\nEscala de valoración del índice Kappa.\n\\[\n\\begin{array}{c|c}\n\\textbf{Kappa} & \\textbf{Grado de concordancia} \\\\\n\\hline\n&lt; 0.00 & \\text{Sin concordancia} \\\\\n0.00 - 0.20  & \\text{Insignificante} \\\\\n0.20 - 0.40  & \\text{Discreta} \\\\\n0.40 - 0.60  & \\text{Moderada} \\\\\n0.60 - 0.80  & \\text{Sustancial} \\\\\n0.80 - 1  & \\text{Casi perfecta} \\\\\n\\end{array}\n\\]\nEjemplo 1.3.\nDos radiólogos independientes informan de la presencia o ausencia de neumonía en 100 radiografías, siendo los resultados los siguientes.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Radiólogo A} \\\\\n& & \\textbf{Neumonía} & \\textbf{No neumonía} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Radiólogo B} & \\textbf{Neumonía} & 4 & 6 & 10 \\\\\n                      & \\textbf{No neumonía} & 10 & 80 & 90 \\\\\n\\hline\n& \\textbf{Total} & 14 & 86 & 100 \\\\\n\\end{array}\n\\]\n\\(P_0 = \\frac{4+80}{100} = 0.84 \\quad \\quad P_c = \\frac{10*14 + 90*86}{100^2} = 0.788\\)\\(\\hat{\\kappa_w} = \\frac{0.84 - 0.788}{1 - 0.788} = 0.245\\)\\(s^2_{\\kappa_w} = 0.018 \\quad \\quad z_{0.05}s_{\\kappa_w} = 1.65 \\sqrt{0.018} = 0.22\\)\nSe podría hacer también un aproximación asintótica pero conlleva arrastrar una simulacion más.\nLa estimación puntual la tengo. Como no tengo una muestra más grande puedo remuestrear.\nEjemplo 1.4.\nDos psiquiatras evaluaron a 129 pacientes que habían sido diagnosticados previamente como clínicamente deprimidos. Las categorías de clasificación fueron: 0 para no deprimido, 1 para moderadamente deprimido y 2 para clínicamente deprimido. La tabla siguiente muestra los resultados de la clasificación realizada por los dos psiquiatras.\n\\[\n\\begin{array}{cc|ccc}\n& & \\textbf{Psiquiatra 1} \\\\\n& & \\textbf{0} & \\textbf{1} & \\textbf{2} & \\textbf{Total} \\\\\n\\hline\n\\textbf{Psiquiatra 2} & \\textbf{0} & 11 & 2 & 19 & 32 \\\\\n                      & \\textbf{1} & 1 & 3 & 3 & 7 \\\\\n                      & \\textbf{2} & 0 & 8 & 82 & 90 \\\\\n\\hline\n& \\textbf{Total} & 12 & 13 & 104 & 129 \\\\\n\\end{array}\n\\]\nDatos.\n\nCódigolibrary(psych)\n(A &lt;- matrix(c(11,2,19,1,3,3,0,8,82),ncol=3,byrow=TRUE))\n\n     [,1] [,2] [,3]\n[1,]   11    2   19\n[2,]    1    3    3\n[3,]    0    8   82\n\n\nPesos Ciccetti y Allison.\n\nCódigo(w_CA&lt;- matrix(c( 1,0.5,0, 0.5,1,0.5,0,0.5,1),ncol=3))\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.5  0.0\n[2,]  0.5  1.0  0.5\n[3,]  0.0  0.5  1.0\n\n\nEstimaciones de kappa.\n\nCódigokappa1 &lt;- cohen.kappa(A, w=w_CA, n.obs=sum(A), alpha=0.1)\nstr(kappa1)\n## List of 11\n##  $ kappa         : num 0.375\n##  $ weighted.kappa: num 0.402\n##  $ n.obs         : num 129\n##  $ agree         : num [1:3, 1:3] 0.08527 0.00775 0 0.0155 0.02326 ...\n##  $ weight        : num [1:3, 1:3] 1 0.5 0 0.5 1 0.5 0 0.5 1\n##  $ var.kappa     : num 0.00622\n##  $ var.weighted  : num 0.00688\n##  $ confid        : num [1:2, 1:3] 0.245 0.265 0.375 0.402 0.504 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:2] \"unweighted kappa\" \"weighted kappa\"\n##   .. ..$ : chr [1:3] \"lower\" \"estimate\" \"upper\"\n##  $ plevel        : num 0.1\n##  $ bad           : logi FALSE\n##  $ Call          : language cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels,      w.exp = w.exp)\n##  - attr(*, \"class\")= chr [1:2] \"psych\" \"kappa\"\n\n\n\nkappa1$kappa\n## [1] 0.3745225\nkappa1$weighted.kappa\n## [1] 0.4018192\nkappa1$var.kappa\n## [1] 0.006221038\nkappa1$var.weighted\n## [1] 0.006884677\nkappa1$confid\n##                      lower  estimate     upper\n## unweighted kappa 0.2447870 0.3745225 0.5042579\n## weighted kappa   0.2653391 0.4018192 0.5382992\n\nPesos de Fleiss y Cohen.\n\n(w_FC&lt;- matrix(c( 1,0.75,0,0.75,1,0.75,0,0.75,1),ncol=3))\n\n     [,1] [,2] [,3]\n[1,] 1.00 0.75 0.00\n[2,] 0.75 1.00 0.75\n[3,] 0.00 0.75 1.00\n\n\nEstimaciones de kappa.\n\nCódigokappa2 &lt;- cohen.kappa(A, w=w_FC, n.obs=sum(A),alpha=0.1)\nstr(kappa2)\n## List of 11\n##  $ kappa         : num 0.375\n##  $ weighted.kappa: num 0.42\n##  $ n.obs         : num 129\n##  $ agree         : num [1:3, 1:3] 0.08527 0.00775 0 0.0155 0.02326 ...\n##  $ weight        : num [1:3, 1:3] 1 0.75 0 0.75 1 0.75 0 0.75 1\n##  $ var.kappa     : num 0.00622\n##  $ var.weighted  : num 0.00796\n##  $ confid        : num [1:2, 1:3] 0.245 0.274 0.375 0.42 0.504 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:2] \"unweighted kappa\" \"weighted kappa\"\n##   .. ..$ : chr [1:3] \"lower\" \"estimate\" \"upper\"\n##  $ plevel        : num 0.1\n##  $ bad           : logi FALSE\n##  $ Call          : language cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels,      w.exp = w.exp)\n##  - attr(*, \"class\")= chr [1:2] \"psych\" \"kappa\"\n\n\n\nkappa2$weighted.kappa\n## [1] 0.4203694\nkappa2$var.weighted\n## [1] 0.007955659\nkappa2$confid\n##                      lower  estimate     upper\n## unweighted kappa 0.2447870 0.3745225 0.5042579\n## weighted kappa   0.2736575 0.4203694 0.5670813\n\n\\[\n\\begin{array}{c|c}\n\\textbf{Estadístico} & \\textbf{Estimación} & \\textbf{Varianza estimada} & LI_{90} \\\\\n\\hline\n\\text{Kappa sin ponderar} & 0.3745225 & 0.006221038 & 0.2447870 \\\\\n\\text{Kappa ponderado (Ciccetti-Allison)} & 0.4018192 & 0.006884677 & 0.2653391 \\\\\n\\text{Kappa ponderado (Fleiss-Cohen)} & 04203694 & 0007955659 & 0.2736575 \\\\\n\\end{array}\n\\]\nDepués de remuestrar tengo las 23 radiografías con la clasificación de los dos radiólogos cuando remuestre.\nObtengo muestras bootstrap con 23 radiografias pero la clasificación es la misma para cada radiografía.\nCalculo el kappa \\(\\Rightarrow\\) calculo las diferencias \\(\\Rightarrow\\) Despercio las más alejadas \\(\\Rightarrow\\) Me quedo con el 95 % central.\nSi hay más de 2 observadores \\(\\Rightarrow\\) Índice de Kappa para múltiples observadores (Fleiss JL Statistical Methods for Rates and Proportions, 2003)"
  },
  {
    "objectID": "tema_01/tema_01_1_cont.html",
    "href": "tema_01/tema_01_1_cont.html",
    "title": "2.1. Continuación ✗",
    "section": "",
    "text": "los estimadores por intervalo miden la precision de esa est puntual. como de buena es esa estimacion puntual para representar otra posible muestra que podríamos haber obtenido.\nuna estimación puntual que se encuentra en alguno de los intervalos de Landis y Koch.\nLa est puntual da moderada y el IC tb está en moderado. el ic respalda/valida la estimacion puntual. tb puede ser que el Ic abarque dos niveles, bastará con explicarlo y yastá.\ndiapo23\n0,245 —&gt; estamos dándole a la concordancia un grado discreto. (est puntual)\nla validez de la estimacion hay que refrentarla con un IC.\nen el 90 % de estudios similares a este tenemos una concordancia iría desde significante a moderada. la validaez de nuestra estimacion no es super cool, es cuestionable. porque hay otros estudios donde podemos encontrar una concordancia diferente a la que hemos calculado en esta muestra.\nH0: las opiniones son independientes (que no hay relación entre las opiniones) (si hay una concordancia que sea casual) AKA rho=0\n¿entonces en el ejemplo del artículo? porque una conclusión que sacan es relación entre profesionales experimentados\npara poder admitir rho=0 el IC tendría que darlo como un valor posible y en este caso no lo da.\nrechazamos que haya un criterio diferente. (juzgamos que los dos puntúen por igual)\nindice kappa sin ponderar: le damos importancia a todo lo que coincide pero no le damos peso a la cercanía entre dos diagnósticos. esta elección por el kappa ponderado es crucial cuando los niveles de clasificación son mayores a dos niveles.\níndice kappa en R:\npesos por defecto: aquello que hacen lo contrario a lo que quiero en la diagonal pone peso 0 (yo quiero que en la diagonal tiene 1 y conforme me alejo lo voy bajando)\n\nlibrary(psych)\n A&lt;- matrix(c(11,2,19,1,3,3,0,8,82), ncol=3, byrow=TRUE)\n#Pesos Ciccetti y Allison\nwCA&lt;- matrix(c( 1,0.5,0,\n                0.5,1,0.5,\n                0,0.5,1),ncol=3)\n\n#Asignamos nombre al objeto\nkappa1 &lt;- cohen.kappa(A, w=wCA, n.obs=129, alpha=0.1)\n\n\n#Para que salga la estructura del objeto\nstr(kappa1)\n\nList of 11\n $ kappa         : num 0.375\n $ weighted.kappa: num 0.402\n $ n.obs         : num 129\n $ agree         : num [1:3, 1:3] 0.08527 0.00775 0 0.0155 0.02326 ...\n $ weight        : num [1:3, 1:3] 1 0.5 0 0.5 1 0.5 0 0.5 1\n $ var.kappa     : num 0.00622\n $ var.weighted  : num 0.00688\n $ confid        : num [1:2, 1:3] 0.245 0.265 0.375 0.402 0.504 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"unweighted kappa\" \"weighted kappa\"\n  .. ..$ : chr [1:3] \"lower\" \"estimate\" \"upper\"\n $ plevel        : num 0.1\n $ bad           : logi FALSE\n $ Call          : language cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels,      w.exp = w.exp)\n - attr(*, \"class\")= chr [1:2] \"psych\" \"kappa\"\n\n#Para obtener un valor individual del objeto\nkappa1$kappa\n\n[1] 0.3745225\n\nkappa1$weighted.kappa\n\n[1] 0.4018192\n\nkappa1$var.kappa\n\n[1] 0.006221038\n\nkappa1$var.weighted\n\n[1] 0.006884677\n\nkappa1$confid\n\n                     lower  estimate     upper\nunweighted kappa 0.2447870 0.3745225 0.5042579\nweighted kappa   0.2653391 0.4018192 0.5382992\n\n#Pesos de Fleiss y Cohen\nwFC&lt;- matrix(c( 1,0.75,0,\n                0.75,1,0.75,\n                0,0.75,1),ncol=3)\nkappa2 &lt;- cohen.kappa(A,w=wFC,n.obs=129,alpha=0.1)\nkappa2$weighted.kappa\n\n[1] 0.4203694\n\nkappa2$var.weighted\n\n[1] 0.007955659\n\nkappa2$confid\n\n                     lower  estimate     upper\nunweighted kappa 0.2447870 0.3745225 0.5042579\nweighted kappa   0.2736575 0.4203694 0.5670813\n\n\nestamos teniendo resultados 0.3 / .,4, las estimaciones puntuales nos incitan a concordancias moderadas\nde discreta a moderada\nno podemos decir que no tengan un criterio común. algo tienen.\n¿qué pasha cuando tenemos que analizar la validez de una estimación pero tenemos una muestra pequeña? po a la metodología booststrap\nnos vamos a tema3, página 55\n20 radiografías 1. k^0 :: estimacion con la muestra inicial n=20. validacion con la metodlogía bootstrap remuestreo entre las 20 diapos, cada diapo tiene 2 clasificaciones para cada muestra tengo una estimación de k^b1 haste tener k^b1000, tendré mil valores de mis estimaciones\nvamos a analizar las diferencias entre la estimacion de la m0 contra cada una de estas estimaciones.\nconcordancia desde insignificante hasta sustancial muy muy muy válida la estimación no es no es pq la hayamos hecho mal, sino porque no podemos defender a muerte nuesta estimacion puntual lo que sí podemos decir es que algún criterio común tienen"
  },
  {
    "objectID": "tema_01/tema_01_3_concordancia_numericas.html#concordancia-entre-las-puntuaciones-dadas-a-un-conjunto-de-individuos",
    "href": "tema_01/tema_01_3_concordancia_numericas.html#concordancia-entre-las-puntuaciones-dadas-a-un-conjunto-de-individuos",
    "title": "3. Análisis de concordancia en variables numéricas",
    "section": "Concordancia entre las puntuaciones dadas a un conjunto de individuos",
    "text": "Concordancia entre las puntuaciones dadas a un conjunto de individuos\nPara establecer comparaciones necesito medidas que no tengan unidades.\nMedidas absolutas\nLas medidas absolutas tienen las mismas unidades que la variable respuesta.\n\n\nDesviación cuadrática media.\n\n\\(\\symbf{RMSE = \\sqrt{\\frac{1}{n}\\sum_{i}{x_i}^2}}\\)\n\n\n\nÍndice de desviación total.\n\nSe utiliza principalmente cuando se quiere evaluar la precisión y exactitud de un método con respecto a otro.\nResponde a cuál es el valor máximo de la desviación absoluta que se puede esperar entre dos mediciones con un cierto nivel de confianza.\n\n\n\\(\\symbf{TDI = P(|X_1 - X_2| \\leq \\delta) = 1 - \\alpha}\\)\n\n\n\\(X_1, X_2\\) son las mediciones a comparar.\n\n\n\\(\\delta\\) es el valor máximo de desviación absoluta.\n\n\n\\(\\alpha\\) es el nivel de significancia o el complemento del nivel de confianza.\n\n\nSi el TDI es bajo, indica que las dos mediciones (o el método y el valor de referencia) están en buen acuerdo y las desviaciones tienden a ser pequeñas.\n\nSi el TDI es alto, significa que hay una mayor variabilidad o discrepancia entre las dos mediciones.\n\n\n\nProbabilidad de cobertura.\n\nÍndice asociado a un intervalo.\nProbabilidad de que la diferencia entre dos mediciones, \\(|X_1 - X_2\\), esté contenida dentro de un intervalo de longitud \\(\\delta\\) (el TDI).\n\\(\\symbf{P(|X_1 - X_2| \\leq \\delta)}\\)\nProbabilidad de que el intervalo de error entre dos mediciones (o entre una medición y un valor de referencia) se mantenga dentro de un rango específico, con un cierto nivel de confianza.\nLa probabilidad de que la diferencia entre dos mediciones caiga dentro de un intervalo predefinido.\nQuiero estudiar con qué probabilidad tomo valores en ese intervalo. no está tan claro que tenga unidades de medida (porque es una prob).\n\nSi le doy la vuelta, qué intervalo tengo para que el x % de las probabilidades estén en él, estoy pidiendo un superior e inferior que sí tienen unidades de medida. Referida a una prbailidad pero en realdiad dada una prob lo que bsco es un intervalo en la variable respuesta que ocurra con esa probabilidad.\n\n\n\nSon buenos para medir las características de esa respuesta. Pero si quiero medir acuerdos comparando dos respuestas distintas no me valen.\nMedidas relativas\nIdea: semejanza de valores.\n\nCoeficiente de Correlación Intraclase. Cuantifica el acuerdo entre agentes.\nCoeficiente de correlación concordante. Analiza la semejanza. (semejanza \\(\\neq\\) acuerdo)\nCoeficiente de correlación de Pearson. Analiza si existe una función que permite llegar de unos valores a otros.\n\nSi no hay relacion:\n- El ICC dice que no hay acuerdo.\n- El coeficiente de correlación concordante dice que no hay semejanza.\n- El coeficiente de correlación de Pearson dice que no hay forma de a partir de unos valores llegar a los otros.\nEjemplos\n\nEjemplo a.\n\n\n\\(Y_a\\) = medida por el método A.\\(Y_b\\) = medida por el método B.\n\\[\n\\displaylines{\n\\begin{array}{c|c c c c}\n\\textbf{Sujeto} & \\textbf{1} & \\textbf{2} & \\textbf{3} & \\textbf{4}  & \\textbf{5} \\\\ \\hline\n\\symbf{Y_a} & 1 & 2 & 3 & 4 & 5 \\\\ \\hline\n\\symbf{Y_b} & 1 & 2 & 3 & 4 & 5 \\\\ \\hline\n\\end{array}\n}\n\\]\n\n\nICC = 1.\n\nCCC = 1.\nComo \\(Y_a = Y_b\\) =&gt; r = 1.\n\n\n\n\n\n\n\n\n\n\n\nEjemplo b.\n\n\n\\(Y_a\\) = medida por el método A.\\(Y_b\\) = medida por el método B.\n\\[\n\\displaylines{\n\\begin{array}{c|c c c c}\n\\textbf{Sujeto} & \\textbf{1} & \\textbf{2} & \\textbf{3} & \\textbf{4}  & \\textbf{5} \\\\ \\hline\n\\symbf{Y_a} & 5 & 6 & 7 & 8 & 9 \\\\ \\hline\n\\symbf{Y_b} & 0 & 1 & 2 & 3 & 4 \\\\ \\hline\n\\end{array}\n}\n\\]\n\nComo \\(Y_b = Y_a - 5\\) =&gt; r = 1. (no sé si hay acuerdo pero sé que puedo hacer una asociación entre uno y otro)\nLas puntuaciones son diferentes pero el orden de los sujetos es el mismo. Además la diferrencia entre puntuaciones es siempre de una unidad. CCC = 1.\naquí frase no he escuchado. ICC = 0.94. No hay independencia, hay un criterio común para los dos. Buscamos si las valoraciones dadas responden a un criterio común (que uno sea más estricto que otro no nos interesa ahora)\n\n\n\n\n\n\n\n\n\n\n\nEjemplo c.\n\n\n\\(Y_a\\) = medida por el método A.\\(Y_b\\) = medida por el método B.\n\\[\n\\displaylines{\n\\begin{array}{c|c c c c}\n\\textbf{Sujeto} & \\textbf{1} & \\textbf{2} & \\textbf{3} & \\textbf{4}  & \\textbf{5} \\\\ \\hline\n\\symbf{Y_a} & 1 & 2 & 3 & 4 & 5 \\\\ \\hline\n\\symbf{Y_b} & 1/2 & 1 & 3/2 & 2 & 5/2 \\\\ \\hline\n\\end{array}\n}\n\\]\n\nAnalizo la relación entre las dos respuestas y veo que con conocer la primera obtengo la segunda. Al exister esa relacion y ser perfecta sé que r=1. \\(Y_b = 1/2 * Y_a\\)\nICC = 0.5.\nCCC &lt; 1. Aunque la puntuación sea diferente, el orden sigue siendo el mismo en ambas valoracioes. Pero la escala de calibracion no es la misma: la diferencia en los primeros es de un punto y de los segundos es de medio punto."
  },
  {
    "objectID": "tema_01/tema_01_3_concordancia_numericas.html#coeficientes-de-correlación-intraclase-icc",
    "href": "tema_01/tema_01_3_concordancia_numericas.html#coeficientes-de-correlación-intraclase-icc",
    "title": "3. Análisis de concordancia en variables numéricas",
    "section": "Coeficientes de correlación intraclase (ICC)",
    "text": "Coeficientes de correlación intraclase (ICC)\nEl ICC cuantifica el ajuste (la concordancia) entre varias valoraciones de agentes de una variable numérica. Un conjunto de agentes/jueces dan valoraciones a una serie de individuos.\nProblema\nHay muchos ICC. Y R da todos. \\(\\Rightarrow\\) Tendremos muchas salidas y hay que interpretar la situación de nuestros datos para saber qué ICC es el acorde a nuestro modelo teórico.\nEn la situación donde la evaluación era la asignación de categorías no nos afectaba saber cómo se han recodigo los datos.\nEl ICC es la proporción de variabilidad debida a la variabilidad de los “sujetos”.\nTendremos un ICC para cada modelo ANOVA.\nPosibles situaciones. (en los modelos unifactorial y bifactorial)\nTenemos una muestra de n sujetos valorados por k agentes.\n\n\nCada sujeto es valorado por un conjunto diferente de k jueces seleccionados aleatoriamente. (al sujeto 1 le evalúan k jueces, al sujeto 2 otros k jueces distintos, etc.)\n\n“cada vez que hago un juicio (cada vez que hago una valoración sobre un sujeto) escojo un juez al azar”\n\n\n\nSe selecciona una muestra aleatoria de k jueces y cada uno valora a todos los sujetos.\n\n“tengo muchos métodos y como no puedo estudiar todos selecciono una muestra de jueces al azar” (introducir incertidumbre de haber podido elegir otros cuatro)\n\n\n\nLos k jueces son fijos (la población, no son una muesta) y cada uno valora a todos los sujetos.\n\n“tengo solo cuatro jueces y cojo los cuatro para medir la concordancia entre todos ellos.”\n\n\n\nModelo teórico.\n\nSea \\(y_{ij}\\) = la puntuación del juez i al sujeto j. (al revés de como lo habría hecho yo y cualquier persona normal)\nj, sujeto. i = 1…n\ni, juez. j = 1…k\n\n\\[\ny_{ij} \\text{ depende de} = \\begin{cases}\n\\text{parte común} \\\\\n\\text{influencia del sujeto i} \\\\\n\\text{influencia del juez j} \\\\\n\\text{interacción sujeto/juez} \\\\\n\\text{azar (lo que no hemos incluido en el modelo)} \\\\\n\\end{cases}\n\\]\nPara analizar cómo de concordantes son esas valoraciones tengo que identificar el modelo de la recogida de datos. No es lo mismo estudiar la concordancia de los jueces si tengo la opinión de todos, si he tenido que elegir cuatro de ellos, si me imponen cuáles elegir, si tengo que elegir cada vez uno distinto, etc.\n\\(\\rho = \\frac{Cov(y_{ij},y_{i'j})}{Var(y_{ij})} = \\frac{\\sigma_{\\beta}^2}{\\sigma_{\\beta}^2 + \\sigma_{e}^2} = \\text{variabilidad de los sujetos respecto a la variabilidad total del modelo}\\)\n\n\n\\(Cov(y_{ij},y_{i'j})\\): puntuación de un individuo dada por diferentes jueces.\n\n\n\\(Var(y_{ij})\\): toda la variabilidad de lo observado.\n\nVarianza.\nVarianza de los datos = varianza del modelo + varianza de todo lo demás.\nCaso 1. ANOVA unifactorial\nCada sujeto ha sido evaluado por un número k de jueces distinto y desconocido para cada sujeto. No sé si la primera puntuación de un individuo y la primera puntuación de otro individuo la ha dado el mismo juez. Si no tengo la influencia del juez medida tampoco puedo medir la interacción.\nFuentes de variabilidad no controladas:\n\nvariabilidad debida a los jueces\nvariabilidad debida a la interacción entre juez y sujeto\nvariabilidad debida al error\n\nModelo teórico.\n\\[\ny_{ij} = \\mu + \\beta_j + e_{ij}, \\quad i = 1, \\dots, k, \\quad j = 1, \\dots, n\n\\]\n\\[\n\\left\\{\n\\begin{aligned}\n  \\{\\beta_j\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{\\beta}^2) \\\\\n  \\{e_{ij}\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{e}^2)\n\\end{aligned}\n\\right\\}\n\\quad \\text{independientes.}\n\\]\n\\[\n\\displaylines{\n& \\textbf {ANOVA y esperanzas de los cuadrados medios} \\\\\n&\\begin{array}\n{|c|c|c|c|}\n\\hline  \\textbf { Fuente de variación } & \\textbf { SC } & \\textbf { g.l. } & \\textbf { CM }  & \\textbf { E(CM) } \\\\\n\\hline\n\\text {Intersujetos} &\\text {SCB = } k\\sum_{j=1}^{n}(\\overline{y}_{·j} - \\overline{y}_{··})^2 & \\text {n-1} & \\text {CMB = } \\frac{SCB}{n-1} & k\\sigma_{\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\text {Intrasujetos} & \\text {SCW = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{·j})^2 & \\text {n(k-1) = N-n} & \\text {CMW = } \\frac{SCW}{n(k-1)} & \\sigma_{e}^2\\\\\n\\hline\n\\text {Total} & \\text {SCT = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{··})^2 & \\text {N-1} & \\text {CMT = } \\frac{SCW}{N-1} & \\sigma^2\\\\\n\\hline\n\\end{array}\n}\n\\]\n\n\nIntersujetos. Entre los sujetos. Variabilidad entre los sujetos que participan. Captura las diferencias naturales entre individuos.\n\nSi hay n sujetos, los g.l. de la variabilidad entre ellos es n-1.\n\n\n\nIntrasujetos. Dentro de los sujetos. Variabilidad que existe dentro de un mismo sujeto debido a los diferentes jueces.\n\nComo es dentro de los sujetos y cada sujeto tiene k evaluaciones, g.l. = k-1.\nComo tenemos n sujetos, g.l. totales son n(k-1).\n\n\n\nICC.\n\\[\\rho = \\frac{Cov(y_{ij},y_{i'j})}{Var(y_{ij})} = \\frac{\\sigma_{\\beta}^2}{\\sigma_{\\beta}^2 + \\sigma_{e}^2} \\Rightarrow \\\\\n\\hat\\rho = ICC(1,1) = \\frac{CMB - CMW}{CMB + (k-1)CMW}\\]\nContraste de hipótesis.\n\\(H0: \\rho =0. \\quad \\quad\\) Estadístico:\n\\[F_0 = \\frac{CMB}{CMW}, \\quad p-value = p(F_{n-1}{N-n} &gt; F_{0})\\]\nIntervalo de confianza.\n\\[IC_{1-\\alpha}(\\hat\\rho) = (\\frac{F_{L}-1}{F_{L}+(k-1)}, \\frac{F_{U}-1}{F_{U}+(k-1)})\\] tal que\n\\[F_{L} = \\frac{F_{0}}{F_{n-1, N-n, \\alpha/2}} \\quad \\quad F_{U} = F_{0} * F_{n-1, N-n, \\alpha/2}\\]\nEjemplo 1.5. (Shrout y Fleiss (1979))\nLa siguiente tabla muestra cuatro valoraciones para cada uno de 6 sujetos. Las valoraciones para cada sujeto corresponden a jueces diferentes para cada sujeto.\n\\[\n\\begin{array}{c|cccc}\n& \\textit{Juez} \\\\\n\\hline\n\\textit{Sujeto} & {1} & {2} & {3} & {4} \\\\\n\\hline\n1 & {9} & {2} & {5} & {8} \\\\\n2 & {6} & {1} & {3} & {2} \\\\\n3 & {8} & {4} & {6} & {8} \\\\\n4 & {7} & {1} & {2} & {6} \\\\\n5 & {10} & {5} & {6} & {9} \\\\\n6 & {6} & {2} & {4} & {7} \\\\\n\\end{array}\n\\]\n\nCódigon &lt;- 6\nk &lt;- 4\n\ndatos &lt;- data.frame(\n  Fuente_de_variación = c(\"Entre sujetos\", \"Dentro de sujetos\"),\n  g_l = c(n-1, n*k-n),\n  Cuadrado_medio = c(11.24, 6.26)\n)\n\ndatos\n\n  Fuente_de_variación g_l Cuadrado_medio\n1       Entre sujetos   5          11.24\n2   Dentro de sujetos  18           6.26\n\n\nEl experimento no se ha diseñado para valorar sobre quienes dan evaluaciones (no se ha diseñado para controlar el efecto de los jueces). Estoy mirando la independencia entre las valoraciones de un mismo individuo (para el \\(ind_1\\) las ka valoraciones, para el \\(ind_2\\) las ka valoraciones, etc.)\nICC(1,1) = 0,1657 \\(\\quad \\quad\\) \\(IC_{95}\\) = (-0.13, 0.72)\nMi objetivo es estudiar si \\(\\rho = 0\\).\n\nNo puedo rechazar la H0: independencia entre las valoraciones AKA las puntuaciones dadas al individuo son independientes.\n\n– medimos si un sujeto tiene medidas concordantes sin poder valorar a los jueces\nCaso 2. ANOVA bifactorial de efectos aleatorios\n(el caso más complicado precisamente por la aleatoriedad que queremos extrapolar a todos los jueces)\nCada sujeto ha sido evaluado por un número k de jueces distinto y desconocido para cada sujeto. No sé si la primera puntuación de un individuo y la primera puntuación de otro individuo la ha dado el mismo juez. Si no tengo la influencia del juez medida tampoco puedo medir la interacción — CAMBIAR\nFuentes de variabilidad no controladas:\n\nvariabilidad debida al error\n\nModelo teórico.\n\\[\ny_{ij} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + e_{ij}, \\quad i = 1, \\dots, k, \\quad j = 1, \\dots, n\n\\]\n\\[\n\\left\\{\n\\begin{aligned}\n  \\{\\alpha_i\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{\\alpha}^2) \\\\\n  \\{\\beta_j\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{\\beta}^2) \\\\\n  \\{(\\alpha\\beta_{ij}\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{\\alpha\\beta}^2) \\\\\n  \\{e_{ij}\\} &\\sim \\text{v.a.i.i. } \\mathcal{N}(0, \\sigma_{e}^2)\n\\end{aligned}\n\\right\\}\n\\quad \\text{independientes.}\n\\]\n\\[\n\\displaylines{\n& \\textbf {ANOVA y esperanzas de los cuadrados medios} \\\\\n&\\begin{array}\n{|c|c|c|c|}\n\\hline  \\textbf { Fuente de variación } & \\textbf { SC } & \\textbf { g.l. } & \\textbf { CM }  & \\textbf { E(CM) } \\\\\n\\hline\n\\text {Intersujetos} &\\text {SCB = } k\\sum_{j=1}^{n}(\\overline{y}_{·j} - \\overline{y}_{··})^2 & \\text {n-1} & \\text {CMB = } \\frac{SCB}{n-1} & k\\sigma_{\\beta}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\text {Intrasujetos} & \\text {SCW = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{·j})^2 & \\text {n(k-1) = N-n} & \\text {CMW = } \\frac{SCW}{n(k-1)} & \\sigma_{\\alpha}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\hfill \\text {Intraagentes} & \\text {SCW = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{·j})^2 & \\text {n(k-1) = N-n} & \\text {CMW = } \\frac{SCW}{n(k-1)} & \\sigma_{\\alpha}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\hfill \\text {Residual} & \\text {SCW = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{·j})^2 & \\text {n(k-1) = N-n} & \\text {CMW = } \\frac{SCW}{n(k-1)} & \\sigma_{\\alpha}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\text {Total} & \\text {SCT = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{··})^2 & \\text {N-1} & \\text {CMT = } \\frac{SCW}{N-1} & \\sigma^2\\\\\n\\hline\n\\end{array}\n}\n\\]\n\\[\n\\displaylines{\n& \\textbf {ANOVA y esperanzas de los cuadrados medios} \\\\\n&\\begin{array}\n{|c|c|c|c|}\n\\hline\n\\textbf { Fuente de variación } & \\textbf { SC } & \\textbf { g.l. } & \\textbf { CM }  & \\textbf { E(CM) } \\\\\n\\hline\n\\text {Intersujetos} & \\text {SCB = } k\\sum_{j=1}^{n}(\\overline{y}_{·j} - \\overline{y}_{··})^2 & \\text {n-1} & \\text {CMB = } \\frac{SCB}{n-1} & k\\sigma_{\\beta}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n{\\text {Intrasujetos}} & \\text {SCW = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{·j})^2 & \\text {n(k-1) = N-n} & \\text {CMW = } \\frac{SCW}{n(k-1)} & \\sigma_{\\alpha}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\text {Intraagentes} & \\text {SCW = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{·j})^2 & \\text {n(k-1) = N-n} & \\text {CMW = } \\frac{SCW}{n(k-1)} & \\sigma_{\\alpha}^2 + \\sigma_{\\alpha\\beta}^2 + \\sigma_{e}^2\\\\\n\\hline\n\\text {Total} & \\text {SCT = } \\sum_{i=1}^{k}\\sum_{j=1}^{n}({y}_{ij} - \\overline{y}_{··})^2 & \\text {N-1} & \\text {CMT = } \\frac{SCW}{N-1} & \\sigma^2\\\\\n\\hline\n\\end{array}\n}\n\\]\n– aquí sí podemos hacer una valoración de los jeuves. estamos haciendo una concordancia de todos los jueces, no solo de los k de la muestra.\nbusco unos sujetos al azar y busco unos jueces al azar. dos factores. puntuacion puntal viene por algo comun para todos, lo que influya factor sujeto, lo que influya factor juez, por interaccion y por lo que dependa de todo lo que no he incluido antes\nhay un grado de concordancia más alto porque el primer juez es elegido al azar pero es el mismo para todos, el segundo es elegido al azar pero es el mismo para todos\nIC no tiene el 0, no rechazo independencia entre las valoracions que los jeuces dan a los suejtos. ahora podemos dar quienes dan las valoraciones\n¿hay un cirterio comun de los jeuces para lar las valoracion? rho=0, no.\nde un grupo de jueces he elegido unos pocos al azar. como podría ser cualquieras los que eleijo, me sirve para represnetar a todos. “independencia entre las valoracones de los jueces (todos) a un mismo individuo”\n\nLa puntuación depende del individuo al que le pongo la nota y del juez. Considero también que hay una nota inicial de la que parten toos los sujetos (\\(\\mu\\))\nun aspecto que mida lo que aporta el sujeto, .. el juez y considerar que puede haber una interacción entre sujyjuezç\nmedir la concordancia: medir el modelo que va por dertas en el diseño de experientos medir la variabilidad total variabilidad del modelo\nal coeficiente desde el punto de vista teórico lo llamamos rho. cuando hablemos del estimador hablaremos del ICC coef es valor desconocido y lo aproximamos muestralmente\nde quién depende ICC? la experimentacion conduce a comparar resultados, comparar puntuaciones de un individuos según diferentes jueces/criterios.\nCada sujeto es valorado por un conjunto diferente de k jueces, seleccionados aleatoriamente –&gt; incluso un juez puede evaluar más de una vez a un sujeto (¿creo que ha dicho eso?) Esto es: cojo k jueces y evalúan al sujeto 1, cojo otrs k jueces y evalúan al sujeto 2, seleccion otros k jueces y evalúan al siguiente sujeto, un juez ha podido salir para evaluar a más de un sujeto.\ndiff entre 1 y 2: en el 2 medimos lo que fluyen las puntuaciones, no tanto la concordancia entre los jueces\ndiapo27\nde qué puede depender las puntuaciones: el individuo al que le pongo la nota y el juez\nun aspecto que mida lo que aporta el sujeto, .. el juez y considerar que puede haber una interacción entre sujyjuez, puedo considerar tb que hay una nota inicial de la que parten toos los sujetos\n\\[\ny_{ij} \\text{ depende de} = \\begin{cases}\n\\text{parte común} \\\\\n\\text{influencia del sujeto i} \\\\\n\\text{influencia del juez j} \\\\\n\\text{interacción sujeto/juez} \\\\\n\\text{azar (lo que no hemos incluido en el modelo)} \\\\\n\\end{cases}\n\\]\nModelo teórico.\nrho: variabilidad de los sujetos respecto a la variabilidad total del modelo.\nnum: puntuación de un individuo dada por diferentes jueces: cov(y_ij, y_.j) den: toda la variabilidad de lo observado: var(y_ij)\ndepende de cómo llegue a\n24/10/2024\ndiapo34\nCaso 3. ANOVA bifactorial de efectos mixtos.\n¿Por qué alphabeta son dependientes?\nEn un modelo ANOVA bifactorial de efectos mixtos, como el que se muestra en la imagen, los términos (αβ)ij()_{ij}(αβ)ij, que representan la interacción entre los efectos fijos (αi_iαi) y los efectos aleatorios (βj_jβj), son dependientes porque la varianza de las interacciones está ligada tanto a la variabilidad de los efectos aleatorios como a los efectos fijos.\nEl efecto (αβ)ij(){ij}(αβ)ij refleja cómo la combinación de un nivel del factor fijo iii con un nivel del factor aleatorio jjj puede influir en la variable de respuesta yijy{ij}yij. Los efectos aleatorios βj_jβj y el término de interacción están correlacionados porque ambos se ven influenciados por las mismas fuentes de variabilidad del factor aleatorio.\nEsto ocurre debido a que el factor aleatorio introduce variabilidad que no es completamente explicada por los efectos fijos αiiαi, generando una dependencia en el término de interacción. Esta dependencia puede modelarse con una distribución de varianza conjunta, como se indica en la imagen: N(0,σαβ2)N(0, {}^2)N(0,σαβ2).\nPor otro lado, los términos βjjβj y eije{ij}eij son independientes entre sí, ya que representan fuentes de variabilidad distintas: βjjβj describe la variabilidad entre niveles del factor aleatorio, mientras que eije{ij}eij representa el error experimental o residual.\nEjemplo 1.5.\nhttps://github.com/cran/psych/blob/master/R/ICC.R\n\nlibrary(psych)\n\nA &lt;- matrix(c(9,    2,   5,    8,\n              6,    1,   3,    2,\n              8,    4,   6,    8,\n              7,    1,   2,    6,\n              10,   5,   6,    9,\n              6,   2,   4,    7),\n            ncol=4,byrow=TRUE)\ncolnames(A) &lt;- paste(\"J\",1:4,sep=\"\")\nrownames(A) &lt;- paste(\"S\",1:6,sep=\"\")\n\nA\n\n   J1 J2 J3 J4\nS1  9  2  5  8\nS2  6  1  3  2\nS3  8  4  6  8\nS4  7  1  2  6\nS5 10  5  6  9\nS6  6  2  4  7\n\n\n\nICC &lt;- ICC(A, lmer=FALSE)\nstr(ICC)\n\nList of 8\n $ results:'data.frame':    6 obs. of  8 variables:\n  ..$ type       : chr [1:6] \"ICC1\" \"ICC2\" \"ICC3\" \"ICC1k\" ...\n  ..$ ICC        : num [1:6] 0.166 0.29 0.715 0.443 0.62 ...\n  ..$ F          : num [1:6] 1.79 11.03 11.03 1.79 11.03 ...\n  ..$ df1        : num [1:6] 5 5 5 5 5 5\n  ..$ df2        : num [1:6] 18 15 15 18 15 15\n  ..$ p          : num [1:6] 0.164769 0.000135 0.000135 0.164769 0.000135 ...\n  ..$ lower bound: num [1:6] -0.1329 0.0188 0.3425 -0.8844 0.0711 ...\n  ..$ upper bound: num [1:6] 0.723 0.761 0.946 0.912 0.927 ...\n $ summary:List of 1\n  ..$ :Classes 'anova' and 'data.frame':    3 obs. of  5 variables:\n  .. ..$ Df     : num [1:3] 5 3 15\n  .. ..$ Sum Sq : num [1:3] 56.2 97.5 15.3\n  .. ..$ Mean Sq: num [1:3] 11.24 32.49 1.02\n  .. ..$ F value: num [1:3] 11 31.9 NA\n  .. ..$ Pr(&gt;F) : num [1:3] 1.35e-04 9.45e-07 NA\n  ..- attr(*, \"class\")= chr [1:2] \"summary.aov\" \"listof\"\n $ stats  : num [1:5, 1:3] 5.00 5.62e+01 1.12e+01 1.10e+01 1.35e-04 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:5] \"df\" \"SumSq\" \"MS\" \"F\" ...\n  .. ..$ : chr [1:3] \"subjects\" \"Judges\" \"Residual\"\n $ MSW    : num 6.26\n $ lme    : NULL\n $ Call   : language ICC(x = A, lmer = FALSE)\n $ n.obs  : int 6\n $ n.judge: int 4\n - attr(*, \"class\")= chr [1:2] \"psych\" \"ICC\"\n\nICC$results\n\n                         type       ICC         F df1 df2            p\nSingle_raters_absolute   ICC1 0.1657418  1.794678   5  18 0.1647688083\nSingle_random_raters     ICC2 0.2897638 11.027248   5  15 0.0001345665\nSingle_fixed_raters      ICC3 0.7148407 11.027248   5  15 0.0001345665\nAverage_raters_absolute ICC1k 0.4427971  1.794678   5  18 0.1647688083\nAverage_random_raters   ICC2k 0.6200505 11.027248   5  15 0.0001345665\nAverage_fixed_raters    ICC3k 0.9093155 11.027248   5  15 0.0001345665\n                        lower bound upper bound\nSingle_raters_absolute  -0.13293232   0.7225601\nSingle_random_raters     0.01878651   0.7610844\nSingle_fixed_raters      0.34246477   0.9458583\nAverage_raters_absolute -0.88444216   0.9124154\nAverage_random_raters    0.07113682   0.9272320\nAverage_fixed_raters     0.67567471   0.9858917\n\n\nICC es la ESTIMACIÓN del rho. Luego el estadístico del contraste. El IC refleja algo sobre la H0"
  },
  {
    "objectID": "tema_01/tema_01_3_concordancia_numericas.html#método-gráfico-de-bland-y-altman",
    "href": "tema_01/tema_01_3_concordancia_numericas.html#método-gráfico-de-bland-y-altman",
    "title": "3. Análisis de concordancia en variables numéricas",
    "section": "Método gráfico de Bland y Altman",
    "text": "Método gráfico de Bland y Altman\nMétodo gráfico, sin análisis estadístico de apoyo.\nSolamente permite comparar dos observaciones por individuo. Si tuviera cuatro medidas con cuatro instrumentos sobre un individuo tendría que hacer comparaciones dos a dos.\n(\\(X_1\\), \\(Y_1\\)), (\\(X_2\\), \\(Y_2\\)), (\\(X_3\\), \\(Y_3\\))…\n\\(n_i\\) ~ individuo i.\\(X_i\\) ~ resultado de la variable en el individuo i con el método 1.\\(X_i\\) ~ resultado de la variable en el individuo i con el método 2.\n¿Hay una concordancia entre medidas de un mismo individuo?\nsi hubiese concordancia y fuere perfecta el resultado de un método tendría que ser exactamente igual que con el otrométodo.\nSe realiza un cambio de referencia \\(\\Rightarrow\\) En la primera coordenada el promedio entre las dos medidas y en la segunda coordenada poner la diferencia.\n\\[\n( \\frac{X_i + Y_i}{2}, \\quad \\quad  X_i - Y_i )\n\\]\nun valor más cerca o hacia otro dependiendo de cuál sea mayor. si x y fueran igual (concordanctes) tendríamos meddaias muy cerca del 0.\ndiagrama de dispersion: nos separamos bastante de la mitad (creo que ha dicho eso)\n¿Sirve hacer un estudio de regresión lineal?\nEso es para ser capaz de encontrar una ecuacion que me relacione ambos valores. yo no quiero ver la relacion entre una valoracion y otra, sino que quiero ver si hay concordancia.\nsaber pasar de unos valores a otros no es concordancia (eso sería como un cambio de escala). concordancia es tener los mismos valores em ambas valoraciones.\nA ver, me valdría solo si me sale que la ecuación es Y = X.\nejemplo 1.6.\ncuando doy una medida con el monitor es superior a la del esfigmomanómetro.\nsig - monitor.\nla mayoría tienen una medida más alta con el método manual que con el monitor\nel gra´fico incluye una recta de la referencia promedio y dos bandas de confianza que calculamosa poyándonos en las propiedades de la media.\nestamos suponiendo que las difenrencias tiene dist media (que la van a tner) y varianza constante en las diferencias.\ncon todas las diferencias hacemos la media \\(\\bar{d} = \\frac{1}{N}\\sum d_i\\)\ncuando hacemos el hist de las dif me saldrá guassiano\nsd = desviacion tipica de la muestra de las diferencias de los individuos\nel 95 % muestras de los individuos tienen unas muestras que van de -55 a +22.\nla diff entre un proc y el otro se mueve en promedio en 16 unidades (sobre estima el monitor, o el otro, vamos) a favor del monitor y las diff entre los dos para el 95 % de los individuos se mueve entre 55 y +22.\na parte del Ic tenemos una banda para el IC.\ndiapo43 aquí es onde entra la suposicion de varianza constante. como damos por hecho que tenemos media gaussiana\nhemos dado unas medidas a los limietes y le añadimos una medida mas de concentracion.\nesa cantidad cuando trabajamos con muestras noramales da la casualidad que tiene una distribucion de t de student.\nconsigo acotar .\nen media mi diff de mediadas era 16. cuando haría los intervalos tenia -55 y 22. en el caso más optimista tengo -14 29.\ntengo una diferncia que como poco -14+29, y como mucho -61+47\nhay que poner el IC y los intervalos sobre los IC.\n-55,22: intervalo para decir que tengo concentrado el 95 de los datos ahí. este Ic no me dice pracitcamente nada, a esos dos límites les doy una holgura con sus respectivos IC.\nEjemplo 1.7.\nComparan el volumen de plasma con la medida normal.\nvemos que el movlumen de plasma está por encima de la diagonal, claramente una de las medidas está por encima siempre.\nsi las dos medidas fieran concordantesdeberían estar al rededor del cero.\nen el eje_x bajo las diferencias son más pequeñas y cuando x es grande las diferencias son más grandes. las diferncias resultasn que de donde estén las medidas van creciendo (las diferrencias se abren) oh oh parece que tenemos una varianza que no es cote. las medidas con valores más bajo se dierencian menos que las medidas de cuando tienen altos valores.\nestamos trabajando con la medidas de un individuo sobre una medida estándar. lo que hacemos es trasnformar. Repasar las transofrmciones de Cox: son una familia de transformaciones potenciales usadas en estadística para corregir sesgos en la distribución de errores. Cuando tenemos que transformar medidas que hacen referencia a un estandar lo que se recomienda es hacer medias logar´timocas.\n\\[\n\\log(\\frac{a}{b}) = \\log(a) - \\log(b)\n\\]\ncambiamos la escala y las unidades de medida. obviamente para comunicar los resultados tengo que volver a la escala original\nlo que quiero es darle un intervalo a las medias iniciales.\nla media de media(log) no la estamos usando, sobre todo la usamos para ver dónde tengo los datos, pero la medida que realmente uso para sacar conclusiones es la diferencia (la dif de logaritmos en este caso)\n1.058 &lt; N_i / H_i &lt; 1.153\nes un cociente, la medida de nerder como mínimo es mayor en un 5 % que la de harley.\nla medida de N siempre es superior a la de H, con una superioridad que va de un 5.8 % a un 15.3 %\ndiapo54\ncuando tengamos jeuces desconocidos que dna puntuaciones y jueces perfectamente conocidos y vemos cómo concuerdan o no (caso 1 y caso 3)\nusamos un método recursivo que busca n reiteradamente\ntodos los intervalos tiene la misma estructura. partiendo de mi intervalo incial voy a obtener mi F_0 (ya viene en la salida del ICC). Con ello identifico los gl (dependiendo del caso en el que estemos tendremos unos grados de libertad distintos)\npara un n, saco mis estadísticos y mi IC. Si mi IC no cumple los requisitos del longitud del intervalo que me piden, aumento mi n en una unidad. calculo mis estadísticos y mi IC. Si mi IC no cumple los requisitos del longitud del intervalo que me piden, aumento mi n en una unidad.\npara determinar el Ic seguimos un procedimiento iterado. partiendo del tamaño inicial n de la muestra y del valor del estadístoco F_0 aumentamos el tamaño de la muestra en una unida y calculamos los límites del nuevo IC. si la longitud de ete IC satistace las condcciones ya tenemos determinado n, si no, volvemos a aumentar en una unidad el tamño muestral."
  },
  {
    "objectID": "tema_01/tema_01_4_tamanyo_muestral.html#determinación-del-tamaño-muestral-para-el-coeficiente-kappa",
    "href": "tema_01/tema_01_4_tamanyo_muestral.html#determinación-del-tamaño-muestral-para-el-coeficiente-kappa",
    "title": "4. Cálculo del tamaño muestral",
    "section": "Determinación del tamaño muestral para el coeficiente kappa",
    "text": "Determinación del tamaño muestral para el coeficiente kappa\n\\[\n\\displaylines{\n\\text{Quiero una precisión más pequeña.} \\\\\n{\\Downarrow} \\\\\n\\text{La precisión depende de la varianza.} \\\\\n{\\Downarrow} \\\\\n\\text{La varianza depende del tamaño muestral.}\n}\n\\]\nSupongo que n muestral es grande. ¿Puedo suponerlo? Pos claro amego, n es lo que estoy buscando calcular así que puedo desear que n sea grande.\nPara buscar esta nueva n pongo una condición: que el valor máximo de la varianza sea menor a un valor dado. La varianza con el nuevo tamaño \\(n\\) debe ser menor a la varianza con el tamaño de la muestra \\(n_{0}\\).\nEjemplo.\nSupongamos tengo n=20. Calculo el IC y tengo la amplitud del intervalo. Voy a querer un IC tal que la amplitud sea menor a cierto valor \\(k\\) (puedo tener la suerte de que con mi estudio piloto ya cumpla esa condición). Por lo que quiero un n condicioando a un valor máximo de la varianza (que depende de n).\nFijando el valor máximo de la varianza puedo despejar el valor de n.\n\nCaso dicotómico: dos observadores y dos categorías\n\nDefinicion de la varianza en el modelo teórico.\n\\[\n{\\bf\nV(k) = \\frac{A+B+C}{n(1-\\Pi_c)^4}\n}\n\\]\n\ntal que\n\\[\n\\displaylines{\nA = \\pi_{11}[1- \\pi_c - (\\pi_{1·}+\\pi_{·1})(1-\\pi_0)]^2 + \\pi_{22}[1- \\pi_c - (\\pi_{2·}+\\pi_{·2})(1-\\pi_0)]^2 \\\\\nB = (1-\\pi_0)^2[\\pi_{12}(\\pi_{·1}+\\pi_{2·})^2 + \\pi_{21}(\\pi_{·2}+\\pi_{1·})^2] \\\\\nC = (\\pi_0 - 2\\pi_c+\\pi_0\\pi_c)^2\n}\n\\]\nTodos los valores quedan determinados a partir de \\(\\pi_{1·}\\), \\(\\pi_{·1}\\) y \\(k\\).\n\\[\n\\displaylines{\n\\pi_{2·} = 1 - \\pi_{1·} \\\\\n\\pi_{·2} = 1 - \\pi_{·1} \\\\\n\\pi_{c} = \\pi_{1·}\\pi_{·1} + (1 - \\pi_{1·})(1 - \\pi_{·1}) \\\\\n\\pi_{0} = k(1-\\pi_{c}) + \\pi_{c} \\\\\n\\pi_{22} = (\\pi_{0} - \\pi_{1·} + \\pi_{2·}) / 2 = (\\pi_{0} + 1) / 2 \\\\\n\\pi_{11} = 1 - \\pi_{22} = 1 - (\\pi_{0} + 1) / 2 \\\\\n\\pi_{12} = \\pi_{1·} - \\pi_{11} \\\\\n\\pi_{21} = \\pi_{·1} - \\pi_{11}\n}\n\\]\n\n\nCaso dicotómico: dos observadores y t categorías (kappa sin ponderar)\n\n\\[\n{\\bf\nV(k) = \\frac{A+B+C}{n(1-\\Pi_c)^4}\n}\n\\]\ntal que\n\\[\n\\displaylines{\n\\bf{A = \\sum_{i=1}^{t} \\pi{ii}[1-\\pi_c-(\\pi_{i·}+\\pi_{·i})(1-\\pi_0)^2]} \\\\\n\\bf{B = (1-\\pi_0)^2 \\sum_{i=1}^{t} \\sum_{\\substack{i=1 \\\\ i \\neq j}}^{t} \\pi_{ij}(\\pi_{·i}+\\pi_{j·})^2} \\\\\n\\bf{C = (\\pi_0 - 2\\pi_c+\\pi_0\\pi_c)^2}\n}\n\\]"
  },
  {
    "objectID": "tema_01/tema_01_4_tamanyo_muestral.html#n-óptimo",
    "href": "tema_01/tema_01_4_tamanyo_muestral.html#n-óptimo",
    "title": "4. Cálculo del tamaño muestral",
    "section": "n óptimo",
    "text": "n óptimo\n¿Cuál sería el n óptimo partiendo de una muestra incial y un IC incial para conseguir reducir la amplitud del intervalo?\n\nTengo un n que me da la posibilidad de estimar un n neuvo dada una condición.\n\n\nFijando la longitud del intervalo\n\nSupongamos que tenemos información de una muestra de tamaño \\(n_0\\) para la cual se tiene que \\[IC_{\\hat{k}}^{1-\\alpha} \\hspace{1em} = \\hspace{1em} (\\hat{k_0} - z_{\\frac{\\alpha}{2}} * \\sqrt{Var(k)}, \\hspace{1em} \\hspace{1em} \\hat{k_0} + z_{\\frac{\\alpha}{2}} * \\sqrt{Var(k)}\\]\nLlamamos \\[l_{0} \\hspace{1em} = \\hspace{1em} \\text{longitud del intervalo} = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{Var(k)}\\]\nBusco un tamaño muestral \\(n\\) tal que el IC para \\(\\hat{k}\\) con la muestra de tamaño n tenga una longitud \\(w\\). \\[w = \\text{longitud del intervalo de la muestra de tamaño n} = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{Var(k)} = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{A+B-C}{n_0(1-\\Pi_{c})^4}}\\]\nA la estimacion de \\(n\\) le asignamos el mismo error que tendría con una muestra más pequeña (la peor de las situaciones que podríamos tener). \\[w = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{A+B-C}{n_0(1-\\Pi_{c})^4}} = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{A+B-C}{n(1-\\Pi_{c})^4}}\\]\nBusco un \\(n\\) tal que su longitud sea menor igual a \\(w\\).\n\n\\[\n\\begin{cases}\nl_{0} = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{1}{n_0}} * \\sqrt{\\frac{A+B-C}{(1-\\Pi_{c})^4}} \\\\\nw = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{1}{n}} * \\sqrt{\\frac{A+B-C}{(1-\\Pi_{c})^4}}\n\\end{cases}\n\\]\n\nDespejando en \\(l_{0}\\).\n\n\\[\\frac{l_{0} * \\sqrt{n_0}}{2*z_{\\frac{\\alpha}{2}}} = \\sqrt{\\frac{A+B-C}{(1-\\Pi_{c})^4}}\\]\n\nSustituyo en \\(w\\).\n\n\\[w = 2*z_{\\frac{\\alpha}{2}} * \\sqrt{\\frac{1}{n}} * \\sqrt{\\frac{A+B-C}{(1-\\Pi_{c})^4}} = \\frac{2*z_{\\frac{\\alpha}{2}}}{\\sqrt{n}} * \\frac{l_{0} * \\sqrt{n_0}}{2*z_{\\frac{\\alpha}{2}}} = \\frac{l_{0} * \\sqrt{n_0}}{\\sqrt{n}}\\]\n\nDespejando en \\(w\\).\n\n\\[w = \\frac{l_{0} * \\sqrt{n_0}}{\\sqrt{n}} \\Longleftrightarrow w^2 = \\frac{l_{0}^2 * n_0}{n}\\]\n\\[n = (\\frac{l_{0}}{w})^2*n_0\\]\n\n\nResolver el contraste de hipótesis sobre \\(k\\)\nla condicion que hemos puesto para fijar n es que la longitud del intervalo la fijamos\ntambien pueddo fijar una hipótesis de un crontraste de hipótesis, pe, diapo52, una hipótesis sobre el índico k_0."
  },
  {
    "objectID": "tema_03/tema_03.html#simulación-numérica-para-desarrollar-pruebas-diagnósticas.",
    "href": "tema_03/tema_03.html#simulación-numérica-para-desarrollar-pruebas-diagnósticas.",
    "title": "TEMA 3",
    "section": "Simulación numérica para desarrollar pruebas diagnósticas.",
    "text": "Simulación numérica para desarrollar pruebas diagnósticas.\nEste es un tema instrumental. Iremos recurriendo a sus contenidos a medida que avance el desarrollo de la asignatura.\nContenidos:\n\nIntroducción (metodología y otros aspectos básicos)\nSelección aleatoria de pacientes y asignación de tratamientos\nObtención de datos simulados\nValidación de estimaciones: Metodología Bootstrap\nValidación del Índice Kappa\nTécnicas de remuestreo aplicadas a la inferencia de curvas ROC\nDeterminar el número de réplicas de simulación\nControlar la calidad de los datos simulados\nAplicaciones"
  },
  {
    "objectID": "tema_03/tema_03_1_introduccion.html#whats-simular",
    "href": "tema_03/tema_03_1_introduccion.html#whats-simular",
    "title": "1. Introducción",
    "section": "What’s simular?",
    "text": "What’s simular?\nSIMULAR. Representar la realidad con un modelo.\nQueremos es una simulación estocástica \\(\\Rightarrow\\) Obtener diferentes “variedades” de una situacion donde el azar interviene. \\(\\Rightarrow\\) Lo que se conoce como estimacion de Monte Carlo.\n\nUn proceso estocástico es aquel cuyo comportamiento no es determinista, en la medida en que el subsiguiente estado del sistema se determina tanto por las acciones predecibles del proceso como por elementos aleatorios.\nUna simulación de Monte Carlo es un modelo probabilístico que puede incluir un elemento de incertidumbre o aleatoriedad en su predicción. https://aws.amazon.com/es/what-is/monte-carlo-simulation/. Técnica numérica basada en conceptos y resultados probabilísticos que consigue IMITAR un fenómeno (situación o sistema) real. Obvio que no tenemos la certeza de cuál va a ser el fenómeno, hay incertidumbre.\n\n\nLa simulación es la antítesis de los modelos teóricos en el sentido de que la simulación no duda nunca porque ya no tiene probabilidades, tiene datos. La simulación no echa cuentas, da resultados (pa’ echar cuentas ya tengo el modelo teórico).\n\nPara simular un modelo tengo que basarme en un modelo teórico. Si quiero hacer simulaciones de la realidad debo conocer el comportamiento teórico del modelo. E identificar qué partes de ese modelo depende del azar y cuáles no. Esos cambios aleatorios impactados por el azar debo definirla como una variable aleatoria.\nPor ello, no hablamos de muestras malas ni muestras buenas. Habrá simulaciones malas o simulaciones buenas.\nLos números aleatorios que nos da el ordenador en realidad son números pseudoalatorios. Dado un valor inicial se consigue el siguiente número, y a partir del segundo el tercero, etc.\nPlanteamiento de un modelo de simulación:\n\nDesarrollar un modelo que represente la situación real que se quiere investigar.\nIdentificar qué partes o fases de la situación real cambian aleatoriamente. (sexo del bebé)\nDescribir los cambios aleatorios con variables aleatorias. (Sexo: {XX, XY}, p(XX)=0.5)\nGenerar observaciones aleatorias del sistema investigado.\nValidar el modelo simulado comparando los valores simulados con las observaciones reales.\n\nLa simulación tiene dos fases:\n1. Simulación de un valor aleatorio.\n2. Dado valor aleatorio asignar el valor simulado.\nCondiciones de una simulación\n\nQue sea rápida (que la generacion de números aletorios sea rápida)\n\nLos números aleatorios generados se distribuyan entre 0-1\n\nLos números aleatorios generados se repartan por igual entre 0-1\n\nLos números aleatorios parezcan independientes\n\nLos números generados no son independientes, ya que dada una semilla siempre recreo la misma sucesión de números aleatorios. H0: necesitamos que esos números pseudoaletorios parezcan independietes (ya que sabemos que no lo son\nEjemplo\nRealizar una asignación aleatoria del sexo de un bebé. Conocido el sexo vamos a simular el peso de cada bebé.\nCriterio: dado un número aleatorio (entre 0-1) elegir el sexo del bebé. Quiero que ambos elementos tengan la misma probabilidad (divido el intervalo en dos partes iguales)\nLa base de mi modelo teorico:\nx = sexo del bebé       y = peso del bebé (kg)\n\nQuiero generar varios bebés.\n(x1, peso_1)\n(x2, peso 2)\n\nPara cada bebé tengo sexo y peso (muestras paradas pero independencia entre las observaciones).\n\np(X=varón) = 0.5 = p(x=hembra)\npeso | x=varón ~ N(3.266, 0.514)\npeso | x=hembra ~ N(3.155, 0.495)\n\n\nGenero un número aleatorio.\nHago una simulación.\nSaco los datos simulados.\nReciclo semilla.\n\n\n\n\n\nTengo dos variables que simular.\n- Un número aleatorio para simular la primera variable.\n- Otra variable que no conozco pero que está condicionada. Otro número para simular para la otra variable.\n\nn=6; set.seed(20175)\n\nU=runif(n,min=0,max=1)\np=0.5;\npeso=numeric(n);\nsexo=character(n);\n\nfor(i in 1:n) {\n  if (U[i]&lt;p){\n    pp=rnorm(1,3.266,0.514)\n    peso[i]=pp\n    sexo[i]=\"varon\"\n } else {\n   pp=rnorm(1,3.155, 0.495)\n   peso[i]=pp\n   sexo[i]=\"mujer\"\n }\n}\n\n\ncat(\"Números aleatorios:\", U, \"\\n\")\n\nNúmeros aleatorios: 0.7621414 0.3668522 0.3111475 0.6270264 0.07654514 0.7816305 \n\ncat(\"Sexo:\", sexo, \"\\n\")\n\nSexo: mujer varon varon mujer varon mujer \n\ncat(\"Peso:\", peso, \"\\n\")\n\nPeso: 3.913199 3.536088 2.989453 3.101755 2.76329 3.636893 \n\n\n\nn=100000; set.seed(20175)\nU=runif(n,min=0,max=1)\np=0.5;\npeso=numeric(n);\nsexo=character(n);\n\nfor(i in 1:n) {\n  if (U[i]&lt;p){\n    pp=rnorm(1,3.266,0.514)\n    peso[i]=pp\n    sexo[i]=\"varon\"\n } else {\n   pp=rnorm(1,3.155, 0.495)\n   peso[i]=pp\n   sexo[i]=\"mujer\"\n }\n}\n\n# Histograma muestral n=100000\nhist(peso,freq=FALSE)\n\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nComo tengo dos muestras, dos variables, necesito que la creación de la simulación de cada una de las variables sea independiente. Necesito que la semilla de cada variable aleatoria, AKA la semilla de cada variable simulada, sea diferente. Al ser números pseudoaleatorios estaría condicionada la primera y segunda variable simulada.\n(esto no tiene nada que ver con que una variable esté condicionada a la otra)\n\n\n\n\n\n\n\n\nAcerca del ejercicio que nos mandó\n\n\n\nPuedo simular un número aleatorio para un hijo y luego otro para el segundo hijo. O un método para simular con un único número aleatorio los dos hijos a la vez (vamos, crear todas las condiciones con un único número aleatorio).\n\n\nMuy bien mijo, ¿pero y si tengo que simular algunas de las distribuciones no conocidas?\nExisten métodos generales para ello. Si tengo que simular distribución conocida estoy ok. Si tengo que generar una variable desconocida entonces tendré que crearla por distintos métodos.\n\n\n\n\nLa pregunta es, ¿qué nrum le pongo?"
  },
  {
    "objectID": "tema_03/tema_03_2_selección_aleatoria.html",
    "href": "tema_03/tema_03_2_selección_aleatoria.html",
    "title": "2. Selección aleatoria de pacientes y asignación de tratamientos",
    "section": "",
    "text": "Objetivo.\nLlegar a una conclusión que sea válida. Valido = representativo.\nAleatorización.\nExigencia teórica impuesta a experimentos y ensayos clínicos con el objetivo de minimizar la variabilidad de las evaluaciones y evitar la distorsión que pueden producir otros factores en las pruebas experimentales.\nCuántas veces debería hacer la simulación para saber que el resultado es verdaderamente cercano al valor desconocido real. La validez de una estimación está ligada al conportamiento de lo que quiero estimar y con la cantidad de información que tenga.\nHipótesis.\n\nLos pacientes se eligen aleatoriamente. Cualquier grupo de n pacientes tiene las mismas posibilidades de ser elegido.\n\nEl tratamiento es asignado aleatoriamente. No hay preferencias en la asignación, cada paciente tiene las mismas oportunidades de recibir uno de los tratamientos.\n\nDebo de partir con la idea de que todas las personas que son similares.\nSi quiero hacer una comparacion voy a procurar que el tamaño entre las personas que reciben cada uno de los tratamientos es similar.\nEjemplo: aplico varias metodologías de aprendizaje en niños, no puedo tener 10k niños con la metodología antigua y 1 unidad de niños con la metodología nueva, aunque todos sabemos que luego esto nunca se lleva a cabo correctamente. (vamos, la diapo14)\nObjetivos de la aleatorización.\n\nAsegurar que cualquier paciente tiene las mismas oportunidades de recibir el tratamiento experimental.\n\nEliminar sesgos en la selección.\n\nEquilibrar el tamaño de los grupos. (en función del objetivo o de las características experimentales)\n\nVerificar o estudiar la eficacia de los tratamientos.\n\nRazones para la aleatorización.\n\nLos sujetos asignados a cada tratamiento tendrán características similares.\n\nSin similitud =&gt; Sesgo en los resultados.\n\n\n\nNi el investigador, ni el paciente tendrán conocimiento del grupo de asignación en el que se va incluir al participante en el estudio.\n\nSesgo en la selección =&gt; Efectos del tratamiento sobredimensionados.\n\n\n\nLa aleatorización garantiza la validez de los test estadísticos utilizados para comparar tratamientos.\n\nLa aleatorización estratificada y la aleatorización adaptada a las covariables controlan la influencia de las covariables.\nSi quiero ver si un medicamento funciona mejor que otro:\n\nNecestiamos trabajar sobre un modelo matematico que me confirme que los individuos no estén relacionados entre sí (si son familia pues les sentará igual de mal o igual de bien cada uno de los tratamietos) y homogeneidad entre los dos grupos (un grupo más sano que otro)\nSi quiero asginar mi muestra en dos grupos la asignación aleatoria de cada individuo uno por uno va mal si tenemos una muestra reducida (elijo una persona y la asigno aleatoriamente a un grupo, luego con la siguiente). Si n es grande a la larga tendré estabilidad de frecuencias.\n\nAleatorización simple\nEstá basada en una única serie de asignaciones aleatorias. Los pacientes se asignan a los grupos de tratamiento del estudio clínico.\nSe puede imponer un control realizando asignaciones de modo que haya el mismo número de individuos en cada grupo.\nDe un grupo de individuo cojo n de ellos sin que se repitan.\n\nset.seed(178900)\ntrat=sample(1:20,10,replace=FALSE)\nindiv=sort(trat)\nindiv\n\n [1]  2  3  4  6  9 10 11 13 17 18\n\n\nSi tengo un n muy grande participando en el estudio no tengo que preocuparme por la selección.\nVentajas:\n\nEs un procedimiento sencillo y fácil de poner en práctica.\n\nEn estudios de muchos pacientes la aleatorización simple conduce a grupos con un número similar de participantes.\n\nDesventajas:\n\nLos resultados de la aleatorización pueden dar lugar a grupos de tamaños muy desiguales cuando el estudio involucra a un número reducido de pacientes.\n\nProblema:\n\nA veces no tengo todos mis pacientes a mis disposición, por ejemplo, a mitad del estudio este se para y tenemos x individuos sin haberle podido dar el tratamiento (ej: se rompe la máquina que da la dosis y muchos pacientes se quedan sin dosis)\n\nSi el suceso de no poder realizar la asignación a todos los individuos es altamente probable debo tenerlo contemplado. Para ello usamos la aleatorización por bloques.\nPor bloques o restringida\nPermite asignar aleatoriamente sujetos en los grupos de tratamiento de igual tamaño dividiendo a los pacientes potenciales en m bloques de tamaño 2n, n&gt;1.\nEn lugar de colocar las n personas al mogollón, voy equilibrando grupos más pequeños. Así si se interrumpe el experimento tengo un experimento más pequeño pero bien repartido.\nEl procedimiento consiste en repartir toda la muestra en x aleternativas igual que antes pero en vez de tener n/2 y m/2, fijo un tamaño del bloque y dentro de cada bloque jeugo con un reparto equitativo.\nEl procedimiento se basa en la construcción de todos los posibles bloques distintos formados con n asignaciones A (tratamiento) y n asignaciones B (placebo). La elección de cada bloque es aleatoria.\nSelección del tamaño de los bloques.\n\nTamaño n múltiplo del número de tratamientos.\n\nTamaño n no muy grande porque precisamente lo que quiero son bloques no grandes para poder tener la asignación equilibrada.\n\nEl n=6 suele ir bien.\n\nTamaño n dividendo del total de la muestra parece no ser una condición, tal vez el último bloque sea de menor tamaño para asignar a los individuos sin tratamiento.\n\nVentajas:\n\nEl método asegura tener grupos de tamaño equilibrado a lo largo del proceso de asignación, siempre que se use el bloque completo.\n\nDesventajas:\n\nHay que ocultar el tamaño del bloque al clínico para que la asignación no sea predecible.\n\nSi el experimento es doble ciego quiero que el médico no sepa que es asignación por blqoues, ya que si estoy asignando el primer bloque y la mitad de los individuos tienen un tratamiento automáticamente sabe que los n siguientes individuos van a tener el otro tratamiento.\nEjemplo\nAsignar tratamiento o placebo a 60 sujetos utilizando bloques de tamaño 6 (2n=6).\nCreo todos los bloques de asignación posible con todos los órdenes de asignación.\n\n\n\n\nSelecciono con reemplazamiento los bloques necesarios.\n\nset.seed(32581)\nsample(1:20,10,replace=TRUE)\n\n [1]  7  4 15  5 19  3  5  2 12 19\n\n\nEstratificada\nSe utiliza para conseguir un equilibrio entre los grupos respecto a otras características (covariables) de los sujetos. Se complica con la cantidad de covariables que quiera incluir para la creación de grupos.\nControla la posible influencia de las covariables en las conclusiones de la investigación.\nLa asignación uno a uno de los individuos no se puede si quiere controlar las covariables. La asignación estratificada debe hacerse de entrada. Necesito conocer qué niveles tengo para cada covariable y definir el tamaño muestral para cada nivel o combinación de niveles.\nEl número de estratos es el múltiplo del número de niveles de cada covariable.\nEn cada estrato se genera una secuencia de asignación mediante aleatorización simple o por bloques.\nVentajas:\n\nEl método asegura tener grupos de tamaño equilibrado teniendo en cuenta los factores influyentes.\n\nDesventajas:\n\nPrecisa conocer todas las características de los sujetos con anterioridad a la asignación en grupos.\n\nLa técnica se complica al aumentar el número de covariables.\n\nNo puede utilizarse si los sujetos se incluyen en el estudio uno a uno.\nAdaptativa o minimización\nTenemos un equilibrio en los bloques en función de los individuos que vamos recibiendo.\nSe utiliza para minimizar las diferencias de los tamaños de los distintos grupos.\nCada nuevo sujeto se asigna secuencialmente a un grupo concreto de tratamiento teniendo en cuenta las covariables y las asignaciones de los sujetos anteriores.\nEl investigador debe elaborar un plan de aleatorización para asignar los tratamientos a los pacientes\nVentajas:\n\nEl método es útil cuando hay muchas covariables y si la muestra de sujetos es pequeña.\n\nDesventajas:\n\nPrecisa recoger todas las características de los participantes con anterioridad a la aleatorización."
  },
  {
    "objectID": "tema_03/tema_03_3_obtener_datos_simulados.html#simulación-paramétrica",
    "href": "tema_03/tema_03_3_obtener_datos_simulados.html#simulación-paramétrica",
    "title": "3. Obtener datos simulados a partir de observaciones reales",
    "section": "Simulación paramétrica",
    "text": "Simulación paramétrica\nEl peor de mis problemas es tener poca muestra ya que me complica cómo validar mis estimaciones.\nA partir de los datos observados (\\(y_1, …, y_n\\)) calculamos el valor del estimador del parámetro del modelo paramétrico, \\(\\hat{\\theta}\\).\n\n\\(\\theta\\) es el verdadero valor del parámetro poblacional.\n\\(\\hat{\\theta}\\) es el estimador que se calcula a partir de los datos muestrales.\n\\(\\hat{\\theta*}\\) es una estimación del mismo parámetro obtenida mediante una técnica de remuestreo, usada para analizar la variabilidad del estimador \\(\\hat{\\theta}\\)\n\nEjemplo\n\\[\n\\theta = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} |z_1 + z_2| e^{-\\frac{z_1^2 + z_2^2}{2}} \\, dz_1 dz_2\n\\]\n\\[\n\\theta = E \\left[ 2\\pi |Z_1 + Z_2| \\right], \\text{ siendo } Z_1 \\text{ y } Z_2 \\text{ v.a. } N(0,1) \\text{ independientes}\n\\]\n\nNsim=10000\nset.seed(5597)\n\nZ1 &lt;- rnorm(Nsim)\nZ2 &lt;- rnorm(Nsim)\nX &lt;- 2*pi*abs(Z1+Z2)\n\nesperanza_X &lt;- mean(X)\nsd_X &lt;- sd(X)\n\nalpha &lt;- 0.05\nz_a2 &lt;- qnorm(1-(alpha/2))\n\nLower &lt;- esperanza_X - (z_a2*sd_X/sqrt(Nsim))\nUpper &lt;- esperanza_X + (z_a2*sd_X/sqrt(Nsim))\nc(Lower, esperanza_X, Upper)\n\n[1] 7.024079 7.129826 7.235572\n\n\nValidez de la estimación\nSe generan nuevas muestras (\\(y*_1, …, y*_n\\)) a partir de la distribución \\(F(\\hat{\\theta})\\).\n\nObjetivo de la simulación.\nConseguir información sobre la distribución del estimador T de interés.\nSi existen resultados teóricos para la distribución de T o la relación entre el estimador y su parámetro es preferible utilizarlos a depender del resultado de la simulación.\n\n¿Cómo procedemos si tenemos problemas?.\nPosibles problemas:\n\nLas propiedades teóricas de T son complicadas.\n\nNo hay resultados asintóticos.\n\nLa muestra observada es pequeña.\n\n\nTécnicas Bootstrap [Man] (remuestreo con reemplazamiento)\n\nTécnica Bootstrap\nCuando no tenemos información de la población, la distribución empírica de una muestra aleatoria es la mejor representación de la distribución de la población ==&gt; La muestra observada se toma como modelo de la distribución desconocida.\nSi hay un resultado teorico, al teorico. Si hay una aproximación, a la apriximación. Si no hay información suficientepara tirar por lo asintótico (asintótico AKA apoximación) y tengo poca muestra: Bootstrap.\nPara mejorar el conocimiento de la distribución real la técnica bootstrap realiza muestreos con reemplazamiento teniendo en cuenta la distribución empírica.\n\nSi las características del estimador no son conocidas o son muy complejas o tengo muestra muy pequeña, uso Bootstrap. Bootstrap no crea ni destruye nada.\n\nRemuestreo. Saco muestras del mismo tamaño que la inicia. “Mi muestra era esta, pero si tomara nuevas muestras del mismo tamaño mi población sería esta”.\nFinalidad.\n- Validar, mediante intervalos de confianza, la estimación del parámetro que se consigue a partir de la muestra observada.\n- Realizar contrastes de hipótesis.\nProcedimiento.\n\nSean \\((y_1, …, y_n)\\) los resultados de una medida X en n sujetos independientes.\n\nSea \\(\\theta\\) una cantidad referida a X (valor medio, mediana, desviación…).\n\nCon los resultados observados podemos calcular el valor estimación de \\(\\theta\\): \\(\\hat{\\theta}\\)\n\n\nValidación de la estimación por IC.\n\nSimulamos una nueva muestra (\\(y*_1, …, y*_n\\)) remuestreando con repetición en los resultados iniciales y calculamos el valor de \\(\\hat{\\theta*}\\).\n\nRepetimos el proceso r-veces obteniendo r estimaciones bootstrap: \\(\\hat{\\theta_i*}\\), i=1,2,…r.\n\n¿Cuántas muestras? Eso es el capítulo final del tema.\n\n\nCalculamos las diferencias entre las estimaciones bootstrap y la estimación conseguida con la muestra inicial: \\(d_i = \\hat{\\theta_i*} - \\hat{\\theta}\\).\n\nObtenemos los cuantiles asociados \\(\\alpha/2\\) y \\(1-\\alpha/2\\): \\(d_b\\), \\(d_u\\).\n\nEl intervalo de confianza bootstrap \\(1-\\alpha\\) es: \\([\\hat{\\theta} + d_b, \\hat{\\theta} + d_u]\\)\n\n\nLa muestra original la guardo y la dejo apartada y trabajo con las muestras de Bootstrap (bueno esto volver a preguntárselo de cara a algún ejercicio pq tampoco creo si me ha contestado lo mismo dos veces seguidas). Trabajamos solo con las r estimaciones Bootstrap."
  },
  {
    "objectID": "tema_03/tema_03_3_obtener_datos_simulados.html#simulación-no-paramétrica",
    "href": "tema_03/tema_03_3_obtener_datos_simulados.html#simulación-no-paramétrica",
    "title": "3. Obtener datos simulados a partir de observaciones reales",
    "section": "Simulación no paramétrica",
    "text": "Simulación no paramétrica\nNo se asume una distribución teórica, se remuestrea los datos originales para simular nuevas muestras.\ndiapo43 es la distribución del estadístico\npero no es la distribucion de p, sino del estadñistico\ncon la simulacion ya no tengo estimaciones, tengo estimaciones de la probabilidades\nlas muestras no tienen probabilidad, pq las muestras están fijas. tienen frecuencias, no dudo. en el modelo teórico tengo algo genérico, ahí sí hablo de probabilidades"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Camargo-Ramos, C. M., and otros autores. 2012. “Evaluación de\nFactores Asociados Al Embarazo Adolescente.” Revista\nColombiana de Obstetricia y Ginecología 61 (3): 256–62. http://www.scielo.org.co/pdf/rcog/v61n3/v61n3a09.pdf.\n\n\nFisterra. 2024. “La Fiabilidad de Las Mediciones Clínicas:\nAnálisis de La Concordancia Para Variables Numéricas.” 2024. https://www.fisterra.com/formacion/metodologia-investigacion/la-fiabilidad-mediciones-clinicas-analisis-concordancia-para-variables-numericas/."
  }
]